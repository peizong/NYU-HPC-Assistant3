file,chunk_id,chunk
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\01-hpc-intro.html,0,"There isn't a GitHub Pages site here. If you're trying to publish one, read the full documentation to learn how to set up GitHub Pages for your repository, organization, or user account."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell.html,0,"This workshop is an introduction to using high-performance computing systems effectively. We obviously can’t cover every case or give an exhaustive course on parallel programming in just two days of teaching time. Instead, this workshop is intended to give students a good introduction and overview of the tools available and how to use them effectively. By the end of this workshop, students will know how to: - Use the UNIX command line (also known as terminal or shell) to operate a computer, connect to a cluster, and write simple shell scripts. NOTE - This is the draft HPC Carpentry release. Comments and feedback are welcome. - Additionally, please explore the lesson, shell-novice until this one is complete. Prerequisites There are no real prerequisites for this lesson, but prior programming and/or command line experience will be helpful."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_00-hpc-intro_index.html.html,0,"Why Use a Cluster? Overview Teaching: 25 min Exercises: 5 minQuestions Why would I be interested in High Performance Computing (HPC)? What can I expect to learn from this course? Objectives Be able to describe what an HPC system is. Identify how an HPC system could benefit you. Why Use These Computers? What do you need? Talk to your neighbor about your research. How does computing help you do your research? How could more computing help you do more or better research? Frequently, research problems that use computing can outgrow the desktop or laptop computer where they started: - A statistics student wants to do cross-validate their model. This involves running the model 1000 times — but each run takes an hour. Running on their laptop will take over a month! - A genomics researcher has been using small datasets of sequence data, but soon will be receiving a new type of sequencing data that is 10 times as large."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_00-hpc-intro_index.html.html,1,"It’s already challenging to open the datasets on their computer — analyzing these larger datasets will probably crash it. - An engineer is using a fluid dynamics package that has an option to run in parallel. So far, they haven’t used this option on their desktop, but in going from 2D to 3D simulations, simulation time has more than tripled and it might be useful to take advantage of that feature. In all these cases, what is needed is access to more computers than can be used at the same time. Luckily, large scale computing systems — shared computing resources with lots of computers — are available at many universities, labs, or through national networks. These resources usually have more central processing units(CPUs), CPUs that operate at higher speeds, more memory, more storage, and faster connections with other computer systems. They are frequently called “clusters”, “supercomputers” or resources for “high performance computing” or HPC."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_00-hpc-intro_index.html.html,2,"In this lesson, we will usually use the terminology of HPC and HPC cluster. Using a cluster often has the following advantages for researchers: - Speed. With many more CPU cores, often with higher performance specs, than a typical laptop or desktop, HPC systems can offer significant speed up. - Volume. Many HPC systems have both the processing memory (RAM) and disk storage to handle very large amounts of data. Terabytes of RAM and petabytes of storage are available for research projects. - Efficiency. Many HPC systems operate a pool of resources that are drawn on by many users. In most cases when the pool is large and diverse enough the resources on the system are used almost constantly. - Cost. Bulk purchasing and government funding mean that the cost to the research community for using these systems in significantly less that it would be otherwise. - Convenience. Maybe your calculations just take a long time to run or are otherwise inconvenient to run on your personal computer."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_00-hpc-intro_index.html.html,3,"There’s no need to tie up your own computer for hours when you can use someone else’s instead. This is how a large-scale compute system like a cluster can help solve problems like those listed at the start of the lesson. Thinking ahead How do you think using a large-scale computing system will be different from using your laptop? Talk to your neighbor about some differences you may already know about, and some differences/difficulties you imagine you may run into. On Command Line Using HPC systems often involves the use of a shell through a command line interface (CLI) and either specialized software or programming techniques. The shell is a program with the special role of having the job of running other programs rather than doing calculations or similar tasks itself. What the user types goes into the shell, which then figures out what commands to run and orders the computer to execute them."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_00-hpc-intro_index.html.html,4,"(Note that the shell is called “the shell” because it encloses the operating system in order to hide some of its complexity and make it simpler to interact with.) The most popular Unix shell is Bash, the Bourne Again SHell (so-called because it’s derived from a shell written by Stephen Bourne). Bash is the default shell on most modern implementations of Unix and in most packages that provide Unix-like tools for Windows. Interacting with the shell is done via a command line interface (CLI) on most HPC systems. In the earliest days of computers, the only way to interact with early computers was to rewire them. From the 1950s to the 1980s most people used line printers. These devices only allowed input and output of the letters, numbers, and punctuation found on a standard keyboard, so programming languages and software interfaces had to be designed around that constraint and text-based interfaces were the way to do this."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_00-hpc-intro_index.html.html,5,"A typing-based interface is often called a command-line interface, or CLI, to distinguish it from a graphical user interface, or GUI, which most people now use. The heart of a CLI is a read-evaluate-print loop, or REPL: when the user types a command and then presses the Enter (or Return) key, the computer reads it, executes it, and prints its output. The user then types another command, and so on until the user logs off. Learning to use Bash or any other shell sometimes feels more like programming than like using a mouse. Commands are terse (often only a couple of characters long), their names are frequently cryptic, and their output is lines of text rather than something visual like a graph. However, using a command line interface can be extremely powerful, and learning how to use one will allow you to reap the benefits described above. The rest of this lesson The only way to use these types of resources is by learning to use the command line."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_00-hpc-intro_index.html.html,6,"This introduction to HPC systems has two parts: - We will learn to use the UNIX command line (also known as Bash). - We will use our new Bash skills to connect to and operate a high-performance computing supercomputer. The skills we learn here have other uses beyond just HPC: Bash and UNIX skills are used everywhere, be it for web development, running software, or operating servers. It’s become so essential that Microsoft now ships it as part of Windows! Knowing how to use Bash and HPC systems will allow you to operate virtually any modern device. With all of this in mind, let’s connect to a cluster and get started! Key Points High Performance Computing (HPC) typically involves connecting to very large computing systems elsewhere in the world. These HPC systems can be used to do work that would either be impossible or much slower or smaller systems. The standard method of interacting with such systems is via a command line interface such as Bash."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,0,"Connecting to the remote HPC system Overview Teaching: 25 min Exercises: 10 minQuestions How do I open a terminal? How do I connect to a remote computer? What is an SSH key? Objectives Connect to a remote HPC system. Prerequisites To access the Greene HPC cluster, you must be connected to the NYU network. If you are physically on campus and connected via a wired connection in your office or through NYU’s WiFi, you can directly SSH into the clusters without any additional steps. However, if you are off-campus or working remotely, connecting through the NYU VPN or using the gateway servers is required to establish a secure connection to the HPC systems. Remote Connections with the NYU VPN & HPC Gateway Server If you are connecting from a remote location that is not on the NYU network (your home for example), you have two options: - VPN Option: set up your computer to use the NYU VPN. Once you’ve created a VPN connection, you can proceed as if you were connected to the NYU net."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,1,"- Gateway Option: go through our gateway servers (example below). Gateways are designed to support only a very minimal set of commands and their only purpose is to let users connect HPC systems without needing to first connect to the VPN. Log into the Greene Cluster NYU Campus: From within the NYU network, that is, from an on-campus location, or after you VPN inside NYU’s network, you can login to the HPC clusters directly. Off-campus: The host name of Greene is ‘greene.hpc.nyu.edu’. Logging in to Greene is the two-stage process. The HPC clusters (Greene) are not directly visible to the internet (outside the NYU Network). If you are outside NYU’s Network (off-campus) you must first login to a bastion host named gw.hpc.nyu.edu. From within the NYU network, that is, from an on-campus location, or after you VPN inside NYU’s network, you can log in to the HPC clusters directly. You do not need to log in to the bastion host."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,2,"To log in to the HPC cluster (Greene), simply use: ssh <NYUNetID>@greene.hpc.nyu.edu For access from Windows stations using PuTTY, please click here. To connect to VPN from Linux/MAC, please click here. From an off-campus location (outside NYU-NET), logging in to the HPC clusters is a two-step process: - First, log in to the bastion host, gw.hpc.nyu.edu . From a Mac or Linux workstation, this is a simple terminal command (replace<NYUNetID> with your NetID). Your password is the same password you use for NYU Home: ssh <NYUNetID>@gw.hpc.nyu.edu Windows users will need to use PuTTY, see here for instructions. - Next, log in to the cluster. For Greene, this is done with: ssh <NYUNetID>@greene.hpc.nyu.edu Opening a Terminal Accessing the Greene HPC cluster is primarily done through the Command Line Interface (CLI). A CLI provides a text-based environment that allows users to manage files, run programs, and navigate directories via command input."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,3,"On macOS, the built-in CLI tool is Terminal, while Windows 10 users can leverage the Windows Subsystem for Linux (WSL) for similar functionality. Additionally, a popular tool for connecting to Linux servers from Windows is PuTTY, a free SSH client. Connecting to an HPC system is most often done through a tool known as “SSH” (Secure SHell) and usually SSH is run through a terminal. So, to begin using an HPC system we need to begin by opening a terminal. Different operating systems have different terminals, none of which are exactly the same in terms of their features and abilities while working on the operating system. When connected to the remote system the experience between terminals will be identical as each will faithfully present the same experience of using that system. Here is the process for opening a terminal in each operating system. Linux There are many different versions (aka “flavours”) of Linux and how to open a terminal window can change between flavours."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,4,"Fortunately most Linux users already know how to open a terminal window since it is a common part of the workflow for Linux users. If this is something that you do not know how to do then a quick search on the Internet for “how to open a terminal window in” with your particular Linux flavour appended to the end should quickly give you the directions you need. To connect to the gateway servers, simply open a terminal application and enter the following command: ssh <NetID>@gw.hpc.nyu.edu After typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to one of the HPC clusters: # this will connect you to Greene HPC cluster ssh <NetID>@greene.hpc.nyu.edu Mac Macs have had a terminal built in since the first version of OS X since it is built on a UNIX-like operating system, leveraging many parts from BSD (Berkeley Software Distribution). The terminal can be quickly opened through the use of the Searchlight tool."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,5,"Hold down the command key and press the spacebar. In the search bar that shows up type “terminal”, choose the terminal app from the list of results (it will look like a tiny, black computer screen) and you will be presented with a terminal window. Alternatively, you can find Terminal under “Utilities” in the Applications menu. To connect to the gateway servers, simply open a terminal application and enter the following command: ssh <NetID>@gw.hpc.nyu.edu After typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to one of the HPC clusters: # this will connect you to Greene HPC cluster ssh <NetID>@greene.hpc.nyu.edu Windows While Windows does have a command-line interface known as the “Command Prompt” that has its roots in MS-DOS (Microsoft Disk Operating System) it does not have an SSH tool built into it and so one needs to be installed."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,6,"There are a variety of programs that can be used for this; a few common ones we describe here, as follows: Git BASH Git BASH gives you a terminal like interface in Windows. You can use this to connect to a remote computer via SSH. It can be downloaded for free from here. Windows Subsystem for Linux The Windows Subsystem for Linux also allows you to connect to a remote computer via SSH. Instructions on installing it can be found here. MobaXterm MobaXterm is a terminal window emulator for Windows and the home edition can be downloaded for free from mobatek.net. If you follow the link you will note that there are two editions of the home version available: Portable and Installer. The portable edition puts all MobaXterm content in a folder on the desktop (or anywhere else you would like it) so that it is easy to add plug-ins or remove the software. The installer edition adds MobaXterm to your Windows installation and menu as any other program you might install."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,7,"If you are not sure that you will continue to use MobaXterm in the future, the portable edition is likely the best choice for you. MobaKeyGen, see the MoabXterm documentation Download the version that you would like to use and install it as you would any other software on your Windows installation. Once the software is installed you can run it by either opening the folder installed with the portable edition and double-clicking on the executable file named MobaXterm_Personal_11.1 (your version number may vary) or, if the installer edition was used, finding the executable through either the start menu or the Windows search option. Once the MobaXterm window is open you should see a large button in the middle of that window with the text “Start Local Terminal”. Click this button and you will have a terminal window at your disposal."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,8,"PuTTY It is strictly speaking not necessary to have a terminal running on your local computer in order to access and use a remote system, only a window into the remote system once connected. PuTTY is likely It is, strictly speaking, not necessary to have a terminal running on your local computer in order to access and use a remote system, only a window into the remote system once connected. PuTTY is likely the oldest, most well-known, and widely used software solution to take this approach. PuTTY is available for free download from https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html. Download the version that is correct for your operating system and install it as you would other software on your Windows system. Once installed it will be available through the start menu or similar. puttygen, see the Putty documentation Running PuTTY will not initially produce a terminal but instead a window full of connection options."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,9,Putting the address of the remote system in the “Host Name (or IP Address)” box and either pressing enter or clicking the “Open” button should begin the connection process. If this works you will see a terminal window open that prompts you for a username through the “login as:” prompt and then for a password. If both of these are passed correctly then you will be given access to the system and will see a message saying so within the terminal. If you need to escape the authentication process you can hold the Control (Ctrl) key and press the c key to exit and start again. Note that you may want to paste in your password rather than typing it. Use Ctrl plus a right-click of the mouse to paste content from the clipboard to the PuTTY terminal. For those logging in with PuTTY it would likely be best to cover the terminal basics already mentioned above before moving on to navigating the remote system.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,10,"Open OnDemand (Web-based Graphical User Interface) Open OnDemand is an open source project funded by the National Science Foundation (NSF). Open OnDemand is designed to create easier access to users to interface with HPC systems. Originally developed by Ohio Supercomputer Center (OSC), used by many universities around the world, and now servicing the NYU Greene HPC cluster. Open OnDemand has a variety of convenient tools to manage files, access the command line, manage and monitor jobs, and launch interactive applications, such as Jupyter Notebooks, RStudio sessions, and even full Linux Desktops."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,11,"Features Include: Easy file management - upload and download files, view HTML and pictures without downloading Command-line shell access without any SSH client locally installed Job management and monitoring Full Linux desktop experience without X11 Interactive Apps such as JupyterHub and RStudio without the need for port forwarding Open OnDemand (OOD) is accessible to all users with a valid NYU HPC account while on-campus network or through a VPN. To access OOD visit: https://ood.hpc.nyu.edu (VPN Required) Access the Shell Under the clusters menu you can select the Greene Shell Access option to access the Linux shell. No local SSH client is required. Interactive Applications GUI based applications are accessible without the need for port or X11 forwarding. Select the Interactive Apps menu, select the desired application, and submit the job based on required resources and options."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,12,"Troubleshooting Connections to Open OnDemand A common issue that can occur is receiving an error that the Open OnDemand page cannot be reached. Sometimes this can indicate that the service is down, but often this is an issue with the the local browser cache. You can test this by opening a private browser window and seeing if https://ood.hpc.nyu.edu will load. If it does, try deleting the cache for https://ood.hpc.nyu.edu in your browser history to resolve this issue. In Chrome, this can be done by navigating to this page in your settings: chrome://settings/content/all?searchSubpage=ood.hpc.nyu.edu&search=site+data The link above will automatically search for the Open OnDemand site data and cookies. You can then simply click on the trashcan icon to delete the site cache. Once done, try navigating again to https://ood.hpc.nyu.edu and the site should load. For other issues please email hpc@nyu.edu."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,13,"Creating an SSH key SSH keys are an alternative method for authentication to obtain access to remote computing systems. They can also be used for authentication when transferring files or for accessing version control systems. In this section you will create a pair of SSH keys, a private key which you keep on your own computer and a public key which is placed on the remote HPC system that you will log in to. Linux, Mac and Windows Subsystem for Linux Once you have opened a terminal check for existing SSH keys and filenames since existing SSH keys are overwritten, $ ls ~/.ssh/ then generate a new public-private key pair, $ ssh-keygen -t ed25519 -a 100 -f ~/.ssh/id_Graham_ed25519 -o (no default): use the OpenSSH key format, rather than PEM.-a (default is 16): number of rounds of passphrase derivation; increase to slow down brute force attacks.-t (default is rsa): specify the “type” or cryptographic algorithm."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,14,"ed25519 is faster and shorter than RSA for comparable strength.-f (default is /home/user/.ssh/id_algorithm): filename to store your keys. If you already have SSH keys, make sure you specify a different name:ssh-keygen will overwrite the default key if you don’t specify! If ed25519 is not available, use the older (but strong and trusted) RSA cryptography: $ ls ~/.ssh/ $ ssh-keygen -o -a 100 -t rsa -b 4096 -f ~/.ssh/id_Graham_rsa The flag -b sets the number of bits in the key. The default is 2048. EdDSA uses a fixed key length, so this flag would have no effect. When prompted, enter a strong password that you will remember. Cryptography is only as good as the weakest link, and this will be used to connect to a powerful, precious, computational resource. Take a look in ~/.ssh (use ls ~/.ssh ). You should see the two new files: your private key (~/.ssh/key_Graham_ed25519 or ~/.ssh/key_Graham_rsa ) and the public key (~/.ssh/key_Graham_ed25519.pub or ~/.ssh/key_Graham_rsa.pub )."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,15,"If a key is requested by the system administrators, the public key is the one to provide. Private keys are your private identity A private key that is visible to anyone but you should be considered compromised, and must be destroyed. This includes having improper permissions on the directory it (or a copy) is stored in, traversing any network in the clear, attachment on unencrypted email, and even displaying the key (which is ASCII text) in your terminal window. Protect this key as if it unlocks your front door. In many ways, it does. Further information For more information on SSH security and some of the flags set here, an excellent resource is Secure Secure Shell. Logging onto the system With all of this in mind, let’s connect to a remote HPC system. In this workshop, we will connect to Graham — an HPC system located at the University of Waterloo."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,16,"Although it’s unlikely that every system will be exactly like Graham, it’s a very good example of what you can expect from an HPC installation. To connect to our example computer, we will use SSH (if you are using PuTTY, see above). SSH allows us to connect to UNIX computers remotely, and use them as if they were our own. The general syntax of the connection command follows the format ssh -i ~/.ssh/key_for_remote_computer <NetID>@greene.hpc.nyu.edu when using SSH keys and ssh yourUsername@some.computer.address if only password access is available. Let’s attempt to connect to the HPC system now: ssh -i ~/.ssh/key_Graham_ed25519 yourUsername@graham.computecanada.ca or ssh -i ~/.ssh/key_Graham_rsa yourUsername@graham.computecanada.ca or if SSH keys have not been enabled ssh yourUsername@graham.computecanada.ca The authenticity of host 'graham.computecanada.ca (199.241.166.2)' can't be established. ECDSA key fingerprint is SHA256:JRj286Pkqh6aeO5zx1QUkS8un5fpcapmezusceSGhok."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,17,"ECDSA key fingerprint is MD5:99:59:db:b1:3f:18:d0:2c:49:4e:c2:74:86:ac:f7:c6. Are you sure you want to continue connecting (yes/no)? # type ""yes""! Warning: Permanently added the ECDSA host key for IP address '199.241.166.2' to the list of known hosts. yourUsername@graham.computecanada.ca's password: # no text appears as you enter your password Last login: Wed Jun 28 16:16:20 2017 from s2.n59.queensu.ca Welcome to the ComputeCanada/SHARCNET cluster Graham. If you’ve connected successfully, you should see a prompt like the one below. This prompt is informative, and lets you grasp certain information at a glance. (If you don’t understand what these things are, don’t worry!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,18,We will cover things in depth as we explore the system further.) [yourUsername@gra-login1 ~]$ Telling the Difference between the Local Terminal and the Remote Terminal You may have noticed that the prompt changed when you logged into the remote system using the terminal (if you logged in using PuTTY this will not apply because it does not offer a local terminal). This change is important because it makes it clear on which system the commands you type will be run when you pass them into the terminal. This change is also a small complication that we will need to navigate throughout the workshop. Exactly what is reported before the $ in the terminal when it is connected to the local system and the remote system will typically be different for every user.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,19,"We still need to indicate which system we are entering commands on though so we will adopt the following convention: [local]$ when the command is to be entered on a terminal connected to your local computer[yourUsername@gra-login1 ~]$ when the command is to be entered on a terminal connected to the remote system$ when it really doesn’t matter which system the terminal is connected to. Being certain which system your terminal is connected to If you ever need to be certain which system a terminal you are using is connected to then use the following command: $ hostname . Keep two terminal windows open It is strongly recommended that you have two terminals open, one connected to the local system and one connected to the remote system, that you can switch back and forth between."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_01-connecting_index.html.html,20,"If you only use one terminal window then you will need to reconnect to the remote system using one of the methods above when you see a change from [local]$ to[yourUsername@gra-login1 ~]$ and disconnect when you see the reverse. Key Points To connect to a remote HPC system using SSH and a password, run ssh <NetID>@greene.hpc.nyu.edu .To connect to a remote HPC system using SSH and an SSH key, run ssh -i ~/.ssh/key_for_remote_computer <NetID>@greene.hpc.nyu.edu ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,0,"Moving around and looking at things Overview Teaching: 15 min Exercises: 5 minQuestions How do I navigate and look around the system? Objectives Learn how to navigate around directories and look at their contents Explain the difference between a file and a directory. Translate an absolute path into a relative path and vice versa. Identify the actual command, flags, and filenames in a command-line call. Demonstrate the use of tab completion, and explain its advantages. At this point in the lesson, we’ve just logged into the system. Nothing has happened yet, and we’re not going to be able to do anything until we learn a few basic commands. By the end of this lesson, you will know how to “move around” the system and look at what’s there. System Architecture Files Systems for usage: The NYU HPC clusters have multiple file systems for user’s files. Each file system is configured differently to serve a different purpose."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,1,"Right now, all we see is something that looks like this: [yourUsername@gra-login1 ~]$ The dollar sign is a prompt, which shows us that the shell is waiting for input; your shell may use a different character as a prompt and may add information before the prompt. When typing commands, either from these lessons or from other sources, do not type the prompt, only the commands that follow it. Type the command whoami , then press the Enter key (sometimes marked Return) to send the command to the shell. The command’s output is the ID of the current user, i.e., it shows us who the shell thinks we are: $ whoami <NetID> More specifically, when we type whoami the shell: - finds a program called whoami , - runs that program, - displays that program’s output, then - displays a new prompt to tell us that it’s ready for more commands. Next, let’s find out where we are by running a command called pwd (which stands for “print working directory”). (“Directory” is another word for “folder”)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,2,"At any moment, our current working directory (where we are) is the directory that the computer assumes we want to run commands in unless we explicitly specify something else. Here, the computer’s response is /home/<NetID> , which is <NetID> home directory. Note that the location of your home directory may differ from system to system. $ pwd /home/<NetID> So, we know where we are. How do we look and see what’s in our current directory? $ ls ls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns. Differences between remote and local system Open a second terminal window on your local computer and run the ls command without logging in remotely. What differences do you see?Solution You would likely see something more like this: Applications Documents Library Music Public Desktop Downloads Movies Pictures In addition you should also note that the preamble before the prompt ( $ ) is different."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,3,"This is very important for making sure you know what system you are issuing commands on when in the shell. If nothing shows up when you run ls , it means that nothing’s there. Let’s make a directory for us to play with. mkdir <new directory name> makes a new directory with that name in your current location. Notice that this command required two pieces of input: the actual name of the command (mkdir ) and an argument that specifies the name of the directory you wish to create. $ mkdir documents Let’s us ls again. What do we see? Our folder is there, awesome. What if we wanted to go inside it and do stuff there? We will use the cd (change directory) command to move around. Let’s cd into our new documents folder. $ cd documents $ pwd ~/documents What is the ~ character? When using the shell, ~ is a shortcut that represents /home/<NetID> . Now that we know how to use cd , we can go anywhere. That’s a lot of responsibility."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,4,"What happens if we get “lost” and want to get back to where we started? To go back to your home directory, the following three commands will work: $ cd /home/<NetID> $ cd ~ $ cd A quick note on the structure of a UNIX (Linux/Mac/Android/Solaris/etc) filesystem. Directories and absolute paths (i.e. exact position in the system) are always prefixed with a / . / by itself is the “root” or base directory. Let’s go there now, look around, and then return to our home directory. $ cd / $ ls $ cd ~ bin dev initrd local mnt proc root scratch tmp work boot etc lib localscratch nix project run srv usr cvmfs home lib64 media opt ram sbin sys var The “home” directory is the one where we generally want to keep all of our files. Other folders on a UNIX OS contain system files, and get modified and changed as you install new software or upgrade your OS. Using HPC filesystems On HPC systems, you have a number of places where you can store your files."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,5,"These differ in both the amount of space allocated and whether or not they are backed up. File storage locations: - Network filesystem - Your home directory is an example of a network filesystem. Data stored here is available throughout the HPC system and files stored here are often backed up (but check your local configuration to be sure!). Files stored here are typically slower to access, the data is actually stored on another computer and is being transmitted and made available over the network! - Scratch - Some systems may offer “scratch” space. Scratch space is typically faster to use than your home directory or network filesystem, but is not usually backed up, and should not be used for long term storage. - Work file system - As an alternative to (or sometimes as well as) Scratch space, some HPC systems offer fast file system access as a work file system. Typically, this will have higher performance than your home directory or network file system and may not be backed up."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,6,"It differs from scratch space in that files in a work file system are not automatically deleted for you, you must manage the space yourself. - Local scratch (job only) - Some systems may offer local scratch space while executing a job. (A job is a program which you submit to run on an HPC system, and will be covered later.) Such storage is very fast, but will be deleted at the end of your job. - Ramdisk (job only) - Some systems may let you store files in a “RAM disk” while running a job, where files are stored directly in the computer’s memory. This extremely fast, but files stored here will count against your job’s memory usage and be deleted at the end of your job. There are several other useful shortcuts you should be aware of. . represents your current directory.. represents the “parent” directory of your current location- While typing nearly anything, you can have bash try to autocomplete what you are typing by pressing the tab key."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,7,"Let’s try these out now: $ cd ./documents $ pwd $ cd .. $ pwd /home/<NetID>/documents /home/<NetID> Many commands also have multiple behaviours that you can invoke with command line ‘flags.’ What is a flag? It’s generally just your command followed by a ‘-‘ and the name of the flag (sometimes it’s ‘–’ followed by the name of the flag). You follow the flag(s) with any additional arguments you might need. We’re going to demonstrate a couple of these “flags” using ls . Show hidden files with -a . Hidden files are files that begin with . , these files will not appear otherwise, but that doesn’t mean they aren’t there! “Hidden” files are not hidden for security purposes, they are usually just config files and other tempfiles that the user doesn’t necessarily need to see all the time. $ ls -a . .. .bash_logout .bash_profile .bashrc documents .emacs .mozilla .ssh Notice how both . and .. are visible as hidden files."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,8,"Show files, their size in bytes, date last modified, permissions, and other things with -l . $ ls -l drwxr-xr-x 2 <NetID> tc001 4096 Jan 14 17:31 documents This is a lot of information to take in at once, but we will explain this later! ls -l is extremely useful, and tells you almost everything you need to know about your files without actually looking at them. We can also use multiple flags at the same time! $ ls -l -a [yourUsername@gra-login1 ~]$ ls -la total 36 drwx--S--- 5 <NetID> tc001 4096 Nov 28 09:58 . drwxr-x--- 3 root tc001 4096 Nov 28 09:40 .. -rw-r--r-- 1 <NetID> tc001 18 Dec 6 2016 .bash_logout -rw-r--r-- 1 <NetID> tc001 193 Dec 6 2016 .bash_profile -rw-r--r-- 1 <NetID> tc001 231 Dec 6 2016 .bashrc drwxr-sr-x 2 <NetID> tc001 4096 Nov 28 09:58 documents -rw-r--r-- 1 <NetID> tc001 334 Mar 3 2017 .emacs drwxr-xr-x 4 <NetID> tc001 4096 Aug 2 2016 .mozilla drwx--S--- 2 <NetID> tc001 4096 Nov 28 09:58 .ssh Flags generally precede any arguments passed to a UNIX command."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,9,"ls actually takes an extra argument that specifies a directory to look into. When you use flags and arguments together, the syntax (how it’s supposed to be typed) generally looks something like this: $ command <flags/options> <arguments> So using ls -l -a on a different directory than the one we’re in would look something like: $ ls -l -a ~/documents drwxr-sr-x 2 <NetID> tc001 4096 Nov 28 09:58 . drwx--S--- 5 <NetID> tc001 4096 Nov 28 09:58 .. Where to go for help? How did I know about the -l and -a options? Is there a manual we can look at for help when we need help? There is a very helpful manual for most UNIX commands: man (if you’ve ever heard of a “man page” for something, this is what it is). $ man ls LS(1) User Commands LS(1) NAME ls - list directory contents SYNOPSIS ls [OPTION]... [FILE]... DESCRIPTION List information about the FILEs (the current directory by default). Sort entries alphabetically if none of -cftuvSUX nor --sort is specified."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,10,"Mandatory arguments to long options are mandatory for short options too. To navigate through the man pages, you may use the up and down arrow keys to move line-by-line, or try the spacebar and “b” keys to skip up and down by full page. Quit the man pages by typing “q”. Alternatively, most commands you run will have a --help option that displays addition information For instance, with ls : $ ls --help Usage: ls [OPTION]... [FILE]... List information about the FILEs (the current directory by default). Sort entries alphabetically if none of -cftuvSUX nor --sort is specified. Mandatory arguments to long options are mandatory for short options too. -a, --all do not ignore entries starting with . -A, --almost-all do not list implied . and .."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,11,"--author with -l, print the author of each file -b, --escape print C-style escapes for nongraphic characters --block-size=SIZE scale sizes by SIZE before printing them; e.g., '--block-size=M' prints sizes in units of 1,048,576 bytes; see SIZE format below -B, --ignore-backups do not list implied entries ending with ~ # further output omitted for clarity Unsupported command-line options If you try to use an option that is not supported, ls and other programs will print an error message similar to this:[remote]$ ls -j ls: invalid option -- 'j' Try 'ls --help' for more information. Looking at documentation Looking at the man page for ls or usingls --help , what does the-h (--human-readable ) option do? Absolute vs Relative Paths Starting from /Users/amanda/data/ , which of the following commands could Amanda use to navigate to her home directory, which is/Users/amanda ? cd . cd / cd /home/amanda cd ../.. cd ~ cd home cd ~/data/.. cd cd .. Solution - No: ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,12,"stands for the current directory.- No: / stands for the root directory.- No: Amanda’s home directory is /Users/amanda .- No: this goes up two levels, i.e. ends in /Users .- Yes: ~ stands for the user’s home directory, in this case/Users/amanda .- No: this would navigate into a directory home in the current directory if it exists.- Yes: unnecessarily complicated, but correct. - Yes: shortcut to go back to the user’s home directory. - Yes: goes up one level. Relative Path Resolution Using the filesystem diagram below, if pwd displays/Users/thing , what willls -F ../backup display? ../backup: No such file or directory 2012-12-01 2013-01-08 2013-01-27 2012-12-01/ 2013-01-08/ 2013-01-27/ original/ pnas_final/ pnas_sub/ Solution - No: there is a directory backup in/Users .- No: this is the content of Users/thing/backup , but with.. we asked for one level further up.- No: see previous explanation. - Yes: ../backup/ refers to/Users/backup/ ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,13,"ls Reading ComprehensionAssuming a directory structure as in the above Figure (File System for Challenge Questions), if pwd displays/Users/backup , and-r tellsls to display things in reverse order, what command will display:pnas_sub/ pnas_final/ original/ ls pwd ls -r -F ls -r -F /Users/backup - Either #2 or #3 above, but not #1. Solution - No: pwd is not the name of a directory.- Yes: ls without directory argument lists files and directories in the current directory.- Yes: uses the absolute path explicitly. - Correct: see explanations above. Exploring More ls ArgumentsWhat does the command ls do when used with the-l and-h arguments?Some of its output is about properties that we do not cover in this lesson (such as file permissions and ownership), but the rest should be useful nevertheless. Solution The -l arguments makesls use a long listing format, showing not only the file/directory names but also additional information such as the file size and the time of its last modification."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_02-navigation_index.html.html,14,"The-h argument makes the file size “human readable”, i.e. display something like5.3K instead of5369 . Listing Recursively and By Time The command ls -R lists the contents of directories recursively, i.e., lists their sub-directories, sub-sub-directories, and so on in alphabetical order at each level. The commandls -t lists things by time of last change, with most recently changed files or directories first. In what order doesls -R -t display things? Hint:ls -l uses a long listing format to view timestamps.Solution The directories are listed alphabetical at each level, the files/directories in each directory are sorted by time of last change. Key Points Your current directory is referred to as the working directory. To change directories, use cd .To view files, use ls .You can view help for a command with man command orcommand --help .Hit tab to autocomplete whatever you’re currently typing."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,0,"Writing and reading files Overview Teaching: 30 min Exercises: 15 minQuestions How do I create/edit text files? How do I move/copy/delete files? Objectives Learn to use the nano text editor.Understand how to move, create, and delete files. Now that we know how to move around and look at things, let’s learn how to read, write, and handle files! We’ll start by moving back to our home directory and creating a scratch directory: $ cd ~ $ mkdir hpc-test $ cd hpc-test Creating and Editing Text Files When working on an HPC system, we will frequently need to create or edit text files. Text is one of the simplest computer file formats, defined as a simple sequence of text lines. What if we want to make a file? There are a few ways of doing this, the easiest of which is simply using a text editor. For this lesson, we are going to us nano , since it’s more intuitive than many other terminal text editors."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,1,"To create or edit a file, type nano <filename> , on the terminal, where <filename> is the name of the file. If the file does not already exist, it will be created. Let’s make a new file now, type whatever you want in it, and save it. $ nano draft.txt Nano defines a number of shortcut keys (prefixed by the Control or Ctrl key) to perform actions such as saving the file or exiting the editor. Here are the shortcut keys for a few common actions: - Ctrl+O — save the file (into a current name or a new name). - Ctrl+X — exit the editor. If you have not saved your file upon exiting, nano will ask you if you want to save. - Ctrl+K — cut (“kill”) a text line. This command deletes a line and saves it on a clipboard. If repeated multiple times without any interruption (key typing or cursor movement), it will cut a chunk of text lines. - Ctrl+U — paste the cut text line (or lines). This command can be repeated to paste the same text elsewhere."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,2,"Using vim as a text editorFrom time to time, you may encounter the vim text editor. Althoughvim isn’t the easiest or most user-friendly of text editors, you’ll be able to find it on any system and it has many more features thannano . vim has several modes, a “command” mode (for doing big operations, like saving and quitting) and an “insert” mode. You can switch to insert mode with thei key, and command mode withEsc .In insert mode, you can type more or less normally. In command mode there are a few commands you should be aware of: :q! — quit, without saving:wq — save and quitdd — cut/delete a liney — paste a line Do a quick check to confirm our file was created. $ ls draft.txt Reading Files Let’s read the file we just created now. There are a few different ways of doing this, one of which is reading the entire file with cat . $ cat draft.txt It's not ""publish or perish"" any more, it's ""share and thrive"". By default, cat prints out the content of the given file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,3,"Although cat may not seem like an intuitive command with which to read files, it stands for “concatenate”. Giving it multiple file names will print out the contents of the input files in the order specified in the cat ’s invocation. For example, $ cat draft.txt draft.txt It's not ""publish or perish"" any more, it's ""share and thrive"". It's not ""publish or perish"" any more, it's ""share and thrive"". Reading Multiple Text Files Create two more files using nano , giving them different names such aschap1.txt andchap2.txt . Then use a singlecat command to read and print the contents ofdraft.txt ,chap1.txt , andchap2.txt . Creating Directory We’ve successfully created a file. What about a directory? We’ve actually done this before, using mkdir . $ mkdir files $ ls draft.txt files Moving, Renaming, Copying Files Moving — We will move draft.txt to the files directory with mv (“move”) command."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,4,"The same syntax works for both files and directories: mv <file/directory> <new-location> $ mv draft.txt files $ cd files $ ls draft.txt Renaming — draft.txt isn’t a very descriptive name. How do we go about changing it? It turns out that mv is also used to rename files and directories. Although this may not seem intuitive at first, think of it as moving a file to be stored under a different name. The syntax is quite similar to moving files: mv oldName newName . $ mv draft.txt newname.testfile $ ls newname.testfile File extensions are arbitrary In the last example, we changed both a file’s name and extension at the same time. On UNIX systems, file extensions (like .txt ) are arbitrary. A file is a.txt file only because we say it is. Changing the name or extension of the file will never change a file’s contents, so you are free to rename things as you wish. With that in mind, however, file extensions are a useful tool for keeping track of what type of data it contains."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,5,"A.txt file typically contains text, for instance. Copying — What if we want to copy a file, instead of simply renaming or moving it? Use cp command (an abbreviated name for “copy”). This command has two different uses that work in the same way as mv : - Copy to same directory (copied file is renamed): cp file newFilename - Copy to other directory (copied file retains original name): cp file directory Let’s try this out. $ cp newname.testfile copy.testfile $ ls $ cp newname.testfile .. $ cd .. $ ls newname.testfile copy.testfile files documents newname.testfile Removing files We’ve begun to clutter up our workspace with all of the directories and files we’ve been making. Let’s learn how to get rid of them. One important note before we start… when you delete a file on UNIX systems, they are gone forever. There is no “recycle bin” or “trash”. Once a file is deleted, it is gone, never to return. So be very careful when deleting files. Files are deleted with rm file [moreFiles] ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,6,"To delete the newname.testfile in our current directory: $ ls $ rm newname.testfile $ ls files Documents newname.testfile files Documents That was simple enough. Directories are deleted in a similar manner using rm -r (the -r option stands for ‘recursive’). $ ls $ rm -r Documents $ rm -r files $ ls files Documents rmdir: failed to remove `files/': Directory not empty files What happened? As it turns out, rmdir is unable to remove directories that have stuff in them. To delete a directory and everything inside it, we will use a special variant of rm , rm -rf directory . This is probably the scariest command on UNIX- it will force delete a directory and all of its contents without prompting. ALWAYS double check your typing before using it… if you leave out the arguments, it will attempt to delete everything on your file system that you have permission to delete. So when deleting directories be very, very careful."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,7,"What happens when you use rm -rf accidentallySteam is a major online sales platform for PC video games with over 125 million users. Despite this, it hasn’t always had the most stable or error-free code. In January 2015, user kevyin on GitHub reported that Steam’s Linux client had deleted every file on his computer. It turned out that one of the Steam programmers had added the following line: rm -rf ""$STEAMROOT/""* . Due to the way that Steam was set up, the variable$STEAMROOT was never initialized, meaning the statement evaluated torm -rf /* . This coding error in the Linux client meant that Steam deleted every single file on a computer when run in certain scenarios (including connected external hard drives). Moral of the story: be very careful when usingrm -rf ! Looking at files Sometimes it’s not practical to read an entire file with cat - the file might be way too large, take a long time to open, or maybe we want to only look at a certain part of the file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,8,"As an example, we are going to look at a large and complex file type used in bioinformatics- a .gtf file. The GTF2 format is commonly used to describe the location of genetic features in a genome. Let’s grab and unpack a set of demo files for use later. To do this, we’ll use wget (wget link downloads a file from a link). $ wget https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz Problems with wget ? wget is a stand-alone application for downloading things over HTTP/HTTPS and FTP/FTPS connections, and it does the job admirably — when it is installed.Some operating systems instead come with cURL, which is the command-line interface to libcurl , a powerful library for programming interactions with remote resources over a wide variety of network protocols."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,9,"If you havecurl but notwget , then try this command instead:$ curl -O https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz For very large downloads, you might consider using Aria2, which has support for downloading the same file from multiple mirrors. You have to install it separately, but if you have it, try this to get it faster than your neighbors: $ aria2c https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz Install cURL - macOS: curl is pre-installed on macOS. If you must have the latest version you canbrew install it, but only do so if the stock version has failed you.Windows: curl comes preinstalled for the Windows 10 command line. For earlier Windows systems, you can download the executable directly; run it in place. curl comes preinstalled in Git for Windows and Windows Subsystem for Linux. On Cygwin, run the setup program again and select thecurl package to install it.- Linux: curl is packaged for every major distribution."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,10,"You can install it through the usual means. - Debian, Ubuntu, Mint: sudo apt install curl - CentOS, Red Hat: sudo yum install curl orzypper install curl - Fedora: sudo dnf install curl Install Aria2 - macOS: aria2c is available through a homebrew.brew install aria2 .- Windows: download the latest release and run aria2c in place. If you’re using the Windows Subsystem for Linux,- Linux: every major distribution has an aria2 package. Install it by the usual means.- Debian, Ubuntu, Mint: sudo apt install aria2 - CentOS, Red Hat: sudo yum install aria2 orzypper install aria2 - Fedora: sudo dnf install aria2 You’ll commonly encounter .tar.gz archives while working in UNIX."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,11,"To extract the files from a .tar.gz file, we run the command tar -xvf filename.tar.gz : $ tar -xvf bash-lesson.tar.gz dmel-all-r6.19.gtf dmel_unique_protein_isoforms_fb_2016_01.tsv gene_association.fb SRR307023_1.fastq SRR307023_2.fastq SRR307024_1.fastq SRR307024_2.fastq SRR307025_1.fastq SRR307025_2.fastq SRR307026_1.fastq SRR307026_2.fastq SRR307027_1.fastq SRR307027_2.fastq SRR307028_1.fastq SRR307028_2.fastq SRR307029_1.fastq SRR307029_2.fastq SRR307030_1.fastq SRR307030_2.fastq Unzipping files We just unzipped a .tar.gz file for this example. What if we run into other file formats that we need to unzip? Just use the handy reference below: gunzip extracts the contents of .gz filesunzip extracts the contents of .zip filestar -xvf extracts the contents of .tar.gz and .tar.bz2 files That is a lot of files! One of these files, dmel-all-r6.19.gtf is extremely large, and contains every annotated feature in the Drosophila melanogaster genome."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,12,"It’s a huge file- what happens if we run cat on it? (Press Ctrl + C to stop it). So, cat is a really bad option when reading big files… it scrolls through the entire file far too quickly! What are the alternatives? Try all of these out and see which ones you like best! head file : Print the top 10 lines in a file to the console. You can control the number of lines you see with the-n numberOfLines flag.tail file : Same ashead , but prints the last 10 lines in a file to the console.less file : Opens a file and display as much as possible on-screen. You can scroll withEnter or the arrow keys on your keyboard. Pressq to close the viewer. Out of cat , head , tail , and less , which method of reading files is your favourite? Why?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_03-files_index.html.html,13,Key Points Use nano to create or edit text files from a terminal.Use cat file1 [file2 ...] to print the contents of one or more files to the terminal.Use mv old dir to move a file or directoryold to another directorydir .Use mv old new to rename a file or directoryold to anew name.Use cp old new to copy a file under a new name or location.Use cp old dir copies a fileold into a directorydir .Use rm old to delete (remove) a file.File extensions are entirely arbitrary on UNIX systems.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_04-wildcards-pipes_index.html.html,0,"Wildcards and pipes Overview Teaching: 45 min Exercises: 10 minQuestions How can I run a command on multiple files at once? Is there an easy way of saving a command’s output? Objectives Redirect a command’s output to a file. Process a file instead of keyboard input using redirection. Construct command pipelines with two or more stages. Explain what usually happens if a program or pipeline isn’t given any input to process. Required files If you didn’t get them in the last lesson, make sure to download the example files used in the next few sections: Using wget: wget https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz Using a web browser: https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz Now that we know some of the basic UNIX commands, we are going to explore some more advanced features. The first of these features is the wildcard * . In our examples before, we’ve done things to files one at a time and otherwise had to specify things explicitly."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_04-wildcards-pipes_index.html.html,1,"The * character lets us speed things up and do things across multiple files. Ever wanted to move, delete, or just do “something” to all files of a certain type in a directory? * lets you do that, by taking the place of one or more characters in a piece of text. So *.txt would be equivalent to all .txt files in a directory for instance. * by itself means all files. Let’s use our example data to see what I mean. $ tar xvf bash-lesson.tar.gz $ ls bash-lesson.tar.gz SRR307026_1.fastq dmel-all-r6.19.gtf SRR307026_2.fastq dmel_unique_protein_isoforms_fb_2016_01.tsv SRR307027_1.fastq gene_association.fb SRR307027_2.fastq SRR307023_1.fastq SRR307028_1.fastq SRR307023_2.fastq SRR307028_2.fastq SRR307024_1.fastq SRR307029_1.fastq SRR307024_2.fastq SRR307029_2.fastq SRR307025_1.fastq SRR307030_1.fastq SRR307025_2.fastq SRR307030_2.fastq Now we have a whole bunch of example files in our directory. For this example we are going to learn a new command that tells us how long a file is: wc ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_04-wildcards-pipes_index.html.html,2,"wc -l file tells us the length of a file in lines. $ wc -l dmel-all-r6.19.gtf 542048 dmel-all-r6.19.gtf Interesting, there are over 540000 lines in our dmel-all-r6.19.gtf file. What if we wanted to run wc -l on every .fastq file? This is where * comes in really handy! *.fastq would match every file ending in .fastq . $ wc -l *.fastq 20000 SRR307023_1.fastq 20000 SRR307023_2.fastq 20000 SRR307024_1.fastq 20000 SRR307024_2.fastq 20000 SRR307025_1.fastq 20000 SRR307025_2.fastq 20000 SRR307026_1.fastq 20000 SRR307026_2.fastq 20000 SRR307027_1.fastq 20000 SRR307027_2.fastq 20000 SRR307028_1.fastq 20000 SRR307028_2.fastq 20000 SRR307029_1.fastq 20000 SRR307029_2.fastq 20000 SRR307030_1.fastq 20000 SRR307030_2.fastq 320000 total That was easy. What if we wanted to do the same command, except on every file in the directory? A nice trick to keep in mind is that * by itself matches every file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_04-wildcards-pipes_index.html.html,3,$ wc -l * 53037 bash-lesson.tar.gz 542048 dmel-all-r6.19.gtf 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv 106290 gene_association.fb 20000 SRR307023_1.fastq 20000 SRR307023_2.fastq 20000 SRR307024_1.fastq 20000 SRR307024_2.fastq 20000 SRR307025_1.fastq 20000 SRR307025_2.fastq 20000 SRR307026_1.fastq 20000 SRR307026_2.fastq 20000 SRR307027_1.fastq 20000 SRR307027_2.fastq 20000 SRR307028_1.fastq 20000 SRR307028_2.fastq 20000 SRR307029_1.fastq 20000 SRR307029_2.fastq 20000 SRR307030_1.fastq 20000 SRR307030_2.fastq 1043504 total Multiple wildcards You can even use multiple * s at a time. How would you runwc -l on every file with “fb” in it?Solution wc -l *fb* i.e. anything or nothing then fb then anything or nothing Using other commands Now let’s try cleaning up our working directory a bit.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_04-wildcards-pipes_index.html.html,4,"Create a folder called “fastq” and move all of our .fastq files there in one mv command.Solution mkdir fastq mv *.fastq fastq/ Redirecting output Each of the commands we’ve used so far does only a very small amount of work. However, we can chain these small UNIX commands together to perform otherwise complicated actions! For our first foray into piping, or redirecting output, we are going to use the > operator to write output to a file. When using > , whatever is on the left of the > is written to the filename you specify on the right of the arrow. The actual syntax looks like command > filename . Let’s try several basic usages of > . echo simply prints back, or echoes whatever you type after it. $ echo ""this is a test"" $ echo ""this is a test"" > test.txt $ ls $ cat test.txt this is a test bash-lesson.tar.gz fastq dmel-all-r6.19.gtf gene_association.fb dmel_unique_protein_isoforms_fb_2016_01.tsv test.txt this is a test Awesome, let’s try that with a more complicated command, like wc -l ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_04-wildcards-pipes_index.html.html,5,"$ wc -l * > word_counts.txt $ cat word_counts.txt wc: fastq: Is a directory 53037 bash-lesson.tar.gz 542048 dmel-all-r6.19.gtf 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv 0 fastq 106290 gene_association.fb 1 test.txt 723505 total Notice how we still got some output to the console even though we “piped” the output to a file? Our expected output still went to the file, but how did the error message get skipped and not go to the file? This phenomena is an artefact of how UNIX systems are built. There are 3 input/output streams for every UNIX program you will run: stdin , stdout , and stderr . Let’s dissect these three streams of input/output in the command we just ran: wc -l * > word_counts.txt stdin is the input to a program. In the command we just ran,stdin is represented by* , which is simply every filename in our current directory.stdout contains the actual, expected output."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_04-wildcards-pipes_index.html.html,6,"In this case,> redirectedstdout to the fileword_counts.txt .stderr typically contains error messages and other information that doesn’t quite fit into the category of “output”. If we insist on redirecting bothstdout andstderr to the same file we would use&> instead of> . (We can redirect juststderr using2> .) Knowing what we know now, let’s try re-running the command, and send all of the output (including the error message) to the same word_counts.txt files as before. $ wc -l * &> word_counts.txt Notice how there was no output to the console that time. Let’s check that the error message went to the file like we specified. $ cat word_counts.txt 53037 bash-lesson.tar.gz 542048 dmel-all-r6.19.gtf 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv wc: fastq: Is a directory 0 fastq 106290 gene_association.fb 1 test.txt 7 word_counts.txt 723512 total Success! The wc: fastq: Is a directory error message was written to the file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_04-wildcards-pipes_index.html.html,7,"Also, note how the file was silently overwritten by directing output to the same place as before. Sometimes this is not the behaviour we want. How do we append (add) to a file instead of overwriting it? Appending to a file is done the same was as redirecting output. However, instead of > , we will use >> . $ echo ""We want to add this sentence to the end of our file"" >> word_counts.txt $ cat word_counts.txt 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv 471308 Drosophila_melanogaster.BDGP5.77.gtf 0 fastq 1304914 fb_synonym_fb_2016_01.tsv 106290 gene_association.fb 1 test.txt 1904642 total We want to add this sentence to the end of our file Chaining commands together We now know how to redirect stdout and stderr to files. We can actually take this a step further and redirect output (stdout ) from one command to serve as the input (stdin ) for the next. To do this, we use the | (pipe) operator. grep is an extremely useful command. It finds things for us within files."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_04-wildcards-pipes_index.html.html,8,"Basic usage (there are a lot of options for more clever things, see the man page) uses the syntax grep whatToFind fileToSearch . Let’s use grep to find all of the entries pertaining to the Act5C gene in Drosophila melanogaster. $ grep Act5C dmel-all-r6.19.gtf The output is nearly unintelligible since there is so much of it. Let’s send the output of that grep command to head so we can just take a peek at the first line. The | operator lets us send output from one command to the next: $ grep Act5C dmel-all-r6.19.gtf | head -n 1 X FlyBase gene 5900861 5905399 . + . gene_id ""FBgn0000042""; gene_symbol ""Act5C""; Nice work, we sent the output of grep to head . Let’s try counting the number of entries for Act5C with wc -l . We can do the same trick to send grep ’s output to wc -l : $ grep Act5C dmel-all-r6.19.gtf | wc -l 46 Note that this is just the same as redirecting output to a file, then reading the number of lines from that file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_04-wildcards-pipes_index.html.html,9,"Writing commands using pipes How many files are there in the “fastq” directory we made earlier? (Use the shell to do this.) Solution ls fastq/ | wc -l Output of ls is one line per item, when chaining commands together like this, so counting lines gives the number of files. Reading from compressed files Let’s compress one of our files using gzip. $ gzip gene_association.fb zcat acts likecat , except that it can read information from.gz (compressed) files. Usingzcat , can you write a command to take a look at the top few lines of thegene_association.fb.gz file (without decompressing the file itself)?Solution zcat gene_association.fb.gz | head The head command without any options shows the first 10 lines of a file. Key Points The * wildcard is used as a placeholder to match any text that follows a pattern.Redirect a command’s output to a file with > .Commands can be chained with |"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,0,"Scripts, variables, and loops Overview Teaching: 45 min Exercises: 10 minQuestions How do I turn a set of commands into a program? Objectives Write a shell script Understand and manipulate UNIX permissions Understand shell variables and how to use them Write a simple “for” loop. We now know a lot of UNIX commands! Wouldn’t it be great if we could save certain commands so that we could run them later or not have to type them out again? As it turns out, this is straightforward to do. A “shell script” is essentially a text file containing a list of UNIX commands to be executed in a sequential manner. These shell scripts can be run whenever we want, and are a great way to automate our work. Writing a Script So how do we write a shell script, exactly? It turns out we can do this with a text editor. Start editing a file called “demo.sh” (to recap, we can do this with nano demo.sh )."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,1,"The “.sh” is the standard file extension for shell scripts that most people use (you may also see “.bash” used). Our shell script will have two parts: - On the very first line, add #!/bin/bash . The#! (pronounced “hash-bang”) tells our computer what program to run our script with. In this case, we are telling it to run our script with our command-line shell (what we’ve been doing everything in so far). If we wanted our script to be run with something else, like Perl, we could add#!/usr/bin/perl - Now, anywhere below the first line, add echo ""Our script worked!"" . When our script runs,echo will happily print outOur script worked! . Our file should now look like this: #!/bin/bash echo ""Our script worked!"" Ready to run our program? Let’s try running it: $ demo.sh bash: demo.sh: command not found... Strangely enough, Bash can’t find our script. As it turns out, Bash will only look in certain directories for scripts to run. To run anything else, we need to tell Bash exactly where to look."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,2,"To run a script that we wrote ourselves, we need to specify the full path to the file, followed by the filename. We could do this one of two ways: either with our absolute path /home/yourUserName/demo.sh , or with the relative path ./demo.sh . $ ./demo.sh bash: ./demo.sh: Permission denied There’s one last thing we need to do. Before a file can be run, it needs “permission” to run. Let’s look at our file’s permissions with ls -l : $ ls -l -rw-rw-r-- 1 yourUsername tc001 12534006 Jan 16 18:50 bash-lesson.tar.gz -rw-rw-r-- 1 yourUsername tc001 40 Jan 16 19:41 demo.sh -rw-rw-r-- 1 yourUsername tc001 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 yourUsername tc001 721242 Jan 25 2016 dmel_unique_protein_is..."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,3,"drwxrwxr-x 2 yourUsername tc001 4096 Jan 16 19:16 fastq -rw-r--r-- 1 yourUsername tc001 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 yourUsername tc001 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 yourUsername tc001 245 Jan 16 19:24 word_counts.txt That’s a huge amount of output: a full listing of everything in the directory. Let’s see if we can understand what each field of a given row represents, working left to right. - Permissions: On the very left side, there is a string of the characters d ,r ,w ,x , and- . Thed indicates if something is a directory (there is a- in that spot if it is not a directory). The otherr ,w ,x bits indicate permission to Read, Write, and eXecute a file. There are three fields ofrwx permissions following the spot ford . If a user is missing a permission to do something, it’s indicated by a- .- The first set of rwx are the permissions that the owner has (in this case the owner isyourUsername )."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,4,"- The second set of rwx s are permissions that other members of the owner’s group share (in this case, the group is namedtc001 ). - The third set of rwx s are permissions that anyone else with access to this computer can do with a file. Though files are typically created with read permissions for everyone, typically the permissions on your home directory prevent others from being able to access the file in the first place. - The first set of - References: This counts the number of references (hard links) to the item (file, folder, symbolic link or “shortcut”). - Owner: This is the username of the user who owns the file. Their permissions are indicated in the first permissions field. - Group: This is the user group of the user who owns the file. Members of this user group have permissions indicated in the second permissions field. - Size of item: This is the number of bytes in a file, or the number of filesystem blocks occupied by the contents of a folder."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,5,"(We can use the -h option here to get a human-readable file size in megabytes, gigabytes, etc.) - Time last modified: This is the last time the file was modified. - Filename: This is the filename. So how do we change permissions? As I mentioned earlier, we need permission to execute our script. Changing permissions is done with chmod . To add executable permissions for all users we could use this: $ chmod +x demo.sh $ ls -l -rw-rw-r-- 1 yourUsername tc001 12534006 Jan 16 18:50 bash-lesson.tar.gz -rwxrwxr-x 1 yourUsername tc001 40 Jan 16 19:41 demo.sh -rw-rw-r-- 1 yourUsername tc001 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 yourUsername tc001 721242 Jan 25 2016 dmel_unique_protein_is..."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,6,"drwxrwxr-x 2 yourUsername tc001 4096 Jan 16 19:16 fastq -rw-r--r-- 1 yourUsername tc001 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 yourUsername tc001 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 yourUsername tc001 245 Jan 16 19:24 word_counts.txt Now that we have executable permissions for that file, we can run it. $ ./demo.sh Our script worked! Fantastic, we’ve written our first program! Before we go any further, let’s learn how to take notes inside our program using comments. A comment is indicated by the # character, followed by whatever we want. Comments do not get run. Let’s try out some comments in the console, then add one to our script! # This won't show anything. Now lets try adding this to our script with nano . Edit your script to look something like this: #!/bin/bash # This is a comment... they are nice for making notes! echo ""Our script worked!"" When we run our script, the output should be unchanged from before!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,7,"Setting execute permission with chmod In Unix, a file has three basic permissions, each of which can be set for three levels of user. The permissions are: - Read permission (“r”) - numeric value 4. - Write permission (“w”) - numeric value 2. - Execute permission (“x”) - numeric value 1. When applied to a directory, execute permission refers to whether the directory can be entered with ‘cd’. The three levels of user are: - The user who owns the file (the “user”, referred to with “u”). - The group to which the file belongs - referred to with “g”. Each user has a primary group and is optionally a member of other groups. When a user creates a file, it is normally associated with the user’s primary group. At NYU HPC, all users are in a group named ‘users’, so group permission has little meaning. - All other users are referred to with “o”. You grant permissions with chmod who+what file and revoke them with chmod who-what file . (Notice that the first has “+” and the second “-“)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,8,"Here, “who” is some combination of “u”, “g”, and “o”, and “what” is some combination of “r”, “w”, and “x”. So, to set execute permission, as in the example above, we use: $ chmod u+x my_script Shell variables One important concept that we’ll need to cover are shell variables. Variables are a great way of saving information under a name you can access later. In programming languages like Python and R, variables can store pretty much anything you can think of. In the shell, they usually just store text. The best way to understand how they work is to see them in action. To set a variable, simply type in a name containing only letters, numbers, and underscores, followed by an = and whatever you want to put in the variable. Shell variable names are often uppercase by convention (but do not have to be). $ VAR=""This is our variable"" To use a variable, prefix its name with a $ sign."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,9,"Note that if we want to simply check what a variable is, we should use echo (or else the shell will try to run the contents of a variable). $ echo $VAR This is our variable Let’s try setting a variable in our script and then recalling its value as part of a command. We’re going to make it so our script runs wc -l on whichever file we specify with FILE . Our script: #!/bin/bash # set our variable to the name of our GTF file FILE=dmel-all-r6.19.gtf # call wc -l on our file wc -l $FILE $ ./demo.sh 542048 dmel-all-r6.19.gtf What if we wanted to do our little wc -l script on other files without having to change $FILE every time we want to use it? There is actually a special shell variable we can use in scripts that allows us to use arguments in our scripts (arguments are extra information that we can pass to our script, like the -l in wc -l ). To use the first argument to a script, use $1 (the second argument is $2 , and so on). Let’s change our script to run wc -l on $1 instead of $FILE ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,10,"Note that we can also pass all of the arguments using $@ (not going to use it in this lesson, but it’s something to be aware of). Our script: #!/bin/bash # call wc -l on our first argument wc -l $1 $ ./demo.sh dmel_unique_protein_isoforms_fb_2016_01.tsv 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv Nice! One thing to be aware of when using variables: they are all treated as pure text. How do we save the output of an actual command like ls -l ? A demonstration of what doesn’t work: $ TEST=ls -l -bash: -l: command not found What does work (we need to surround any command with $(command) ): $ TEST=$(ls -l) $ echo $TEST total 90372 -rw-rw-r-- 1 jeff jeff 12534006 Jan 16 18:50 bash-lesson.tar.gz -rwxrwxr-x. 1 jeff jeff 40 Jan 1619:41 demo.sh -rw-rw-r-- 1 jeff jeff 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 jeff jeff 721242 Jan 25 2016 dmel_unique_protein_isoforms_fb_2016_01.tsv drwxrwxr-x."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,11,"2 jeff jeff 4096 Jan 16 19:16 fastq -rw-r--r-- 1 jeff jeff 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 jeff jeff 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 jeff jeff 245 Jan 16 19:24 word_counts.txt Note that everything got printed on the same line. This is a feature, not a bug, as it allows us to use $(commands) inside lines of script without triggering line breaks (which would end our line of code and execute it prematurely). Loops To end our lesson on scripts, we are going to learn how to write a for-loop to execute a lot of commands at once. This will let us do the same string of commands on every file in a directory (or other stuff of that nature). for-loops generally have the following syntax: #!/bin/bash for VAR in first second third do echo $VAR done When a for-loop gets run, the loop will run once for everything following the word in . In each iteration, the variable $VAR is set to a particular value for that iteration."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,12,"In this case it will be set to first during the first iteration, second on the second, and so on. During each iteration, the code between do and done is performed. Let’s run the script we just wrote (I saved mine as loop.sh ). $ chmod +x loop.sh $ ./loop.sh first second third What if we wanted to loop over a shell variable, such as every file in the current directory? Shell variables work perfectly in for-loops."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,13,"In this example, we’ll save the result of ls and loop over each file: #!/bin/bash FILES=$(ls) for VAR in $FILES do echo $VAR done $ ./loop.sh bash-lesson.tar.gz demo.sh dmel_unique_protein_isoforms_fb_2016_01.tsv dmel-all-r6.19.gtf fastq gene_association.fb.gz loop.sh test.txt word_counts.txt There’s a shortcut to run on all files of a particular type, say all .gz files: #!/bin/bash for VAR in *.gz do echo $VAR done bash-lesson.tar.gz gene_association.fb.gz Writing our own scripts and loops cd to ourfastq directory from earlier and write a loop to print off the name and top 4 lines of every fastq file in that directory.Is there a way to only run the loop on fastq files ending in _1.fastq ?Solution Create the following script in a file called head_all.sh #!/bin/bash for FILE in *.fastq do echo $FILE head -n 4 $FILE done The “for” line could be modified to be for FILE in *_1.fastq to achieve the second aim. Concatenating variables Concatenating (i.e."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,14,"mashing together) variables is quite easy to do. Add whatever you want to concatenate to the beginning or end of the shell variable after enclosing it in {} characters.FILE=stuff.txt echo ${FILE}.example stuff.txt.example Can you write a script that prints off the name of every file in a directory with “.processed” added to it? Solution Create the following script in a file called process.sh #!/bin/bash for FILE in * do echo ${FILE}.processed done Note that this will also print directories appended with “.processed”. To truly only get files and not directories, we need to modify this to use the find command to give us only files in the current directory:#!/bin/bash for FILE in $(find . -max-depth 1 -type f) do echo ${FILE}.processed done but this will have the side-effect of listing hidden files too. Special permissions What if we want to give different sets of users different permissions. chmod actually accepts special numeric codes instead of stuff likechmod +x ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_05-scripts_index.html.html,15,"The numeric codes are as follows: read = 4, write = 2, execute = 1. For each user we will assign permissions based on the sum of these permissions (must be between 7 and 0).Let’s make an example file and give everyone permission to do everything with it. touch example ls -l example chmod 777 example ls -l example How might we give ourselves permission to do everything with a file, but allow no one else to do anything with it. Solution chmod 700 example We want all permissions so: 4 (read) + 2 (write) + 1 (execute) = 7 for user (first position), no permissions, i.e. 0, for group (second position) and all (third position). Key Points A shell script is just a list of bash commands in a text file. To make a shell script file executable, run chmod +x script.sh ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_about_index.html.html,0,"The Carpentries comprises Software Carpentry, Data Carpentry, and Library Carpentry communities of Instructors, Trainers, Maintainers, helpers, and supporters who share a mission to teach foundational coding and data science skills to researchers and people working in library- and information-related roles. In January, 2018, The Carpentries was formed by the merger of Software Carpentry and Data Carpentry. Library Carpentry became an official Carpentries Lesson Program in November 2018. While individual lessons and workshops continue to be run under each lesson project, The Carpentries provide overall staffing and governance, as well as support for assessment, instructor training and mentoring. Memberships are joint, and the Carpentries project maintains a shared Code of Conduct. The Carpentries is a fiscally sponsored project of Community Initiatives, a registered 501(c)3 non-profit based in California, USA."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_about_index.html.html,1,"Since 1998, Software Carpentry has been teaching researchers across all disciplines the foundational coding skills they need to get more done in less time and with less pain. Its volunteer instructors have run hundreds of events for thousands of learners around the world. Now that all research involves some degree of computational work, whether with big data, cloud computing, or simple task automation, these skills are needed more than ever. Data Carpentry develops and teaches workshops on the fundamental data skills needed to conduct research. Its target audience is researchers who have little to no prior computational experience, and its lessons are domain specific, building on learners' existing knowledge to enable them to quickly apply skills learned to their own research. Data Carpentry workshops take researchers through the entire data life cycle. Library Carpentry develops lessons and teaches workshops for and with people working in library- and information-related roles."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_about_index.html.html,2,"Its goal is to create an on-ramp to empower this community to use software and data in their own work, as well as be advocates for and train others in efficient, effective and reproducible data and software practices."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,0,"Why Use a Cluster? Overview Teaching: 25 min Exercises: 5 minQuestions Why would I be interested in High Performance Computing (HPC)? What can I expect to learn from this course? Objectives Be able to describe what an HPC system is. Identify how an HPC system could benefit you. Why Use These Computers? What do you need? Talk to your neighbor about your research. How does computing help you do your research? How could more computing help you do more or better research? Frequently, research problems that use computing can outgrow the desktop or laptop computer where they started: - A statistics student wants to do cross-validate their model. This involves running the model 1000 times — but each run takes an hour. Running on their laptop will take over a month! - A genomics researcher has been using small datasets of sequence data, but soon will be receiving a new type of sequencing data that is 10 times as large."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,1,"It’s already challenging to open the datasets on their computer — analyzing these larger datasets will probably crash it. - An engineer is using a fluid dynamics package that has an option to run in parallel. So far, they haven’t used this option on their desktop, but in going from 2D to 3D simulations, simulation time has more than tripled and it might be useful to take advantage of that feature. In all these cases, what is needed is access to more computers than can be used at the same time. Luckily, large scale computing systems — shared computing resources with lots of computers — are available at many universities, labs, or through national networks. These resources usually have more central processing units(CPUs), CPUs that operate at higher speeds, more memory, more storage, and faster connections with other computer systems. They are frequently called “clusters”, “supercomputers” or resources for “high performance computing” or HPC."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,2,"In this lesson, we will usually use the terminology of HPC and HPC cluster. Using a cluster often has the following advantages for researchers: - Speed. With many more CPU cores, often with higher performance specs, than a typical laptop or desktop, HPC systems can offer significant speed up. - Volume. Many HPC systems have both the processing memory (RAM) and disk storage to handle very large amounts of data. Terabytes of RAM and petabytes of storage are available for research projects. - Efficiency. Many HPC systems operate a pool of resources that are drawn on by many users. In most cases when the pool is large and diverse enough the resources on the system are used almost constantly. - Cost. Bulk purchasing and government funding mean that the cost to the research community for using these systems in significantly less that it would be otherwise. - Convenience. Maybe your calculations just take a long time to run or are otherwise inconvenient to run on your personal computer."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,3,"There’s no need to tie up your own computer for hours when you can use someone else’s instead. This is how a large-scale compute system like a cluster can help solve problems like those listed at the start of the lesson. Thinking ahead How do you think using a large-scale computing system will be different from using your laptop? Talk to your neighbor about some differences you may already know about, and some differences/difficulties you imagine you may run into. On Command Line Using HPC systems often involves the use of a shell through a command line interface (CLI) and either specialized software or programming techniques. The shell is a program with the special role of having the job of running other programs rather than doing calculations or similar tasks itself. What the user types goes into the shell, which then figures out what commands to run and orders the computer to execute them."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,4,"(Note that the shell is called “the shell” because it encloses the operating system in order to hide some of its complexity and make it simpler to interact with.) The most popular Unix shell is Bash, the Bourne Again SHell (so-called because it’s derived from a shell written by Stephen Bourne). Bash is the default shell on most modern implementations of Unix and in most packages that provide Unix-like tools for Windows. Interacting with the shell is done via a command line interface (CLI) on most HPC systems. In the earliest days of computers, the only way to interact with early computers was to rewire them. From the 1950s to the 1980s most people used line printers. These devices only allowed input and output of the letters, numbers, and punctuation found on a standard keyboard, so programming languages and software interfaces had to be designed around that constraint and text-based interfaces were the way to do this."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,5,"A typing-based interface is often called a command-line interface, or CLI, to distinguish it from a graphical user interface, or GUI, which most people now use. The heart of a CLI is a read-evaluate-print loop, or REPL: when the user types a command and then presses the Enter (or Return) key, the computer reads it, executes it, and prints its output. The user then types another command, and so on until the user logs off. Learning to use Bash or any other shell sometimes feels more like programming than like using a mouse. Commands are terse (often only a couple of characters long), their names are frequently cryptic, and their output is lines of text rather than something visual like a graph. However, using a command line interface can be extremely powerful, and learning how to use one will allow you to reap the benefits described above. The rest of this lesson The only way to use these types of resources is by learning to use the command line."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,6,"This introduction to HPC systems has two parts: - We will learn to use the UNIX command line (also known as Bash). - We will use our new Bash skills to connect to and operate a high-performance computing supercomputer. The skills we learn here have other uses beyond just HPC: Bash and UNIX skills are used everywhere, be it for web development, running software, or operating servers. It’s become so essential that Microsoft now ships it as part of Windows! Knowing how to use Bash and HPC systems will allow you to operate virtually any modern device. With all of this in mind, let’s connect to a cluster and get started! Key Points High Performance Computing (HPC) typically involves connecting to very large computing systems elsewhere in the world. These HPC systems can be used to do work that would either be impossible or much slower or smaller systems. The standard method of interacting with such systems is via a command line interface such as Bash."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,7,"Connecting to the remote HPC system Overview Teaching: 25 min Exercises: 10 minQuestions How do I open a terminal? How do I connect to a remote computer? What is an SSH key? Objectives Connect to a remote HPC system. Prerequisites To access the Greene HPC cluster, you must be connected to the NYU network. If you are physically on campus and connected via a wired connection in your office or through NYU’s WiFi, you can directly SSH into the clusters without any additional steps. However, if you are off-campus or working remotely, connecting through the NYU VPN or using the gateway servers is required to establish a secure connection to the HPC systems. Remote Connections with the NYU VPN & HPC Gateway Server If you are connecting from a remote location that is not on the NYU network (your home for example), you have two options: - VPN Option: set up your computer to use the NYU VPN. Once you’ve created a VPN connection, you can proceed as if you were connected to the NYU net."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,8,"- Gateway Option: go through our gateway servers (example below). Gateways are designed to support only a very minimal set of commands and their only purpose is to let users connect HPC systems without needing to first connect to the VPN. Log into the Greene Cluster NYU Campus: From within the NYU network, that is, from an on-campus location, or after you VPN inside NYU’s network, you can login to the HPC clusters directly. Off-campus: The host name of Greene is ‘greene.hpc.nyu.edu’. Logging in to Greene is the two-stage process. The HPC clusters (Greene) are not directly visible to the internet (outside the NYU Network). If you are outside NYU’s Network (off-campus) you must first login to a bastion host named gw.hpc.nyu.edu. From within the NYU network, that is, from an on-campus location, or after you VPN inside NYU’s network, you can log in to the HPC clusters directly. You do not need to log in to the bastion host."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,9,"To log in to the HPC cluster (Greene), simply use: ssh <NYUNetID>@greene.hpc.nyu.edu For access from Windows stations using PuTTY, please click here. To connect to VPN from Linux/MAC, please click here. From an off-campus location (outside NYU-NET), logging in to the HPC clusters is a two-step process: - First, log in to the bastion host, gw.hpc.nyu.edu . From a Mac or Linux workstation, this is a simple terminal command (replace<NYUNetID> with your NetID). Your password is the same password you use for NYU Home: ssh <NYUNetID>@gw.hpc.nyu.edu Windows users will need to use PuTTY, see here for instructions. - Next, log in to the cluster. For Greene, this is done with: ssh <NYUNetID>@greene.hpc.nyu.edu Opening a Terminal Accessing the Greene HPC cluster is primarily done through the Command Line Interface (CLI). A CLI provides a text-based environment that allows users to manage files, run programs, and navigate directories via command input."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,10,"On macOS, the built-in CLI tool is Terminal, while Windows 10 users can leverage the Windows Subsystem for Linux (WSL) for similar functionality. Additionally, a popular tool for connecting to Linux servers from Windows is PuTTY, a free SSH client. Connecting to an HPC system is most often done through a tool known as “SSH” (Secure SHell) and usually SSH is run through a terminal. So, to begin using an HPC system we need to begin by opening a terminal. Different operating systems have different terminals, none of which are exactly the same in terms of their features and abilities while working on the operating system. When connected to the remote system the experience between terminals will be identical as each will faithfully present the same experience of using that system. Here is the process for opening a terminal in each operating system. Linux There are many different versions (aka “flavours”) of Linux and how to open a terminal window can change between flavours."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,11,"Fortunately most Linux users already know how to open a terminal window since it is a common part of the workflow for Linux users. If this is something that you do not know how to do then a quick search on the Internet for “how to open a terminal window in” with your particular Linux flavour appended to the end should quickly give you the directions you need. To connect to the gateway servers, simply open a terminal application and enter the following command: ssh <NetID>@gw.hpc.nyu.edu After typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to one of the HPC clusters: # this will connect you to Greene HPC cluster ssh <NetID>@greene.hpc.nyu.edu Mac Macs have had a terminal built in since the first version of OS X since it is built on a UNIX-like operating system, leveraging many parts from BSD (Berkeley Software Distribution). The terminal can be quickly opened through the use of the Searchlight tool."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,12,"Hold down the command key and press the spacebar. In the search bar that shows up type “terminal”, choose the terminal app from the list of results (it will look like a tiny, black computer screen) and you will be presented with a terminal window. Alternatively, you can find Terminal under “Utilities” in the Applications menu. To connect to the gateway servers, simply open a terminal application and enter the following command: ssh <NetID>@gw.hpc.nyu.edu After typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to one of the HPC clusters: # this will connect you to Greene HPC cluster ssh <NetID>@greene.hpc.nyu.edu Windows While Windows does have a command-line interface known as the “Command Prompt” that has its roots in MS-DOS (Microsoft Disk Operating System) it does not have an SSH tool built into it and so one needs to be installed."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,13,"There are a variety of programs that can be used for this; a few common ones we describe here, as follows: Git BASH Git BASH gives you a terminal like interface in Windows. You can use this to connect to a remote computer via SSH. It can be downloaded for free from here. Windows Subsystem for Linux The Windows Subsystem for Linux also allows you to connect to a remote computer via SSH. Instructions on installing it can be found here. MobaXterm MobaXterm is a terminal window emulator for Windows and the home edition can be downloaded for free from mobatek.net. If you follow the link you will note that there are two editions of the home version available: Portable and Installer. The portable edition puts all MobaXterm content in a folder on the desktop (or anywhere else you would like it) so that it is easy to add plug-ins or remove the software. The installer edition adds MobaXterm to your Windows installation and menu as any other program you might install."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,14,"If you are not sure that you will continue to use MobaXterm in the future, the portable edition is likely the best choice for you. MobaKeyGen, see the MoabXterm documentation Download the version that you would like to use and install it as you would any other software on your Windows installation. Once the software is installed you can run it by either opening the folder installed with the portable edition and double-clicking on the executable file named MobaXterm_Personal_11.1 (your version number may vary) or, if the installer edition was used, finding the executable through either the start menu or the Windows search option. Once the MobaXterm window is open you should see a large button in the middle of that window with the text “Start Local Terminal”. Click this button and you will have a terminal window at your disposal."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,15,"PuTTY It is strictly speaking not necessary to have a terminal running on your local computer in order to access and use a remote system, only a window into the remote system once connected. PuTTY is likely It is, strictly speaking, not necessary to have a terminal running on your local computer in order to access and use a remote system, only a window into the remote system once connected. PuTTY is likely the oldest, most well-known, and widely used software solution to take this approach. PuTTY is available for free download from https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html. Download the version that is correct for your operating system and install it as you would other software on your Windows system. Once installed it will be available through the start menu or similar. puttygen, see the Putty documentation Running PuTTY will not initially produce a terminal but instead a window full of connection options."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,16,Putting the address of the remote system in the “Host Name (or IP Address)” box and either pressing enter or clicking the “Open” button should begin the connection process. If this works you will see a terminal window open that prompts you for a username through the “login as:” prompt and then for a password. If both of these are passed correctly then you will be given access to the system and will see a message saying so within the terminal. If you need to escape the authentication process you can hold the Control (Ctrl) key and press the c key to exit and start again. Note that you may want to paste in your password rather than typing it. Use Ctrl plus a right-click of the mouse to paste content from the clipboard to the PuTTY terminal. For those logging in with PuTTY it would likely be best to cover the terminal basics already mentioned above before moving on to navigating the remote system.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,17,"Open OnDemand (Web-based Graphical User Interface) Open OnDemand is an open source project funded by the National Science Foundation (NSF). Open OnDemand is designed to create easier access to users to interface with HPC systems. Originally developed by Ohio Supercomputer Center (OSC), used by many universities around the world, and now servicing the NYU Greene HPC cluster. Open OnDemand has a variety of convenient tools to manage files, access the command line, manage and monitor jobs, and launch interactive applications, such as Jupyter Notebooks, RStudio sessions, and even full Linux Desktops."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,18,"Features Include: Easy file management - upload and download files, view HTML and pictures without downloading Command-line shell access without any SSH client locally installed Job management and monitoring Full Linux desktop experience without X11 Interactive Apps such as JupyterHub and RStudio without the need for port forwarding Open OnDemand (OOD) is accessible to all users with a valid NYU HPC account while on-campus network or through a VPN. To access OOD visit: https://ood.hpc.nyu.edu (VPN Required) Access the Shell Under the clusters menu you can select the Greene Shell Access option to access the Linux shell. No local SSH client is required. Interactive Applications GUI based applications are accessible without the need for port or X11 forwarding. Select the Interactive Apps menu, select the desired application, and submit the job based on required resources and options."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,19,"Troubleshooting Connections to Open OnDemand A common issue that can occur is receiving an error that the Open OnDemand page cannot be reached. Sometimes this can indicate that the service is down, but often this is an issue with the the local browser cache. You can test this by opening a private browser window and seeing if https://ood.hpc.nyu.edu will load. If it does, try deleting the cache for https://ood.hpc.nyu.edu in your browser history to resolve this issue. In Chrome, this can be done by navigating to this page in your settings: chrome://settings/content/all?searchSubpage=ood.hpc.nyu.edu&search=site+data The link above will automatically search for the Open OnDemand site data and cookies. You can then simply click on the trashcan icon to delete the site cache. Once done, try navigating again to https://ood.hpc.nyu.edu and the site should load. For other issues please email hpc@nyu.edu."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,20,"Creating an SSH key SSH keys are an alternative method for authentication to obtain access to remote computing systems. They can also be used for authentication when transferring files or for accessing version control systems. In this section you will create a pair of SSH keys, a private key which you keep on your own computer and a public key which is placed on the remote HPC system that you will log in to. Linux, Mac and Windows Subsystem for Linux Once you have opened a terminal check for existing SSH keys and filenames since existing SSH keys are overwritten, $ ls ~/.ssh/ then generate a new public-private key pair, $ ssh-keygen -t ed25519 -a 100 -f ~/.ssh/id_Graham_ed25519 -o (no default): use the OpenSSH key format, rather than PEM.-a (default is 16): number of rounds of passphrase derivation; increase to slow down brute force attacks.-t (default is rsa): specify the “type” or cryptographic algorithm."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,21,"ed25519 is faster and shorter than RSA for comparable strength.-f (default is /home/user/.ssh/id_algorithm): filename to store your keys. If you already have SSH keys, make sure you specify a different name:ssh-keygen will overwrite the default key if you don’t specify! If ed25519 is not available, use the older (but strong and trusted) RSA cryptography: $ ls ~/.ssh/ $ ssh-keygen -o -a 100 -t rsa -b 4096 -f ~/.ssh/id_Graham_rsa The flag -b sets the number of bits in the key. The default is 2048. EdDSA uses a fixed key length, so this flag would have no effect. When prompted, enter a strong password that you will remember. Cryptography is only as good as the weakest link, and this will be used to connect to a powerful, precious, computational resource. Take a look in ~/.ssh (use ls ~/.ssh ). You should see the two new files: your private key (~/.ssh/key_Graham_ed25519 or ~/.ssh/key_Graham_rsa ) and the public key (~/.ssh/key_Graham_ed25519.pub or ~/.ssh/key_Graham_rsa.pub )."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,22,"If a key is requested by the system administrators, the public key is the one to provide. Private keys are your private identity A private key that is visible to anyone but you should be considered compromised, and must be destroyed. This includes having improper permissions on the directory it (or a copy) is stored in, traversing any network in the clear, attachment on unencrypted email, and even displaying the key (which is ASCII text) in your terminal window. Protect this key as if it unlocks your front door. In many ways, it does. Further information For more information on SSH security and some of the flags set here, an excellent resource is Secure Secure Shell. Logging onto the system With all of this in mind, let’s connect to a remote HPC system. In this workshop, we will connect to Graham — an HPC system located at the University of Waterloo."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,23,"Although it’s unlikely that every system will be exactly like Graham, it’s a very good example of what you can expect from an HPC installation. To connect to our example computer, we will use SSH (if you are using PuTTY, see above). SSH allows us to connect to UNIX computers remotely, and use them as if they were our own. The general syntax of the connection command follows the format ssh -i ~/.ssh/key_for_remote_computer <NetID>@greene.hpc.nyu.edu when using SSH keys and ssh yourUsername@some.computer.address if only password access is available. Let’s attempt to connect to the HPC system now: ssh -i ~/.ssh/key_Graham_ed25519 yourUsername@graham.computecanada.ca or ssh -i ~/.ssh/key_Graham_rsa yourUsername@graham.computecanada.ca or if SSH keys have not been enabled ssh yourUsername@graham.computecanada.ca The authenticity of host 'graham.computecanada.ca (199.241.166.2)' can't be established. ECDSA key fingerprint is SHA256:JRj286Pkqh6aeO5zx1QUkS8un5fpcapmezusceSGhok."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,24,"ECDSA key fingerprint is MD5:99:59:db:b1:3f:18:d0:2c:49:4e:c2:74:86:ac:f7:c6. Are you sure you want to continue connecting (yes/no)? # type ""yes""! Warning: Permanently added the ECDSA host key for IP address '199.241.166.2' to the list of known hosts. yourUsername@graham.computecanada.ca's password: # no text appears as you enter your password Last login: Wed Jun 28 16:16:20 2017 from s2.n59.queensu.ca Welcome to the ComputeCanada/SHARCNET cluster Graham. If you’ve connected successfully, you should see a prompt like the one below. This prompt is informative, and lets you grasp certain information at a glance. (If you don’t understand what these things are, don’t worry!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,25,We will cover things in depth as we explore the system further.) [yourUsername@gra-login1 ~]$ Telling the Difference between the Local Terminal and the Remote Terminal You may have noticed that the prompt changed when you logged into the remote system using the terminal (if you logged in using PuTTY this will not apply because it does not offer a local terminal). This change is important because it makes it clear on which system the commands you type will be run when you pass them into the terminal. This change is also a small complication that we will need to navigate throughout the workshop. Exactly what is reported before the $ in the terminal when it is connected to the local system and the remote system will typically be different for every user.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,26,"We still need to indicate which system we are entering commands on though so we will adopt the following convention: [local]$ when the command is to be entered on a terminal connected to your local computer[yourUsername@gra-login1 ~]$ when the command is to be entered on a terminal connected to the remote system$ when it really doesn’t matter which system the terminal is connected to. Being certain which system your terminal is connected to If you ever need to be certain which system a terminal you are using is connected to then use the following command: $ hostname . Keep two terminal windows open It is strongly recommended that you have two terminals open, one connected to the local system and one connected to the remote system, that you can switch back and forth between."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,27,"If you only use one terminal window then you will need to reconnect to the remote system using one of the methods above when you see a change from [local]$ to[yourUsername@gra-login1 ~]$ and disconnect when you see the reverse. Key Points To connect to a remote HPC system using SSH and a password, run ssh <NetID>@greene.hpc.nyu.edu .To connect to a remote HPC system using SSH and an SSH key, run ssh -i ~/.ssh/key_for_remote_computer <NetID>@greene.hpc.nyu.edu . Moving around and looking at things Overview Teaching: 15 min Exercises: 5 minQuestions How do I navigate and look around the system? Objectives Learn how to navigate around directories and look at their contents Explain the difference between a file and a directory. Translate an absolute path into a relative path and vice versa. Identify the actual command, flags, and filenames in a command-line call. Demonstrate the use of tab completion, and explain its advantages."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,28,"At this point in the lesson, we’ve just logged into the system. Nothing has happened yet, and we’re not going to be able to do anything until we learn a few basic commands. By the end of this lesson, you will know how to “move around” the system and look at what’s there. System Architecture Files Systems for usage: The NYU HPC clusters have multiple file systems for user’s files. Each file system is configured differently to serve a different purpose. Right now, all we see is something that looks like this: [yourUsername@gra-login1 ~]$ The dollar sign is a prompt, which shows us that the shell is waiting for input; your shell may use a different character as a prompt and may add information before the prompt. When typing commands, either from these lessons or from other sources, do not type the prompt, only the commands that follow it. Type the command whoami , then press the Enter key (sometimes marked Return) to send the command to the shell."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,29,"The command’s output is the ID of the current user, i.e., it shows us who the shell thinks we are: $ whoami <NetID> More specifically, when we type whoami the shell: - finds a program called whoami , - runs that program, - displays that program’s output, then - displays a new prompt to tell us that it’s ready for more commands. Next, let’s find out where we are by running a command called pwd (which stands for “print working directory”). (“Directory” is another word for “folder”). At any moment, our current working directory (where we are) is the directory that the computer assumes we want to run commands in unless we explicitly specify something else. Here, the computer’s response is /home/<NetID> , which is <NetID> home directory. Note that the location of your home directory may differ from system to system. $ pwd /home/<NetID> So, we know where we are. How do we look and see what’s in our current directory?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,30,"$ ls ls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns. Differences between remote and local system Open a second terminal window on your local computer and run the ls command without logging in remotely. What differences do you see?Solution You would likely see something more like this: Applications Documents Library Music Public Desktop Downloads Movies Pictures In addition you should also note that the preamble before the prompt ( $ ) is different. This is very important for making sure you know what system you are issuing commands on when in the shell. If nothing shows up when you run ls , it means that nothing’s there. Let’s make a directory for us to play with. mkdir <new directory name> makes a new directory with that name in your current location."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,31,"Notice that this command required two pieces of input: the actual name of the command (mkdir ) and an argument that specifies the name of the directory you wish to create. $ mkdir documents Let’s us ls again. What do we see? Our folder is there, awesome. What if we wanted to go inside it and do stuff there? We will use the cd (change directory) command to move around. Let’s cd into our new documents folder. $ cd documents $ pwd ~/documents What is the ~ character? When using the shell, ~ is a shortcut that represents /home/<NetID> . Now that we know how to use cd , we can go anywhere. That’s a lot of responsibility. What happens if we get “lost” and want to get back to where we started? To go back to your home directory, the following three commands will work: $ cd /home/<NetID> $ cd ~ $ cd A quick note on the structure of a UNIX (Linux/Mac/Android/Solaris/etc) filesystem. Directories and absolute paths (i.e. exact position in the system) are always prefixed with a / ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,32,"/ by itself is the “root” or base directory. Let’s go there now, look around, and then return to our home directory. $ cd / $ ls $ cd ~ bin dev initrd local mnt proc root scratch tmp work boot etc lib localscratch nix project run srv usr cvmfs home lib64 media opt ram sbin sys var The “home” directory is the one where we generally want to keep all of our files. Other folders on a UNIX OS contain system files, and get modified and changed as you install new software or upgrade your OS. Using HPC filesystems On HPC systems, you have a number of places where you can store your files. These differ in both the amount of space allocated and whether or not they are backed up. File storage locations: - Network filesystem - Your home directory is an example of a network filesystem. Data stored here is available throughout the HPC system and files stored here are often backed up (but check your local configuration to be sure!)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,33,"Files stored here are typically slower to access, the data is actually stored on another computer and is being transmitted and made available over the network! - Scratch - Some systems may offer “scratch” space. Scratch space is typically faster to use than your home directory or network filesystem, but is not usually backed up, and should not be used for long term storage. - Work file system - As an alternative to (or sometimes as well as) Scratch space, some HPC systems offer fast file system access as a work file system. Typically, this will have higher performance than your home directory or network file system and may not be backed up. It differs from scratch space in that files in a work file system are not automatically deleted for you, you must manage the space yourself. - Local scratch (job only) - Some systems may offer local scratch space while executing a job."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,34,"(A job is a program which you submit to run on an HPC system, and will be covered later.) Such storage is very fast, but will be deleted at the end of your job. - Ramdisk (job only) - Some systems may let you store files in a “RAM disk” while running a job, where files are stored directly in the computer’s memory. This extremely fast, but files stored here will count against your job’s memory usage and be deleted at the end of your job. There are several other useful shortcuts you should be aware of. . represents your current directory.. represents the “parent” directory of your current location- While typing nearly anything, you can have bash try to autocomplete what you are typing by pressing the tab key. Let’s try these out now: $ cd ./documents $ pwd $ cd .. $ pwd /home/<NetID>/documents /home/<NetID> Many commands also have multiple behaviours that you can invoke with command line ‘flags.’ What is a flag?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,35,"It’s generally just your command followed by a ‘-‘ and the name of the flag (sometimes it’s ‘–’ followed by the name of the flag). You follow the flag(s) with any additional arguments you might need. We’re going to demonstrate a couple of these “flags” using ls . Show hidden files with -a . Hidden files are files that begin with . , these files will not appear otherwise, but that doesn’t mean they aren’t there! “Hidden” files are not hidden for security purposes, they are usually just config files and other tempfiles that the user doesn’t necessarily need to see all the time. $ ls -a . .. .bash_logout .bash_profile .bashrc documents .emacs .mozilla .ssh Notice how both . and .. are visible as hidden files. Show files, their size in bytes, date last modified, permissions, and other things with -l . $ ls -l drwxr-xr-x 2 <NetID> tc001 4096 Jan 14 17:31 documents This is a lot of information to take in at once, but we will explain this later!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,36,"ls -l is extremely useful, and tells you almost everything you need to know about your files without actually looking at them. We can also use multiple flags at the same time! $ ls -l -a [yourUsername@gra-login1 ~]$ ls -la total 36 drwx--S--- 5 <NetID> tc001 4096 Nov 28 09:58 . drwxr-x--- 3 root tc001 4096 Nov 28 09:40 .. -rw-r--r-- 1 <NetID> tc001 18 Dec 6 2016 .bash_logout -rw-r--r-- 1 <NetID> tc001 193 Dec 6 2016 .bash_profile -rw-r--r-- 1 <NetID> tc001 231 Dec 6 2016 .bashrc drwxr-sr-x 2 <NetID> tc001 4096 Nov 28 09:58 documents -rw-r--r-- 1 <NetID> tc001 334 Mar 3 2017 .emacs drwxr-xr-x 4 <NetID> tc001 4096 Aug 2 2016 .mozilla drwx--S--- 2 <NetID> tc001 4096 Nov 28 09:58 .ssh Flags generally precede any arguments passed to a UNIX command. ls actually takes an extra argument that specifies a directory to look into."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,37,"When you use flags and arguments together, the syntax (how it’s supposed to be typed) generally looks something like this: $ command <flags/options> <arguments> So using ls -l -a on a different directory than the one we’re in would look something like: $ ls -l -a ~/documents drwxr-sr-x 2 <NetID> tc001 4096 Nov 28 09:58 . drwx--S--- 5 <NetID> tc001 4096 Nov 28 09:58 .. Where to go for help? How did I know about the -l and -a options? Is there a manual we can look at for help when we need help? There is a very helpful manual for most UNIX commands: man (if you’ve ever heard of a “man page” for something, this is what it is). $ man ls LS(1) User Commands LS(1) NAME ls - list directory contents SYNOPSIS ls [OPTION]... [FILE]... DESCRIPTION List information about the FILEs (the current directory by default). Sort entries alphabetically if none of -cftuvSUX nor --sort is specified. Mandatory arguments to long options are mandatory for short options too."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,38,"To navigate through the man pages, you may use the up and down arrow keys to move line-by-line, or try the spacebar and “b” keys to skip up and down by full page. Quit the man pages by typing “q”. Alternatively, most commands you run will have a --help option that displays addition information For instance, with ls : $ ls --help Usage: ls [OPTION]... [FILE]... List information about the FILEs (the current directory by default). Sort entries alphabetically if none of -cftuvSUX nor --sort is specified. Mandatory arguments to long options are mandatory for short options too. -a, --all do not ignore entries starting with . -A, --almost-all do not list implied . and .."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,39,"--author with -l, print the author of each file -b, --escape print C-style escapes for nongraphic characters --block-size=SIZE scale sizes by SIZE before printing them; e.g., '--block-size=M' prints sizes in units of 1,048,576 bytes; see SIZE format below -B, --ignore-backups do not list implied entries ending with ~ # further output omitted for clarity Unsupported command-line options If you try to use an option that is not supported, ls and other programs will print an error message similar to this:[remote]$ ls -j ls: invalid option -- 'j' Try 'ls --help' for more information. Looking at documentation Looking at the man page for ls or usingls --help , what does the-h (--human-readable ) option do? Absolute vs Relative Paths Starting from /Users/amanda/data/ , which of the following commands could Amanda use to navigate to her home directory, which is/Users/amanda ? cd . cd / cd /home/amanda cd ../.. cd ~ cd home cd ~/data/.. cd cd .. Solution - No: ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,40,"stands for the current directory.- No: / stands for the root directory.- No: Amanda’s home directory is /Users/amanda .- No: this goes up two levels, i.e. ends in /Users .- Yes: ~ stands for the user’s home directory, in this case/Users/amanda .- No: this would navigate into a directory home in the current directory if it exists.- Yes: unnecessarily complicated, but correct. - Yes: shortcut to go back to the user’s home directory. - Yes: goes up one level. Relative Path Resolution Using the filesystem diagram below, if pwd displays/Users/thing , what willls -F ../backup display? ../backup: No such file or directory 2012-12-01 2013-01-08 2013-01-27 2012-12-01/ 2013-01-08/ 2013-01-27/ original/ pnas_final/ pnas_sub/ Solution - No: there is a directory backup in/Users .- No: this is the content of Users/thing/backup , but with.. we asked for one level further up.- No: see previous explanation. - Yes: ../backup/ refers to/Users/backup/ ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,41,"ls Reading ComprehensionAssuming a directory structure as in the above Figure (File System for Challenge Questions), if pwd displays/Users/backup , and-r tellsls to display things in reverse order, what command will display:pnas_sub/ pnas_final/ original/ ls pwd ls -r -F ls -r -F /Users/backup - Either #2 or #3 above, but not #1. Solution - No: pwd is not the name of a directory.- Yes: ls without directory argument lists files and directories in the current directory.- Yes: uses the absolute path explicitly. - Correct: see explanations above. Exploring More ls ArgumentsWhat does the command ls do when used with the-l and-h arguments?Some of its output is about properties that we do not cover in this lesson (such as file permissions and ownership), but the rest should be useful nevertheless. Solution The -l arguments makesls use a long listing format, showing not only the file/directory names but also additional information such as the file size and the time of its last modification."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,42,"The-h argument makes the file size “human readable”, i.e. display something like5.3K instead of5369 . Listing Recursively and By Time The command ls -R lists the contents of directories recursively, i.e., lists their sub-directories, sub-sub-directories, and so on in alphabetical order at each level. The commandls -t lists things by time of last change, with most recently changed files or directories first. In what order doesls -R -t display things? Hint:ls -l uses a long listing format to view timestamps.Solution The directories are listed alphabetical at each level, the files/directories in each directory are sorted by time of last change. Key Points Your current directory is referred to as the working directory. To change directories, use cd .To view files, use ls .You can view help for a command with man command orcommand --help .Hit tab to autocomplete whatever you’re currently typing."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,43,"Writing and reading files Overview Teaching: 30 min Exercises: 15 minQuestions How do I create/edit text files? How do I move/copy/delete files? Objectives Learn to use the nano text editor.Understand how to move, create, and delete files. Now that we know how to move around and look at things, let’s learn how to read, write, and handle files! We’ll start by moving back to our home directory and creating a scratch directory: $ cd ~ $ mkdir hpc-test $ cd hpc-test Creating and Editing Text Files When working on an HPC system, we will frequently need to create or edit text files. Text is one of the simplest computer file formats, defined as a simple sequence of text lines. What if we want to make a file? There are a few ways of doing this, the easiest of which is simply using a text editor. For this lesson, we are going to us nano , since it’s more intuitive than many other terminal text editors."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,44,"To create or edit a file, type nano <filename> , on the terminal, where <filename> is the name of the file. If the file does not already exist, it will be created. Let’s make a new file now, type whatever you want in it, and save it. $ nano draft.txt Nano defines a number of shortcut keys (prefixed by the Control or Ctrl key) to perform actions such as saving the file or exiting the editor. Here are the shortcut keys for a few common actions: - Ctrl+O — save the file (into a current name or a new name). - Ctrl+X — exit the editor. If you have not saved your file upon exiting, nano will ask you if you want to save. - Ctrl+K — cut (“kill”) a text line. This command deletes a line and saves it on a clipboard. If repeated multiple times without any interruption (key typing or cursor movement), it will cut a chunk of text lines. - Ctrl+U — paste the cut text line (or lines). This command can be repeated to paste the same text elsewhere."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,45,"Using vim as a text editorFrom time to time, you may encounter the vim text editor. Althoughvim isn’t the easiest or most user-friendly of text editors, you’ll be able to find it on any system and it has many more features thannano . vim has several modes, a “command” mode (for doing big operations, like saving and quitting) and an “insert” mode. You can switch to insert mode with thei key, and command mode withEsc .In insert mode, you can type more or less normally. In command mode there are a few commands you should be aware of: :q! — quit, without saving:wq — save and quitdd — cut/delete a liney — paste a line Do a quick check to confirm our file was created. $ ls draft.txt Reading Files Let’s read the file we just created now. There are a few different ways of doing this, one of which is reading the entire file with cat . $ cat draft.txt It's not ""publish or perish"" any more, it's ""share and thrive"". By default, cat prints out the content of the given file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,46,"Although cat may not seem like an intuitive command with which to read files, it stands for “concatenate”. Giving it multiple file names will print out the contents of the input files in the order specified in the cat ’s invocation. For example, $ cat draft.txt draft.txt It's not ""publish or perish"" any more, it's ""share and thrive"". It's not ""publish or perish"" any more, it's ""share and thrive"". Reading Multiple Text Files Create two more files using nano , giving them different names such aschap1.txt andchap2.txt . Then use a singlecat command to read and print the contents ofdraft.txt ,chap1.txt , andchap2.txt . Creating Directory We’ve successfully created a file. What about a directory? We’ve actually done this before, using mkdir . $ mkdir files $ ls draft.txt files Moving, Renaming, Copying Files Moving — We will move draft.txt to the files directory with mv (“move”) command."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,47,"The same syntax works for both files and directories: mv <file/directory> <new-location> $ mv draft.txt files $ cd files $ ls draft.txt Renaming — draft.txt isn’t a very descriptive name. How do we go about changing it? It turns out that mv is also used to rename files and directories. Although this may not seem intuitive at first, think of it as moving a file to be stored under a different name. The syntax is quite similar to moving files: mv oldName newName . $ mv draft.txt newname.testfile $ ls newname.testfile File extensions are arbitrary In the last example, we changed both a file’s name and extension at the same time. On UNIX systems, file extensions (like .txt ) are arbitrary. A file is a.txt file only because we say it is. Changing the name or extension of the file will never change a file’s contents, so you are free to rename things as you wish. With that in mind, however, file extensions are a useful tool for keeping track of what type of data it contains."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,48,"A.txt file typically contains text, for instance. Copying — What if we want to copy a file, instead of simply renaming or moving it? Use cp command (an abbreviated name for “copy”). This command has two different uses that work in the same way as mv : - Copy to same directory (copied file is renamed): cp file newFilename - Copy to other directory (copied file retains original name): cp file directory Let’s try this out. $ cp newname.testfile copy.testfile $ ls $ cp newname.testfile .. $ cd .. $ ls newname.testfile copy.testfile files documents newname.testfile Removing files We’ve begun to clutter up our workspace with all of the directories and files we’ve been making. Let’s learn how to get rid of them. One important note before we start… when you delete a file on UNIX systems, they are gone forever. There is no “recycle bin” or “trash”. Once a file is deleted, it is gone, never to return. So be very careful when deleting files. Files are deleted with rm file [moreFiles] ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,49,"To delete the newname.testfile in our current directory: $ ls $ rm newname.testfile $ ls files Documents newname.testfile files Documents That was simple enough. Directories are deleted in a similar manner using rm -r (the -r option stands for ‘recursive’). $ ls $ rm -r Documents $ rm -r files $ ls files Documents rmdir: failed to remove `files/': Directory not empty files What happened? As it turns out, rmdir is unable to remove directories that have stuff in them. To delete a directory and everything inside it, we will use a special variant of rm , rm -rf directory . This is probably the scariest command on UNIX- it will force delete a directory and all of its contents without prompting. ALWAYS double check your typing before using it… if you leave out the arguments, it will attempt to delete everything on your file system that you have permission to delete. So when deleting directories be very, very careful."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,50,"What happens when you use rm -rf accidentallySteam is a major online sales platform for PC video games with over 125 million users. Despite this, it hasn’t always had the most stable or error-free code. In January 2015, user kevyin on GitHub reported that Steam’s Linux client had deleted every file on his computer. It turned out that one of the Steam programmers had added the following line: rm -rf ""$STEAMROOT/""* . Due to the way that Steam was set up, the variable$STEAMROOT was never initialized, meaning the statement evaluated torm -rf /* . This coding error in the Linux client meant that Steam deleted every single file on a computer when run in certain scenarios (including connected external hard drives). Moral of the story: be very careful when usingrm -rf ! Looking at files Sometimes it’s not practical to read an entire file with cat - the file might be way too large, take a long time to open, or maybe we want to only look at a certain part of the file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,51,"As an example, we are going to look at a large and complex file type used in bioinformatics- a .gtf file. The GTF2 format is commonly used to describe the location of genetic features in a genome. Let’s grab and unpack a set of demo files for use later. To do this, we’ll use wget (wget link downloads a file from a link). $ wget https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz Problems with wget ? wget is a stand-alone application for downloading things over HTTP/HTTPS and FTP/FTPS connections, and it does the job admirably — when it is installed.Some operating systems instead come with cURL, which is the command-line interface to libcurl , a powerful library for programming interactions with remote resources over a wide variety of network protocols."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,52,"If you havecurl but notwget , then try this command instead:$ curl -O https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz For very large downloads, you might consider using Aria2, which has support for downloading the same file from multiple mirrors. You have to install it separately, but if you have it, try this to get it faster than your neighbors: $ aria2c https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz Install cURL - macOS: curl is pre-installed on macOS. If you must have the latest version you canbrew install it, but only do so if the stock version has failed you.Windows: curl comes preinstalled for the Windows 10 command line. For earlier Windows systems, you can download the executable directly; run it in place. curl comes preinstalled in Git for Windows and Windows Subsystem for Linux. On Cygwin, run the setup program again and select thecurl package to install it.- Linux: curl is packaged for every major distribution."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,53,"You can install it through the usual means. - Debian, Ubuntu, Mint: sudo apt install curl - CentOS, Red Hat: sudo yum install curl orzypper install curl - Fedora: sudo dnf install curl Install Aria2 - macOS: aria2c is available through a homebrew.brew install aria2 .- Windows: download the latest release and run aria2c in place. If you’re using the Windows Subsystem for Linux,- Linux: every major distribution has an aria2 package. Install it by the usual means.- Debian, Ubuntu, Mint: sudo apt install aria2 - CentOS, Red Hat: sudo yum install aria2 orzypper install aria2 - Fedora: sudo dnf install aria2 You’ll commonly encounter .tar.gz archives while working in UNIX."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,54,"To extract the files from a .tar.gz file, we run the command tar -xvf filename.tar.gz : $ tar -xvf bash-lesson.tar.gz dmel-all-r6.19.gtf dmel_unique_protein_isoforms_fb_2016_01.tsv gene_association.fb SRR307023_1.fastq SRR307023_2.fastq SRR307024_1.fastq SRR307024_2.fastq SRR307025_1.fastq SRR307025_2.fastq SRR307026_1.fastq SRR307026_2.fastq SRR307027_1.fastq SRR307027_2.fastq SRR307028_1.fastq SRR307028_2.fastq SRR307029_1.fastq SRR307029_2.fastq SRR307030_1.fastq SRR307030_2.fastq Unzipping files We just unzipped a .tar.gz file for this example. What if we run into other file formats that we need to unzip? Just use the handy reference below: gunzip extracts the contents of .gz filesunzip extracts the contents of .zip filestar -xvf extracts the contents of .tar.gz and .tar.bz2 files That is a lot of files! One of these files, dmel-all-r6.19.gtf is extremely large, and contains every annotated feature in the Drosophila melanogaster genome."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,55,"It’s a huge file- what happens if we run cat on it? (Press Ctrl + C to stop it). So, cat is a really bad option when reading big files… it scrolls through the entire file far too quickly! What are the alternatives? Try all of these out and see which ones you like best! head file : Print the top 10 lines in a file to the console. You can control the number of lines you see with the-n numberOfLines flag.tail file : Same ashead , but prints the last 10 lines in a file to the console.less file : Opens a file and display as much as possible on-screen. You can scroll withEnter or the arrow keys on your keyboard. Pressq to close the viewer. Out of cat , head , tail , and less , which method of reading files is your favourite? Why?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,56,Key Points Use nano to create or edit text files from a terminal.Use cat file1 [file2 ...] to print the contents of one or more files to the terminal.Use mv old dir to move a file or directoryold to another directorydir .Use mv old new to rename a file or directoryold to anew name.Use cp old new to copy a file under a new name or location.Use cp old dir copies a fileold into a directorydir .Use rm old to delete (remove) a file.File extensions are entirely arbitrary on UNIX systems. Wildcards and pipes Overview Teaching: 45 min Exercises: 10 minQuestions How can I run a command on multiple files at once? Is there an easy way of saving a command’s output? Objectives Redirect a command’s output to a file. Process a file instead of keyboard input using redirection. Construct command pipelines with two or more stages. Explain what usually happens if a program or pipeline isn’t given any input to process.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,57,"Required files If you didn’t get them in the last lesson, make sure to download the example files used in the next few sections: Using wget: wget https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz Using a web browser: https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz Now that we know some of the basic UNIX commands, we are going to explore some more advanced features. The first of these features is the wildcard * . In our examples before, we’ve done things to files one at a time and otherwise had to specify things explicitly. The * character lets us speed things up and do things across multiple files. Ever wanted to move, delete, or just do “something” to all files of a certain type in a directory? * lets you do that, by taking the place of one or more characters in a piece of text. So *.txt would be equivalent to all .txt files in a directory for instance. * by itself means all files. Let’s use our example data to see what I mean."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,58,"$ tar xvf bash-lesson.tar.gz $ ls bash-lesson.tar.gz SRR307026_1.fastq dmel-all-r6.19.gtf SRR307026_2.fastq dmel_unique_protein_isoforms_fb_2016_01.tsv SRR307027_1.fastq gene_association.fb SRR307027_2.fastq SRR307023_1.fastq SRR307028_1.fastq SRR307023_2.fastq SRR307028_2.fastq SRR307024_1.fastq SRR307029_1.fastq SRR307024_2.fastq SRR307029_2.fastq SRR307025_1.fastq SRR307030_1.fastq SRR307025_2.fastq SRR307030_2.fastq Now we have a whole bunch of example files in our directory. For this example we are going to learn a new command that tells us how long a file is: wc . wc -l file tells us the length of a file in lines. $ wc -l dmel-all-r6.19.gtf 542048 dmel-all-r6.19.gtf Interesting, there are over 540000 lines in our dmel-all-r6.19.gtf file. What if we wanted to run wc -l on every .fastq file? This is where * comes in really handy! *.fastq would match every file ending in .fastq ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,59,"$ wc -l *.fastq 20000 SRR307023_1.fastq 20000 SRR307023_2.fastq 20000 SRR307024_1.fastq 20000 SRR307024_2.fastq 20000 SRR307025_1.fastq 20000 SRR307025_2.fastq 20000 SRR307026_1.fastq 20000 SRR307026_2.fastq 20000 SRR307027_1.fastq 20000 SRR307027_2.fastq 20000 SRR307028_1.fastq 20000 SRR307028_2.fastq 20000 SRR307029_1.fastq 20000 SRR307029_2.fastq 20000 SRR307030_1.fastq 20000 SRR307030_2.fastq 320000 total That was easy. What if we wanted to do the same command, except on every file in the directory? A nice trick to keep in mind is that * by itself matches every file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,60,$ wc -l * 53037 bash-lesson.tar.gz 542048 dmel-all-r6.19.gtf 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv 106290 gene_association.fb 20000 SRR307023_1.fastq 20000 SRR307023_2.fastq 20000 SRR307024_1.fastq 20000 SRR307024_2.fastq 20000 SRR307025_1.fastq 20000 SRR307025_2.fastq 20000 SRR307026_1.fastq 20000 SRR307026_2.fastq 20000 SRR307027_1.fastq 20000 SRR307027_2.fastq 20000 SRR307028_1.fastq 20000 SRR307028_2.fastq 20000 SRR307029_1.fastq 20000 SRR307029_2.fastq 20000 SRR307030_1.fastq 20000 SRR307030_2.fastq 1043504 total Multiple wildcards You can even use multiple * s at a time. How would you runwc -l on every file with “fb” in it?Solution wc -l *fb* i.e. anything or nothing then fb then anything or nothing Using other commands Now let’s try cleaning up our working directory a bit.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,61,"Create a folder called “fastq” and move all of our .fastq files there in one mv command.Solution mkdir fastq mv *.fastq fastq/ Redirecting output Each of the commands we’ve used so far does only a very small amount of work. However, we can chain these small UNIX commands together to perform otherwise complicated actions! For our first foray into piping, or redirecting output, we are going to use the > operator to write output to a file. When using > , whatever is on the left of the > is written to the filename you specify on the right of the arrow. The actual syntax looks like command > filename . Let’s try several basic usages of > . echo simply prints back, or echoes whatever you type after it. $ echo ""this is a test"" $ echo ""this is a test"" > test.txt $ ls $ cat test.txt this is a test bash-lesson.tar.gz fastq dmel-all-r6.19.gtf gene_association.fb dmel_unique_protein_isoforms_fb_2016_01.tsv test.txt this is a test Awesome, let’s try that with a more complicated command, like wc -l ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,62,"$ wc -l * > word_counts.txt $ cat word_counts.txt wc: fastq: Is a directory 53037 bash-lesson.tar.gz 542048 dmel-all-r6.19.gtf 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv 0 fastq 106290 gene_association.fb 1 test.txt 723505 total Notice how we still got some output to the console even though we “piped” the output to a file? Our expected output still went to the file, but how did the error message get skipped and not go to the file? This phenomena is an artefact of how UNIX systems are built. There are 3 input/output streams for every UNIX program you will run: stdin , stdout , and stderr . Let’s dissect these three streams of input/output in the command we just ran: wc -l * > word_counts.txt stdin is the input to a program. In the command we just ran,stdin is represented by* , which is simply every filename in our current directory.stdout contains the actual, expected output."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,63,"In this case,> redirectedstdout to the fileword_counts.txt .stderr typically contains error messages and other information that doesn’t quite fit into the category of “output”. If we insist on redirecting bothstdout andstderr to the same file we would use&> instead of> . (We can redirect juststderr using2> .) Knowing what we know now, let’s try re-running the command, and send all of the output (including the error message) to the same word_counts.txt files as before. $ wc -l * &> word_counts.txt Notice how there was no output to the console that time. Let’s check that the error message went to the file like we specified. $ cat word_counts.txt 53037 bash-lesson.tar.gz 542048 dmel-all-r6.19.gtf 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv wc: fastq: Is a directory 0 fastq 106290 gene_association.fb 1 test.txt 7 word_counts.txt 723512 total Success! The wc: fastq: Is a directory error message was written to the file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,64,"Also, note how the file was silently overwritten by directing output to the same place as before. Sometimes this is not the behaviour we want. How do we append (add) to a file instead of overwriting it? Appending to a file is done the same was as redirecting output. However, instead of > , we will use >> . $ echo ""We want to add this sentence to the end of our file"" >> word_counts.txt $ cat word_counts.txt 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv 471308 Drosophila_melanogaster.BDGP5.77.gtf 0 fastq 1304914 fb_synonym_fb_2016_01.tsv 106290 gene_association.fb 1 test.txt 1904642 total We want to add this sentence to the end of our file Chaining commands together We now know how to redirect stdout and stderr to files. We can actually take this a step further and redirect output (stdout ) from one command to serve as the input (stdin ) for the next. To do this, we use the | (pipe) operator. grep is an extremely useful command. It finds things for us within files."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,65,"Basic usage (there are a lot of options for more clever things, see the man page) uses the syntax grep whatToFind fileToSearch . Let’s use grep to find all of the entries pertaining to the Act5C gene in Drosophila melanogaster. $ grep Act5C dmel-all-r6.19.gtf The output is nearly unintelligible since there is so much of it. Let’s send the output of that grep command to head so we can just take a peek at the first line. The | operator lets us send output from one command to the next: $ grep Act5C dmel-all-r6.19.gtf | head -n 1 X FlyBase gene 5900861 5905399 . + . gene_id ""FBgn0000042""; gene_symbol ""Act5C""; Nice work, we sent the output of grep to head . Let’s try counting the number of entries for Act5C with wc -l . We can do the same trick to send grep ’s output to wc -l : $ grep Act5C dmel-all-r6.19.gtf | wc -l 46 Note that this is just the same as redirecting output to a file, then reading the number of lines from that file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,66,"Writing commands using pipes How many files are there in the “fastq” directory we made earlier? (Use the shell to do this.) Solution ls fastq/ | wc -l Output of ls is one line per item, when chaining commands together like this, so counting lines gives the number of files. Reading from compressed files Let’s compress one of our files using gzip. $ gzip gene_association.fb zcat acts likecat , except that it can read information from.gz (compressed) files. Usingzcat , can you write a command to take a look at the top few lines of thegene_association.fb.gz file (without decompressing the file itself)?Solution zcat gene_association.fb.gz | head The head command without any options shows the first 10 lines of a file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,67,"Key Points The * wildcard is used as a placeholder to match any text that follows a pattern.Redirect a command’s output to a file with > .Commands can be chained with | Scripts, variables, and loops Overview Teaching: 45 min Exercises: 10 minQuestions How do I turn a set of commands into a program? Objectives Write a shell script Understand and manipulate UNIX permissions Understand shell variables and how to use them Write a simple “for” loop. We now know a lot of UNIX commands! Wouldn’t it be great if we could save certain commands so that we could run them later or not have to type them out again? As it turns out, this is straightforward to do. A “shell script” is essentially a text file containing a list of UNIX commands to be executed in a sequential manner. These shell scripts can be run whenever we want, and are a great way to automate our work. Writing a Script So how do we write a shell script, exactly? It turns out we can do this with a text editor."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,68,"Start editing a file called “demo.sh” (to recap, we can do this with nano demo.sh ). The “.sh” is the standard file extension for shell scripts that most people use (you may also see “.bash” used). Our shell script will have two parts: - On the very first line, add #!/bin/bash . The#! (pronounced “hash-bang”) tells our computer what program to run our script with. In this case, we are telling it to run our script with our command-line shell (what we’ve been doing everything in so far). If we wanted our script to be run with something else, like Perl, we could add#!/usr/bin/perl - Now, anywhere below the first line, add echo ""Our script worked!"" . When our script runs,echo will happily print outOur script worked! . Our file should now look like this: #!/bin/bash echo ""Our script worked!"" Ready to run our program? Let’s try running it: $ demo.sh bash: demo.sh: command not found... Strangely enough, Bash can’t find our script."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,69,"As it turns out, Bash will only look in certain directories for scripts to run. To run anything else, we need to tell Bash exactly where to look. To run a script that we wrote ourselves, we need to specify the full path to the file, followed by the filename. We could do this one of two ways: either with our absolute path /home/yourUserName/demo.sh , or with the relative path ./demo.sh . $ ./demo.sh bash: ./demo.sh: Permission denied There’s one last thing we need to do. Before a file can be run, it needs “permission” to run. Let’s look at our file’s permissions with ls -l : $ ls -l -rw-rw-r-- 1 yourUsername tc001 12534006 Jan 16 18:50 bash-lesson.tar.gz -rw-rw-r-- 1 yourUsername tc001 40 Jan 16 19:41 demo.sh -rw-rw-r-- 1 yourUsername tc001 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 yourUsername tc001 721242 Jan 25 2016 dmel_unique_protein_is..."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,70,"drwxrwxr-x 2 yourUsername tc001 4096 Jan 16 19:16 fastq -rw-r--r-- 1 yourUsername tc001 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 yourUsername tc001 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 yourUsername tc001 245 Jan 16 19:24 word_counts.txt That’s a huge amount of output: a full listing of everything in the directory. Let’s see if we can understand what each field of a given row represents, working left to right. - Permissions: On the very left side, there is a string of the characters d ,r ,w ,x , and- . Thed indicates if something is a directory (there is a- in that spot if it is not a directory). The otherr ,w ,x bits indicate permission to Read, Write, and eXecute a file. There are three fields ofrwx permissions following the spot ford . If a user is missing a permission to do something, it’s indicated by a- .- The first set of rwx are the permissions that the owner has (in this case the owner isyourUsername )."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,71,"- The second set of rwx s are permissions that other members of the owner’s group share (in this case, the group is namedtc001 ). - The third set of rwx s are permissions that anyone else with access to this computer can do with a file. Though files are typically created with read permissions for everyone, typically the permissions on your home directory prevent others from being able to access the file in the first place. - The first set of - References: This counts the number of references (hard links) to the item (file, folder, symbolic link or “shortcut”). - Owner: This is the username of the user who owns the file. Their permissions are indicated in the first permissions field. - Group: This is the user group of the user who owns the file. Members of this user group have permissions indicated in the second permissions field. - Size of item: This is the number of bytes in a file, or the number of filesystem blocks occupied by the contents of a folder."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,72,"(We can use the -h option here to get a human-readable file size in megabytes, gigabytes, etc.) - Time last modified: This is the last time the file was modified. - Filename: This is the filename. So how do we change permissions? As I mentioned earlier, we need permission to execute our script. Changing permissions is done with chmod . To add executable permissions for all users we could use this: $ chmod +x demo.sh $ ls -l -rw-rw-r-- 1 yourUsername tc001 12534006 Jan 16 18:50 bash-lesson.tar.gz -rwxrwxr-x 1 yourUsername tc001 40 Jan 16 19:41 demo.sh -rw-rw-r-- 1 yourUsername tc001 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 yourUsername tc001 721242 Jan 25 2016 dmel_unique_protein_is..."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,73,"drwxrwxr-x 2 yourUsername tc001 4096 Jan 16 19:16 fastq -rw-r--r-- 1 yourUsername tc001 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 yourUsername tc001 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 yourUsername tc001 245 Jan 16 19:24 word_counts.txt Now that we have executable permissions for that file, we can run it. $ ./demo.sh Our script worked! Fantastic, we’ve written our first program! Before we go any further, let’s learn how to take notes inside our program using comments. A comment is indicated by the # character, followed by whatever we want. Comments do not get run. Let’s try out some comments in the console, then add one to our script! # This won't show anything. Now lets try adding this to our script with nano . Edit your script to look something like this: #!/bin/bash # This is a comment... they are nice for making notes! echo ""Our script worked!"" When we run our script, the output should be unchanged from before!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,74,"Setting execute permission with chmod In Unix, a file has three basic permissions, each of which can be set for three levels of user. The permissions are: - Read permission (“r”) - numeric value 4. - Write permission (“w”) - numeric value 2. - Execute permission (“x”) - numeric value 1. When applied to a directory, execute permission refers to whether the directory can be entered with ‘cd’. The three levels of user are: - The user who owns the file (the “user”, referred to with “u”). - The group to which the file belongs - referred to with “g”. Each user has a primary group and is optionally a member of other groups. When a user creates a file, it is normally associated with the user’s primary group. At NYU HPC, all users are in a group named ‘users’, so group permission has little meaning. - All other users are referred to with “o”. You grant permissions with chmod who+what file and revoke them with chmod who-what file . (Notice that the first has “+” and the second “-“)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,75,"Here, “who” is some combination of “u”, “g”, and “o”, and “what” is some combination of “r”, “w”, and “x”. So, to set execute permission, as in the example above, we use: $ chmod u+x my_script Shell variables One important concept that we’ll need to cover are shell variables. Variables are a great way of saving information under a name you can access later. In programming languages like Python and R, variables can store pretty much anything you can think of. In the shell, they usually just store text. The best way to understand how they work is to see them in action. To set a variable, simply type in a name containing only letters, numbers, and underscores, followed by an = and whatever you want to put in the variable. Shell variable names are often uppercase by convention (but do not have to be). $ VAR=""This is our variable"" To use a variable, prefix its name with a $ sign."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,76,"Note that if we want to simply check what a variable is, we should use echo (or else the shell will try to run the contents of a variable). $ echo $VAR This is our variable Let’s try setting a variable in our script and then recalling its value as part of a command. We’re going to make it so our script runs wc -l on whichever file we specify with FILE . Our script: #!/bin/bash # set our variable to the name of our GTF file FILE=dmel-all-r6.19.gtf # call wc -l on our file wc -l $FILE $ ./demo.sh 542048 dmel-all-r6.19.gtf What if we wanted to do our little wc -l script on other files without having to change $FILE every time we want to use it? There is actually a special shell variable we can use in scripts that allows us to use arguments in our scripts (arguments are extra information that we can pass to our script, like the -l in wc -l ). To use the first argument to a script, use $1 (the second argument is $2 , and so on). Let’s change our script to run wc -l on $1 instead of $FILE ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,77,"Note that we can also pass all of the arguments using $@ (not going to use it in this lesson, but it’s something to be aware of). Our script: #!/bin/bash # call wc -l on our first argument wc -l $1 $ ./demo.sh dmel_unique_protein_isoforms_fb_2016_01.tsv 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv Nice! One thing to be aware of when using variables: they are all treated as pure text. How do we save the output of an actual command like ls -l ? A demonstration of what doesn’t work: $ TEST=ls -l -bash: -l: command not found What does work (we need to surround any command with $(command) ): $ TEST=$(ls -l) $ echo $TEST total 90372 -rw-rw-r-- 1 jeff jeff 12534006 Jan 16 18:50 bash-lesson.tar.gz -rwxrwxr-x. 1 jeff jeff 40 Jan 1619:41 demo.sh -rw-rw-r-- 1 jeff jeff 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 jeff jeff 721242 Jan 25 2016 dmel_unique_protein_isoforms_fb_2016_01.tsv drwxrwxr-x."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,78,"2 jeff jeff 4096 Jan 16 19:16 fastq -rw-r--r-- 1 jeff jeff 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 jeff jeff 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 jeff jeff 245 Jan 16 19:24 word_counts.txt Note that everything got printed on the same line. This is a feature, not a bug, as it allows us to use $(commands) inside lines of script without triggering line breaks (which would end our line of code and execute it prematurely). Loops To end our lesson on scripts, we are going to learn how to write a for-loop to execute a lot of commands at once. This will let us do the same string of commands on every file in a directory (or other stuff of that nature). for-loops generally have the following syntax: #!/bin/bash for VAR in first second third do echo $VAR done When a for-loop gets run, the loop will run once for everything following the word in . In each iteration, the variable $VAR is set to a particular value for that iteration."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,79,"In this case it will be set to first during the first iteration, second on the second, and so on. During each iteration, the code between do and done is performed. Let’s run the script we just wrote (I saved mine as loop.sh ). $ chmod +x loop.sh $ ./loop.sh first second third What if we wanted to loop over a shell variable, such as every file in the current directory? Shell variables work perfectly in for-loops."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,80,"In this example, we’ll save the result of ls and loop over each file: #!/bin/bash FILES=$(ls) for VAR in $FILES do echo $VAR done $ ./loop.sh bash-lesson.tar.gz demo.sh dmel_unique_protein_isoforms_fb_2016_01.tsv dmel-all-r6.19.gtf fastq gene_association.fb.gz loop.sh test.txt word_counts.txt There’s a shortcut to run on all files of a particular type, say all .gz files: #!/bin/bash for VAR in *.gz do echo $VAR done bash-lesson.tar.gz gene_association.fb.gz Writing our own scripts and loops cd to ourfastq directory from earlier and write a loop to print off the name and top 4 lines of every fastq file in that directory.Is there a way to only run the loop on fastq files ending in _1.fastq ?Solution Create the following script in a file called head_all.sh #!/bin/bash for FILE in *.fastq do echo $FILE head -n 4 $FILE done The “for” line could be modified to be for FILE in *_1.fastq to achieve the second aim. Concatenating variables Concatenating (i.e."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,81,"mashing together) variables is quite easy to do. Add whatever you want to concatenate to the beginning or end of the shell variable after enclosing it in {} characters.FILE=stuff.txt echo ${FILE}.example stuff.txt.example Can you write a script that prints off the name of every file in a directory with “.processed” added to it? Solution Create the following script in a file called process.sh #!/bin/bash for FILE in * do echo ${FILE}.processed done Note that this will also print directories appended with “.processed”. To truly only get files and not directories, we need to modify this to use the find command to give us only files in the current directory:#!/bin/bash for FILE in $(find . -max-depth 1 -type f) do echo ${FILE}.processed done but this will have the side-effect of listing hidden files too. Special permissions What if we want to give different sets of users different permissions. chmod actually accepts special numeric codes instead of stuff likechmod +x ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_aio_index.html.html,82,"The numeric codes are as follows: read = 4, write = 2, execute = 1. For each user we will assign permissions based on the sum of these permissions (must be between 7 and 0).Let’s make an example file and give everyone permission to do everything with it. touch example ls -l example chmod 777 example ls -l example How might we give ourselves permission to do everything with a file, but allow no one else to do anything with it. Solution chmod 700 example We want all permissions so: 4 (read) + 2 (write) + 1 (execute) = 7 for user (first position), no permissions, i.e. 0, for group (second position) and all (third position). Key Points A shell script is just a list of bash commands in a text file. To make a shell script file executable, run chmod +x script.sh ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_CODE_OF_CONDUCT.html.html,0,"As contributors and maintainers of this project, we pledge to follow the Carpentry Code of Conduct. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by following our reporting guidelines. As contributors and maintainers of this project, we pledge to follow the Carpentry Code of Conduct. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by following our reporting guidelines."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_discuss_index.html.html,0,"This lesson is in the early stages of development (Alpha version) Toggle navigation Home Code of Conduct Setup Episodes Why Use a Cluster? Connecting to the remote HPC system Moving around and looking at things Writing and reading files Wildcards and pipes Scripts, variables, and loops All in one page (Beta) Extras Reference About Discussion Figures Instructor Notes License Improve this page Introduction to Using the Shell in a High-Performance Computing Context : Discussion"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_figures_index.html.html,0,"This lesson is in the early stages of development (Alpha version) Toggle navigation Home Code of Conduct Setup Episodes Why Use a Cluster? Connecting to the remote HPC system Moving around and looking at things Writing and reading files Wildcards and pipes Scripts, variables, and loops All in one page (Beta) Extras Reference About Discussion Figures Instructor Notes License Improve this page Introduction to Using the Shell in a High-Performance Computing Context : Figures"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_guide_index.html.html,0,"This lesson is in the early stages of development (Alpha version) Toggle navigation Home Code of Conduct Setup Episodes Why Use a Cluster? Connecting to the remote HPC system Moving around and looking at things Writing and reading files Wildcards and pipes Scripts, variables, and loops All in one page (Beta) Extras Reference About Discussion Figures Instructor Notes License Improve this page Introduction to Using the Shell in a High-Performance Computing Context : Instructor Notes Instructor Notes Teaching Overview Specific Lessons Why Use a Cluster?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_license.html,0,"Instructional Material All Software Carpentry and Data Carpentry instructional material is made available under the Creative Commons Attribution license. The following is a human-readable summary of (and not a substitute for) the full legal text of the CC BY 4.0 license. You are free: - to Share—copy and redistribute the material in any medium or format - to Adapt—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: - Attribution—You must give appropriate credit (mentioning that your work is derived from work that is Copyright © Software Carpentry and, where practical, linking to http://software-carpentry.org/), provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_license.html,1,"No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. With the understanding that: Notices: - You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. - No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. Software Except where otherwise noted, the example programs and other software provided by Software Carpentry and Data Carpentry are made available under the OSI-approved MIT license."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_license.html,2,"Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_license.html,3,"IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Trademark “Software Carpentry” and “Data Carpentry” and their respective logos are registered trademarks of NumFOCUS."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_reference.html,0,"Key Points Why Use a Cluster? High Performance Computing (HPC) typically involves connecting to very large computing systems elsewhere in the world. These HPC systems can be used to do work that would either be impossible or much slower or smaller systems. The standard method of interacting with such systems is via a command line interface such as Bash. Connecting to the remote HPC system To connect to a remote HPC system using SSH and a password, run ssh <NetID>@greene.hpc.nyu.edu. To connect to a remote HPC system using SSH and an SSH key, run ssh -i ~/.ssh/key_for_remote_computer <NetID>@greene.hpc.nyu.edu. Moving around and looking at things Your current directory is referred to as the working directory. To change directories, use cd. To view files, use ls. You can view help for a command with man command or command --help. Hit tab to autocomplete whatever you’re currently typing. Writing and reading files Use nano to create or edit text files from a terminal."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_reference.html,1,"Use cat file1 [file2 ...] to print the contents of one or more files to the terminal. Use mv old dir to move a file or directory old to another directory dir. Use mv old new to rename a file or directory old to a new name. Use cp old new to copy a file under a new name or location. Use cp old dir copies a file old into a directory dir. Use rm old to delete (remove) a file. File extensions are entirely arbitrary on UNIX systems. Wildcards and pipes The * wildcard is used as a placeholder to match any text that follows a pattern. Redirect a command’s output to a file with >. Commands can be chained with | Scripts, variables, and loops A shell script is just a list of bash commands in a text file. To make a shell script file executable, run chmod +x script.sh. Glossary External References Text Editing Nano editor home page Nano tutorial VIM editor home page Vim also has a built-in tutorial. From the bash prompt, type vimtutor and follow the instructions."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_setup.html.html,0,"There are several pieces of software you will wish to install before the workshop. Though installation help will be provided at the workshop, we recommend that these tools are installed (or at least downloaded) beforehand. Bash and SSH This lesson requires a terminal application ( bash ,zsh , or others) with the ability to securely connect to a remote machine (ssh ). Where to type commands: How to open a new shell The shell is a program that enables us to send commands to the computer and receive output. It is also referred to as the terminal or command line. Some computers include a default Unix Shell program. The steps below describe some methods for identifying and opening a Unix Shell program if you already have one installed. There are also options for identifying and downloading a Unix Shell program, a Linux/UNIX emulator, or a program to access a Unix Shell on a server. Windows Computers with Windows operating systems do not automatically have a Unix Shell program installed."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_setup.html.html,1,"In this lesson, we encourage you to use an emulator included in Git for Windows, which gives you access to both Bash shell commands and Git. If you have attended a Software Carpentry workshop session, it is likely you have already received instructions on how to install Git for Windows. Once installed, you can open a terminal by running the program Git Bash from the Windows start menu. Reference - Git for Windows — Recommended - Windows Subsystem for Linux — advanced option for Windows 10 Alternatives to Git for Windows Other solutions are available for running Bash commands on Windows. There is now a Bash shell command-line tool available for Windows 10. Additionally, you can run Bash commands on a remote computer or server that already has a Unix Shell, from your Windows machine. This can usually be done through a Secure Shell (SSH) client. One such client available for free for Windows computers is PuTTY."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_setup.html.html,2,"See the reference below for information on installing and using PuTTY, using the Windows 10 command-line tool, or installing and using a Unix/Linux emulator. For advanced users, you may choose one of the following alternatives: - Install the Windows Subsystem for Linux - Use the Windows Powershell - Read up on Using a Unix/Linux emulator (Cygwin) or Secure Shell (SSH) client (Putty) Warning Commands in the Windows Subsystem for Linux (WSL), Powershell, or Cygwin may differ slightly from those shown in the lesson or presented in the workshop. Please ask if you encounter such a mismatch — you’re probably not alone. macOS On macOS, the default Unix Shell is accessible by running the Terminal program from the /Application/Utilities folder in Finder. To open Terminal, try one or both of the following: - In Finder, select the Go menu, then select Utilities. Locate Terminal in the Utilities folder and open it. - Use the Mac ‘Spotlight’ computer search function."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_setup.html.html,3,"Search for: Terminal and press Return. Reference Linux On most versions of Linux, the default Unix Shell is accessible by running the (Gnome) Terminal or (KDE) Konsole or xterm, which can be found via the applications menu or the search bar. Special cases If none of the options above address your circumstances, try an online search for: Unix shell [your operating system] . SSH for Secure Connections All students should have an SSH client installed. SSH is a tool that allows us to connect to and use a remote computer as our own. Windows Git for Windows comes with SSH preinstalled: you do not have to do anything. GUI Support If you know that the software you will be running on the cluster requires a graphical user interface (a GUI window needs to open for the application to run properly), please install MobaXterm Home Edition. macOS macOS comes with SSH pre-installed: you do not have to do anything."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\nyuhpc.github.io\hpc-shell_setup.html.html,4,"GUI Support If you know that the software you will be running requires a graphical user interface, please install XQuartz. Linux Linux comes with SSH and X window support preinstalled: you do not have to do anything."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc.html,0,"High Performance Computing NYU High Performance Computing (NYU HPC) provides access to state of the art supercomputer hardware and cloud services to eligible faculty and students across all of NYU Announcements What We Do The NYU High Performance Computing team, within IT's Research Technology division, provides seamless and efficient access to advanced computing resources, outstanding services, and expertise that: Enable NYU scholars to work on the most challenging computational research projects that involve HPC, Big Data Analytics, and Artificial Intelligence Enable and participate in research collaborations Facilitate usage of computing resources, minimize barriers of entry, promote the use of best practices, and develop the computing skills of users by providing training and consultations Facilitate the use of HPC resources in courses Who Can Access NYU HPC Resources NYU HPC resources are available to full-time NYU faculty and research staff."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc.html,1,"Access for NYU students and external collaborators requires sponsorship from full-time NYU faculty. Full-time NYU faculty and research staff Faculty-sponsored students, postdocs, adjuncts, and other non-full time faculty and research staff Faculty-sponsored external (non-NYU) collaborators Please Note: To apply or renew an NYU HPC user account click here Faculty, students and staff at NYU School of Medicine require the sponsorship of an eligible NYU faculty member at Washington Square to work on the NYU HPC clusters. External (non-NYU) collaborators may require sponsorship for an NYU netid before applying for HPC user account. The process is described here. All sponsored accounts are valid for a period of 12 months, at which point a renewal process is required to continue to use the clusters."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing About Acknowledgement Statement Events Collaborations HPC Policies Featured Research Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_acknowledgement-statement.html,0,"Acknowledgement Statement The following acknowledgment statement should appear in a publication of any material that resulted from using the NYU IT HPC resources, services, and staff expertise. ""This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise"" Please let the NYU HPC team know (via email to hpc@nyu.edu) when your research projects that utilized HPC resources publishes papers, or presentations."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_announcements-news.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_announcements-news.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing Announcements & News Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_cluster-stats-and-usage-data.html,0,"Cluster Stats and Usage Data Cluster Statistics and Usage Data Cluster Statistics and Usage Data System Status System Status Cluster Resources Utilization Cluster Resources Utilization Usage is grouped by Schools/Cluster/Role HPC account sponsors can see usage consumed by NetIDs they sponsored User feedback: Efficiency of Jobs, Wait time User feedback: Efficiency of Jobs, Wait time Efficiency of Users Jobs (per user and whole cluster) Wait time experienced by users - use 'wait time' tab here"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_cluster-stats-and-usage-data_efficiency-of-user-jobs.html,0,"Efficiency of User Jobs Efficiency of jobs and general job statistics Efficiency of jobs and general job statistics Use the button ""Sign In"" in the app below (on the left hand side) to see the data Use the button ""Sign In"" in the app below (on the left hand side) to see the data"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_cluster-stats-and-usage-data_resources-consumption.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_cluster-stats-and-usage-data_resources-consumption.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing HPC resources consumption Use the button ""Sign In"" in the app below (on the left hand side) to see the data Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_collaborations.html,0,Collaborations The Open Science Data Cloud Provides 1TB free storage for science data.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_collaborations.html,1,"We encourage researchers to publish datasets associated with published research as “Public Data” on OSDCThe NY State High Performance Computing Consortium (hpc^2) Provides high performance computing resources for New York State industry and academic institutions:Rensselaer Polytechnic Institute Stony Brook University – Dave Ecker University at Buffalo Brookhaven National Lab The Extreme Science and Engineering Discovery Environment (XSEDE) The most advanced, powerful, and robust collection of integrated advanced digital resources and services in the world; a single virtual system that scientists can use to interactively share computing resources, data, and expertise.Open Science Grid A national, distributed computing grid for data-intensive research.The Common Solutions Group A group for cooperative exploration of common solutions to IT challenges in higher educationThe Open Science Project The OSP is dedicated to writing and releasing free and Open Source scientific software.NYSERNet NYSERNet is a private not-for-profit corporation created to foster science and education in New York StateThe National Science Foundation An independent federal agency created by Congress in 1950 “to promote the progress of science; to advance the national health, prosperity, and welfare; to secure the national defense.”Oak Ridge National Laboratory The Department of Energy’s largest science and energy laboratory.Argonne National Laboratory One of the U.S."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_collaborations.html,2,"Department of Energy’s largest research centers. It is also the nation’s first national laboratory, chartered in 1946.TOP500 Supercomputer Sites A project started in 1993 to provide a reliable basis for tracking and detecting trends in high-performance computing."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_events.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_events.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing Events Please Note: To view the calendar you must be logged into your NYU Google Account."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_events.html,2,Report abuse Page details Page updated Report abuse
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research.html,0,"The Brooklyn Research Cluster provides flexible computing, including for NYU Tandon’s Visualization, Imaging, and Data Analytics research center An app that translates sign language into spoken English using Brooklyn Research Cluster Platform An Event-Driven Model for Estimation of Phase-Amplitude Coupling at Time Scales of Cognitive Phenomena Drug design for treatment of heart attack and stroke - How the hSCAN-1 enzyme is activated How carcinogenic chemicals slip past DNA repair mechanisms Seeing below the resolution of MRI The link between Atlantic Ocean warming and Antarctic climate change"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_cognitive-phenomena.html,0,"Cognitive Phenomena Phase-amplitude coupling (PAC) is a form of cross-frequency coupling (CFC) in which the amplitude of a fast signal is shown to depend on the phase of a slower signal. The fast signal is said to be ""nested"" within the cycles of the slow modulation signal. PAC between distinct neural oscillations has been shown to play a critical role to in the execution of cognitive functions that include cross-scale organization, selection of attention, routing the flow of information through neural circuits, memory processing, and information coding. PAC itself is thought to be a neural computation involved in regulating signal synchrony in the brain, and has been demonstrably identified in various species and across multiple brain areas. Several methods exist for PAC estimation, but the physiological bases of PAC are not well understood, so there is not yet a singular ""golden standard"" presented by the neuroscience community for PAC estimation. Researchers Dr."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_cognitive-phenomena.html,1,"Andre Fenton and Ph.D student Dino Dvorak at NYU's Center for Neural Science have published a new approach to PAC estimation which extends PAC analysis beyond current methods, which are limited by filter parameters to analytical windows of no less than 10 seconds. The new approach can accurately estimate PAC phenomena within a single cycle of the modulation signal while still providing the same information as current PAC estimation techniques at larger time scales. Importantly, the extension of PAC analysis to smaller timescales enables the investigation of PAC at the time scales of cognitive phenomena. It is not always certain before analysis which frequency bands are involved in PAC phenomena, so PAC estimation typically begins with the construction of a comodulogram to view the signal interactions for all detectable combinations of the phase signal ¿(t) and the amplitude signal A(t)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_cognitive-phenomena.html,2,"Once the oscillatory events of the band being analyzed have been properly displayed in time-frequency space, the proper modulation index is estimated by using high power events as time locking points for the whole signal. It is often difficult to estimate the modulation index presented by the slow wave if the slow wave signal is not sinusoidal, which is currently a strong limitation for filtering techniques. Andre Fenton's and Dino Dvorak's work in this field describes another form of CFC called oscillation-triggered coupling (OTC) which is used to characterize PAC phenomena at the scale of individual oscillations. The OTC approach provides the same global estimates of PAC properties such as the frequencies of the modulatory rhythms, the frequencies of modulated bands, as well as the preferred phases of coupling for the individual modulated bands."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_cognitive-phenomena.html,3,"OTC analysis extends beyond the global scale into the scale of individual oscillations by using frequency-specific oscillation events to view and estimate PAC patterns. Oscillation triggered coupling (OTC) analysis. (A) The OTC algorithm operates at two temporal scales – the “global” scale, which estimates the general properties of PAC (modulated bands, preferred phases of coupling) and the “local” scale of individual oscillations. Oscillations that are initially used to create the oscillation triggered comodulogram (OTCG) at the global scale can be filtered using various criteria (e.g. specific frequency, phase and power) in order to obtain the specific events that are responsible for generating phase–amplitude coupling. (B) In the first step, the raw LFP signal is transformed into (C) a z-score normalized wavelet spectra. Individual oscillations are detected as local maxima in time–frequency space. (D) Time stamps of large (>2 S.D."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_cognitive-phenomena.html,4,"from mean power) and frequency-specific (f±delta-f) oscillations are then used as trigger points for summing the time windows of the raw LFPs centered at these time stamps. The development of the OTC signal is displayed as a function of increasing numbers of summed event windows N = 1, 50, 100 and 1000. Notice that the amplitude and smoothness of the resulting OTC signal increases with the number of events. This indicates there is a systematic relationship between the peaks of detected oscillations and the phase of the slow rhythm. The red dotted horizontal lines mark the significant amplitude threshold of the OTC signal, which was computed from a surrogate test using random trigger points. (E) The resulting OTC signal displays several important properties. Its peak-to-peak amplitude corresponds to the strength of coupling, its phase at time 0 (middle of the time window) corresponds to the preferred phase of the coupling and its frequency corresponds to the modulatory rhythm."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_cognitive-phenomena.html,5,"(F) In order to obtain a significant amplitude of the modulatory signal, approximately 70 events (corresponding to approx. 30 s of data) need to be added to the summation. (G) The above process can be repeated for a range of frequency bands (e.g. 20–200 Hz) to obtain the oscillation-triggered comodulogram (OTCG). The profile of the modulation strength (peak-to-peak amplitude of the modulatory signal) across frequencies (G right) shows peaks in the slow (∼40 Hz) and fast (∼80 Hz) gamma bands (red arrows) and (H) the same peaks can be also observed in the FFT spectra computed from all frequency-specific modulatory signals. The FFT also shows that the wave pattern of the OTC corresponds to a single modulatory frequency of ∼8 Hz (theta) that is present across the whole frequency range (20–200 Hz). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.) Dr."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_cognitive-phenomena.html,6,"Andre Fenton's and Ph.D student Dino Dvorak's work is published in the Journal of Neuroscience Methods titled, Toward a Proper Estimation of Phase-Amplitude Coupling in Neural Oscillations"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_dna-repair.html,0,"DNA Repair The Broyde Laboratory is concerned with determining how DNA lesions, including some that are derived from environmental chemical pollutants present in automobile exhaust, tobacco smoke, and broiled meats, can cause mutations that initiate cancer if they are not repaired. These types of pollutants are multi-ringed chemical compounds, such as the widely-studied benzo[a]pyrene, that are activated in the body so that they can attack our DNA and link to it. The repair mechanism that is responsible for excising these types of DNA damage is termed nucleotide excision repair (NER). The NER machinery operates through recognition of the lesion by a specific human protein known as XPC that detects destabilizing DNA damage. A complex of additional proteins is then assembled, which cut out a section of the DNA strand containing the damage and then restore the correct DNA sequence."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_dna-repair.html,1,"Remarkably, the number and arrangement of the rings in the chemicals determine how well the NER process operates, with differences that vary over many orders of magnitude, and the origin of this phenomenon is not understood. Our work is elucidating the factors that are responsible for the differential repair rates. Lesions that are not repaired are the most harmful ones as they survive to be mutagenic, and identifying them for remediation through a combination of computational and experimental approaches is an overall goal. We work closely with our collaborator Professor Nicholas E. Geacintov in the Chemistry Department, whose laboratory performs experimental investigations of NER for various lesions with differing repair susceptibilities using human HeLa cell extracts. These data provide benchmarks in our simulations of the structural, dynamic and energetic properties of the damaged DNA."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_dna-repair.html,2,"We utilize molecular dynamics simulations employing the AMBER suite of programs on the NYU ITS HPC cluster. These resources are essential for efficiently obtaining, storing and analyzing the data from the long-timeframe simulations of large systems that are the current state-of-the-art. Because of the availability of the HPC resources to all postdoctoral researchers and graduate students in our group, we are able to carry out simultaneously a number of different projects that has resulted in 9 peer-reviewed publications in 2012-2013. As an example, a recent publication in the journal Biochemistry (Mu, et al, 2013 Aug 20;52(33):5517-21) explained an intriguing data set obtained in the Geacintov Laboratory. They found that the identical bulky benzo[a]pyrene-derived lesion attached to the DNA base guanine could be excised or overlooked by the NER machinery depending on whether the base opposite the guanine was its normal partner cytosine or a mismatched adenine."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_dna-repair.html,3,"Our simulations showed that the damaged DNA was dynamically destabilized with normal partner, while the mismatched adenine locally stabilized the duplex, preventing the lesion recognition protein from detecting the damage. This interpretation fits in well with our recent simulations indicating that have indicated that lesions which locally stabilize duplex DNA resist repair, while destabilizing ones are repaired. This work required simulating 16 systems whose size was ~ 12,000 atoms each including counterions and aqueous solvent for ~ 400 ns. The coordinates were collected every 1 ps, resulting in 64 GB of data that was accumulated over about 4 months. We analyzed how the structures evolved dynamically in time and the energetic interactions within each of the damaged DNAs, to interpret the experimental functional studies on structural, energetic and dynamic grounds."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_dna-repair.html,4,"In this first video, we see that the small normal partner C in the damaged DNA is dynamically unstable, which could trigger Nucleotide Excision Repair. Contrast the second video, in which the damaged G-base has mis-paired with an A-base rather than it's normal C-base partner. In this case, the larger partner A is stable, which could leave the lesion undetected PLACEHOLDER FOR VIDEOS Currently and in future work, we are investigating ER of lesioned-DNA when associated with histone proteins in the nucleosome, the next higher order of DNA packaging in the cell. The nucleosome system entails ~150,000 atoms when solvated and neutralized by counterions. The HPC resources have enabled our group to carry out a series of computational projects that provide structural understanding of experimental data and thereby explain biological function. These published studies are featured at http://broyde.nyu.edu/."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_heart-attack-treatment.html,0,"Heart Attack Treatment Recently it was discovered that saliva of certain blood-sucking insects contains an enzyme which indirectly prevents the blood from clotting, and that humans have an homologous protein, named hSCAN-1. Drs. David Rooklin and Yingkai Zhang from NYU's Department of Chemistry, with Dr. Min Lu from UMDNJ, set out to find how it might be modified for use in heart attack and stroke treatments. The key lay in developing an understanding of how the protein is activated, and the group ran hundreds of molecular simulations at quantum mechanical and atomistic scales. With any one simulation using up to 64 processors and running sometimes for weeks, the group used NYU's HPC resources to complete the workload. From the results the trio elucidated the detailed enzymatic mechanism of the human protein."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_heart-attack-treatment.html,1,"Their identification of a previously uncharacterized catalytic calcium-binding site (the magenta ion in the figure) reconciles an incongruous connection between acidic side chains and phosphate binding, explains the previously puzzling sigmoidal relationship between enzymatic rate and calcium concentration and provides a rich basis for understanding why hSCAN-1 is exclusively dependent on calcium and, in general, how catalytic calcium-activation is achieved. Zhang, Rooklin and Lu published Revelation of a Catalytic Calcium-Binding Site Elucidates Unusual Metal Dependence of a Human Apyrase in the Journal Of The American Chemical Society (published August 28, 2012) and have laid out a plan for interested experimental mutagenesis researchers to collaborate to support our understanding and push forward the redesign of hSCAN-1. An article about their work can be found here, and more about the research undertaken by Dr. Zhang's group can be found at this page."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_mri.html,0,"MRI Biological tissue, porous rock and composite material samples appear uniform at the macro-scale and well-organized at the micro-scale, but their structural disorder at the meso-scale - such as the cellular level - is an important indicator for categorizing samples and identifying diseases. One non-invasive technique is to measure molecular diffusion of, for example, water, through the sample. Mesoscopic structural parameters such as pore or cell sizes and shapes can then be inferred from the time-dependent diffusion behavior. Making this inference is, however, a difficult and ill-posed problem, requiring a structural model which predicts the bulk diffusion coefficient, against which the measured one could be compared. Drs. Dmitry Novikov and Els Fieremans of NYU School of Medicine, working with Drs."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_mri.html,1,"Jens Jensen and Joseph Helpern of the Medical University of South Carolina, have proposed that the structural disorder in a sample can be adequately and parsimoniously represented with just a small set of ""structural universality classes"", as illustrated below. These are characterized by a structural exponent p in the relation ϑ=(p+d)/2, where d is the number of spatial dimensions and ϑ is the dynamical exponent characterizing diffusion in the long-time limit. Dr. Novikov and colleagues used the NYU HPC clusters to run simulations, based on Monte-Carlo methods, showing the time dependent diffusion behavior of each structural universality class. In this example the steady-state diffusion of each simulated sample is the same, but the structural disorder is clearly visible in the time-dependent behavior, each sample showing a distinctly different curve determined by its structural exponent p."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_mri.html,2,"The novel disorder classification paradigm allowed the authors to make important conclusions on what slows down water diffusion in muscle and in brain, and to elucidate the cellular-level changes responsible for the decrease of diffusion coefficient in acute stroke, which is used as a clinical MRI contrast Drs. Novikov and Fieremans's work is published in PNAS at http://www.pnas.org/content/111/14/5088."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_ocean-warming.html,0,"Ocean Warming In recent decades Antarctica, especially the Antarctic Peninsula, has experienced dramatic climate change, one observed phenomenon being a dipolar pattern of high sea-level pressure south of Australia and low sea-level pressure in the Amundsen Sea during the souther winter. The mechanism driving this has been identified by researchers at NYU's Center for Atmospheric and Ocean Science as warming in the north and tropical Atlantic.Xichen Li, David Holland, Edwin Gerber and Changhyun Yoo recently published a paper in Nature showing that this pattern of Antarctic sea-level pressure correlates with sea surface temperature in the north and tropical Atlantic, moreover that the Atlantic sea surface temperature is driving the Antarctic sea-level pressure and that the low pressure thus established in the Amundsen Sea enhances warm-air advection and warm-water transport to the Antarctic Peninsula, thus contributing to the observed climate change in that region."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_ocean-warming.html,1,"The sea surface temperature in the north and tropical Atlantic has shown an increasing trend since 1870 overlaid on the Atlantic Multidecadal Oscillation (AMO), a longer-term, 60-70 year cycle in atlantic sea surface temperatures. Since 1979 the AMO has been in an increasing-temperature phase of its cycle. Xichen Li and colleagues used regression and maximum covariance analysis to establish the correlation between sea surface temperature in the north and tropical Atlantic and sea-level pressure in the Antarctic, as illustrated below. However correlation does not imply causation, and physical experiments testing the effect on Antarctica of altering temperatures in the Atlantic are not practical (or ethical!). But by running numerical simulations on NYUs HPC clusters, using the Community Atmosphere Model (CAM4) from NCAR they were able to show that by forcing warming in the north and tropical Atlantic causes sea-level patterns in Antarctica corresponding to those observed."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_ocean-warming.html,2,"The researchers went further, running simulations using the dry dynamical core of a GFDL atmosphere model to show that Rossby waves link the tropical Atlantic to the Antarctic, providing the mechanism for this correlation. Li et al have published their results in Nature, and their work has also been reported in the International Science Times, phys.org and at NYU"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_sign-language-app.html,0,"Sign Language App A team of students from the NYU Tandon School of Engineering built the prototype of a mobile app that enables hearing people to understand sign language and helps the deaf by translating spoken words into sign language. Their project is part of the Verizon's Connected Futures challenge which, in partnership with NYC Media Lab, supports new media and technology projects from universities across New York City. The team was led by Zhongheng Li who was inspired by his friend Fanny whose parents are deaf. Since there is no universal sign language, Fanny ’s family was having a tough time when they moved to the US. Therefore, these students created an app to empower millions of deaf people across the globe. The team built this app using the concepts of Machine Learning, Augmented Reality, Computer Vision and Cloud Computing."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_sign-language-app.html,1,"Zhongheng Li and his team used Brooklyn Research Cluster as their high-performance cloud computing platform to host their deep learning API using both OpenPose and TensorFlow trained Image classification model on the cloud. Initially, they wanted to use 'Depth Mode' camera features for better recognition. But, soon they realized that not everyone can afford a high-end smartphone model with depth camera features. So, they converted RGB images into skeleton images using OpenPose library for better accuracy and eliminated the need for a depth camera. They leveraged the power and flexibility of cloud computing to enhance their recognition model. Since they are using RGB camera and processing the information in the cloud, they don't have to depend on any devices or platforms. They can implement their framework on other technologies such as Hololens."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_sign-language-app.html,2,"They preferred to use Brooklyn Research Cluster for its availability, cost, support, and resources that are required to run high compute processes. Since cost was an important parameter for their project, they chose Brooklyn Research Cluster over Amazon AWS EC2 instances which charged them $1.14 per hour. It was not a feasible option for them because they wanted to use the servers for long duration and the cost added up. Being full-time students at NYU, all the HPC resources are available to them for free which includes compute power and storage. In terms of performance, Brooklyn Research Cluster is a better alternative than Amazon EC2 instance for their team because AWS provides Nvidia Tesla M60 whereas BRC provides 3 NVIDIA P100 GPU's which gave them a better performance. They are thankful to the HPC team for their guidance and support to use these services effectively and provide detailed instructions on certain topics."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_featured-research_sign-language-app.html,3,"As of now, the app is still in its pilot phase where it can detect and translate few phrases. A user can book an appointment with the medical clinic using sign interpretations. You can watch the demo video to understand how this app works."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_hpc-policies.html,0,"HPC Policies The policies and procedures governing access to and usage of the shared NYU HPC clusters managed and hosted by NYU IT have been approved by the Research Technology Faculty Advisory Board. These policies are necessary to ensure that the resources are equitably shared, properly used, and effective in supporting the needs of all researchers. Contact information Your NYU email account is the HPC official means of communication. All HPC announcements and communication about downtime, upgrades, etc., will be sent to the account holder's NetID-associated email (NetID@nyu.edu). It is each account owner's responsibility to read these correspondences. Monthly Maintenance Window To provide the best possible service, IT must regularly update and perform routine maintenance on its systems and networks. Some of these activities require that the affected systems and networks be shut down. While this work is essential, we also recognize that it presents an inconvenience."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_hpc-policies.html,1,"To enable those who use these systems to better plan for maintenance, we have guidelines for scheduling routine maintenance and upgrades to the HPC clusters as described below. A MONTHLY SCHEDULED MAINTENANCE OF UP TO 12 HOURS WILL BE TAKEN, IF NEEDED, BEGINNING AT 8AM ON THE FIRST MONDAY OF EACH MONTH Major scheduled maintenance and upgrade activities will take place, if needed, once per month. These will be scheduled for the first Monday of each month at 8am to noon to start these scheduled maintenance and upgrade activities. The maintenance period may often be brief or not used at all, but can last up to 12 hours if this amount of time is needed to complete the work. We have chosen early morning on the first Monday of each month for our maintenance work as it has been the time period during the week which has low usage on our clusters. A notification will be sent to all HPC account holders announcing any scheduled maintenance work in advance."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_hpc-policies.html,2,"A WEEKLY SCHEDULED MAINTENANCE OF UP TO FOUR HOURS (MONDAY 8 AM to NOON) MAY BE USED TO ADDRESS SMALLER MAINTENANCE AND UPGRADE NEEDS. This time will not be used if not needed. Storage Quotas and Policies Each individual user is assigned a standard storage quota (a limit on usage). Researchers who use more than their quota will be blocked from submitting jobs or using additional resources until they clean their file space and reduce their usage. The table below shows, for every storage space, the standard storage quotas for individual accounts and the cost to expand quotas beyond the standard quota. For details on quotas, see the HPC Storage Best Practices page. Storage policies are described under the Data Management page. Group Quotas An HPC sponsor may request that their quota and the quotas of their research group be combined to create a single larger space. Some conditions: Requests must be made by the sponsor. All of the members of the group must share the same sponsor."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_hpc-policies.html,3,"All group members must be active users of the HPC system. The sponsor's account will hold the full quota and each individual's quota will be set to 0. Requests will be considered by HPC team and assessed by evaluating the need for it and availability. To apply for a group quota please use the form at this link. You will receive a response to your request within 1 week. Automatic File Deletion Policy The table below describes the policy concerning the automatic deletion of files. HPC Hosting and Equipment Life Cycle Policy NYU IT data centers are secure, state-of-the-art facilities with 24/7 monitoring and redundant AC and power. All HPC equipment in IT data centers are taken out of service after 4 years. If used/refurbished equipment is put into the data center, this is measured from the original manufacture date rather than the date it was put into the data center. Allocation of space and other scarce resources in IT facilities is determined on a case by case basis."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_hpc-policies.html,4,All requests to qualify for this service and/or to extend the management life of a cluster beyond 4 years (for a maximum of 1 additional year) must be approved by the Research Faculty Advisory Group. Requests should be emailed to hpc@nyu.edu. For requests to extend beyond 4 years up to 1 additional year should include the following: Reason for extension request Plans for replacement or retirement of cluster Length of time of extension being requested
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_about_job-opportunities.html,0,"Job Opportunities Would you like to join one of the best academic HPC teams ? Apply for one of our positions. For the most up to date position information, please see the NYU Careers page. If you have any questions about our positions, please emails us at hpc@nyu.edu Full Time Positions Contractor Positions Student Positions"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc.html,0,"Accessing HPC Introduction There are several ways to interact with the Greene HPC cluster. Similar to other Linux clusters, the most common method of connection is via a Command Line Interface (CLI). A CLI is a program that allows you to create and delete files, run programs, and navigate through directories and files via a typed prompt. On Mac, the built-in CLI application is called Terminal. While some Windows 10 machines support a Linux Subsystem, which allows for similar functionality, a popular tool used to connect to a Linux server is a free application called PuTTY. The following sections will outline basic ways to connect to the Greene cluster. Access to the clusters is primarily handled via the Secure Shell Protocol, or ssh. Below we outline ways to connect via ssh on Mac, Linux, and Windows machines."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc.html,1,"Remote Connections with the NYU VPN & HPC Gateway Server If you are connecting from a remote location that is not on the NYU network (your home for example), you have two options: VPN Option: set up your computer to use the NYU VPN. Once you've created a VPN connection, you can proceed as if you were connected to the NYU net. Gateway Option: go through our gateway servers (example below). Gateways are designed to support only a very minimal set of commands and their only purpose is to let users connect HPC systems without needing to first connect to the VPN. You do not need to use the NYU VPN or gateways if you are connected to the NYU network (wired connection in your office or WiFi) or if you have VPN connection initiated. In this case you can ssh directly to the clusters."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc.html,2,"Command Line Interface (Use Terminal) Mac & Linux Access To connect to the gateway servers, simply open a terminal application and enter the following command: ssh <NetID>@gw.hpc.nyu.eduAfter typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to one of the HPC clusters: # this will connect you to Greene HPC clusterssh <NetID>@greene.hpc.nyu.edu Windows CMD Windows 10 users have several options. First, the CMD program on updated Windows 10 machines should contain an ssh client, allowing you to log into Greene or Hudson the same way as with a Linux terminal. Windows WSL2 If you run Windows 10, you can install WSL, and then install Ubuntu or other Linux distribution (for example, from Microsoft Store)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc.html,3,"You will have a fully functional Ubuntu with terminal and can connect to cluster using instructions provided above for Linux/Mac users Instructions on WSL installation: https://docs.microsoft.com/en-us/windows/wsl/install-win10 Tips: One of many options to get terminal that support tabs, etc. is to install 'Windows Terminal' from Microsoft Store. If you are using WSL 2 (Windows subsystem for Linux), you may not be able to access internet when Cisco AnyConnect VPN, installed from exe file, is activated. A potential solution: uninstall Cisco AnyConnect and install AnyConnect using Microsoft Store, and then setup new VPN connection using settings described on IT webpage. Setting Up SSH Keys Instead of typing your password every time you need to log in, you can also specify an ssh key. Only do that on the computer you trust Generate ssh key pair (terminal in Linux/Mac or cmd/WSL in Windows): https://www.ssh.com/ssh/keygen/ Note the path to ssh key files."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc.html,4,"Don't share key files with anybody - anybody with this key file can login to your account Log into cluster using regular login/password and then add the content of generated public key file (the one with .pub) to $HOME/.ssh/authorized_keys on cluster Next time you will log into cluster no password will be required. For additional recommendations on how to configure your SSH sessions, see this page. PuTTY (Windows) There are many SSH clients for Windows OS, but we recommend using PuTTY SSH if you have not already. Once it is installed, launch PuTTY and configure new session ""Session"" category as in the screenshot below: Here we are instructing PuTTY to connect to host gw.hpc.nyu.edu on port 22 using SSH protocol (note, that this interface allows you to save this connection configuration for future). Just like for Linux and Mac users, if you are connecting from the outside of NYU network, you need to go through the gateway servers."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc.html,5,"Once you click ""Open"", a terminal window with prompt for password will pop up. Enter your NetID password and you should be authorized on the gateway server. Gateways are designed to support only a very minimal set of commands and their only purpose it to let users access HPC systems. Once you are there type in an ssh command that will let you connect to Greene cluster : # Greene Loginssh greene.hpc.nyu.edu A new command line interface window will open up that prompts you for your password on the gateway server, from there you can connect to Greene by entering the following: ssh greene.hpc.nyu.eduOpen OnDemand (Web-based Graphical User Interface) Open OnDemand is an open source project funded by the National Science Foundation (NSF). Open OnDemand is designed to create easier access to users to interface with HPC systems. Originally developed by Ohio Supercomputer Center (OSC), used by many universities around the world, and now servicing the NYU Greene HPC cluster."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc.html,6,"Open OnDemand has a variety of convenient tools to manage files, access the command line, manage and monitor jobs, and launch interactive applications, such as Jupyter Notebooks, RStudio sessions, and even full Linux Desktops. Features Include: Easy file management - upload and download files, view HTML and pictures without downloading Command-line shell access without any SSH client locally installed Job management and monitoring Full Linux desktop experience without X11 Interactive Apps such as JupyterHub and RStudio without the need for port forwarding Open OnDemand (OOD) is accessible to all users with a valid NYU HPC account while on-campus network or through a VPN. To access OOD visit: https://ood.hpc.nyu.edu (VPN Required) Access the Shell Under the clusters menu you can select the Greene Shell Access option to access the Linux shell. No local SSH client is required. Interactive Applications GUI based applications are accessible without the need for port or X11 forwarding."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc.html,7,"Select the Interactive Apps menu, select the desired application, and submit the job based on required resources and options. Troubleshooting Connections to Open OnDemand A common issue that can occur is receiving an error that the Open OnDemand page cannot be reached. Sometimes this can indicate that the service is down, but often this is an issue with the the local browser cache. You can test this by opening a private browser window and seeing if https://ood.hpc.nyu.edu will load. If it does, try deleting the cache for https://ood.hpc.nyu.edu in your browser history to resolve this issue. In Chrome, this can be done by navigating to this page in your settings: chrome://settings/content/all?searchSubpage=ood.hpc.nyu.edu&search=site+data The link above will automatically search for the Open OnDemand site data and cookies. You can then simply click on the trashcan icon to delete the site cache. Once done, try navigating again to https://ood.hpc.nyu.edu and the site should load."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc.html,8,For other issues please email hpc@nyu.edu.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account.html,0,"Getting and Renewing an Account Click here to request HPC accounts. (NYU VPN is required) Who is eligible for an NYU HPC account? NYU HPC clusters and related resources are available to full-time NYU faculty and to all other NYU staff and students with sponsorship from a full-time NYU faculty. Please note: All sponsored accounts will be created for a period of 12 months, at which point a renewal process is required to continue to use the clusters Faculty, students and staff from the NYU School of Medicine require the sponsorship of an eligible NYU faculty member to access the NYU HPC clusters Non-NYU Researchers who are collaborating with NYU researchers must obtain an affiliate status before applying for an NYU HPC account. Please see the section at the bottom of the page. VPN In order to request a new HPC account or renew an expired one, you need to be connected to the NYU VPN if you are working remotely. Please see instructions on how to install and use the NYU VPN."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account.html,1,"Linux clients are not officially supported, however we were able to successfully use openVPN client. Here are installation and connection instructions for an apt-based linux distribution: # install withapt-get install openconnect # connect by running > sudo openconnect -b vpn.nyu.edu # when prompted follow the instructions and provide your netID, password, and second password ('push', 'phone1' or 'sms') This method was tested on multiple Linux laptops but is not guaranteed to work in future. Getting a new account on the NYU HPC clusters To request an NYU HPC account please log in to NYU Identity Management service and follow the link to “Request HPC account”. We have a walkthrough of how to request an account through IIQ. If you are a student, alumni or an external collaborator you need an NYU faculty sponsor. Renewing HPC account Each year, non-faculty users must renew their HPC account by filling in the account renewal form from the NYU Identity Management service."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account.html,2,"See Renewing your HPC account with IIQ for a walkthrough of the process. Information for faculty who sponsor HPC users All full-time NYU faculty members (other than NYU Med School) are eligible to become sponsors, and in turn can sponsor: NYU Degree program students Scholars visiting NYU NYU Research staff NYU School of Medicine faculty, staff and students Other NYU staff/affiliates with a NetID Non-NYU researchers with whom they are actively collaborating If you need to sponsor an HPC account for an external collaborator (for example, for an NYU alumnus), please, request a ""research affiliate"" affiliation for your collaborator. You can find the instructions here . You can request a NetID for your student(s) or collaborator(s) here. The request form has additional information about affiliates."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account.html,3,"HPC faculty sponsors are expected to: Approve/disapprove sponsored users' association with you Approve/disapprove the purpose for which user is requesting an account on NYU HPC resources Agree to supervise the sponsored individual, to the extent necessary, to ensure proper use of the NYU HPC resources and compliance with all applicable policies. Respond promptly to account-related requests from HPC staff Each year, your sponsored users must renew their account. You will need to approve the renewal by logging into the NYU Identity Management service. We have a walkthrough of the approval process, with screenshots. Bulk HPC Accounts for Courses HPC bulk accounts request is disabled for HPC sponsors."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account.html,4,"If you would like to use JupyterHub for your classes, please don't submit the form below, read here instead (the link to an intake form is also there) Please fill out this Request Form for the course, we’ll create HPC accounts for the class per request Please note that accounts created for courses last until the end of the semester, rather than a full year. Getting an account with one of NYU partners NYU partners (look for the list here) with many state and national facilities with a variety of HPC systems and expertise. Contact us for assistance setting up a collaboration with any of these. Non-NYU Researchers If you are part of collaboration with NYU researcher you need to obtain an affiliate status before applying for an NYU HPC account. A full-time NYU faculty member must sponsor a non-NYU collaborator for an affiliate status. Please see instructions for affiliate management (NYU NetID login is required to follow the link)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account.html,5,"Please read instructions about sponsoring external collaborators. Access to cluster after Graduation If you will still work on a project with an NYU researchers after graduation - refer to the section above for ""Non-NYU Researchers"" If you are not part of a collaboration, your access to cluster will end together with NetID becoming non-active. Please copy all your data from cluster (if you need any) before that time."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account_how-to-approve-an-hpc-account-request.html,0,"How to approve an HPC Account Request When someone nominates you as their HPC sponsor, you should be notified by email. You can also log into IIQ at any time, and if you have a request awaiting your approval, it will appear in your ""Actions Items"" box, as per the following screenshot: Another way to get to pending approvals is to click on the line item in the “Latest Approvals” section which will lead directly to the approval page. For new HPC Account Requests, the page will look like this: Here, the Approve or Deny button should be clicked, then confirmed, in order to complete the request. If you have any difficulties or questions, please contact us at hpc@nyu.edu."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account_hpc-accounts-for-sponsored-external-collaborators.html,0,"HPC Accounts for Sponsored External Collaborators External (non-NYU) collaborators can access, with proper sponsorship, the NYU HPC Environment. The first step is to sponsor a collaborator for an NYU netid (if they do not have one already). A department administrator or the faculty sponsor must submit the Affiliate Management Form (the link is only accessible over VPN, or within NYU-Net). Once a netid for the external collaborator is created, the collaborator must submit the Request for an NYU HPC account. The collaborator must setup VPN access to be able to access the HPC account request form. The collaborator must enter in the account request form the Netid of the sponsoring NYU Full time faculty The collaborator should select ""External Collaborator"" as Affiliation, when filing the HPC account request form. Once the HPC request is submitted, the sponsoring faculty will receive an email with a link to approve (or deny) the HPC account request for the external collaborator."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account_hpc-accounts-for-sponsored-external-collaborators.html,1,"The account approval link can only be accessed over VPN. Once the sponsoring faculty approves the account request, the HPC account is created within one hour. Once the HPC account is created, the external collaborator can access HPC resources as described here. Please Note: As with all sponsored accounts, HPC accounts for external collaborators are valid for a period of 12 months, at which point a renewal process is required to continue access to the NYU HPC environment."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account_renewing-your-hpc-account-with-iiq.html,0,"Renewing your HPC Account with IIQ Login to the URL given below, using your netid/password, to create or manage HPC Account Requests: https://identity.it.nyu.edu/ (NYU VPN is required) Upon logging in, an end user’s landing page will look like this If the menu does not appear, select the ""burger"" menu on the top left hand corner: The burger menu will show an ""Update/Renew HPC Account"" option - select this. Next complete the form as instructed. Please note that all accounts require the sponsorship of a full-time NYU faculty member. The user’s name will be pre-populated, and the forms required fields must be completed (sponsor, reason for request, consent to terms of use). After clicking “Submit” the chosen sponsor will be notified of the request and provisioning will only occur after approval."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account_renewing-your-hpc-account-with-iiq.html,1,"NOTE: If your HPC Account is due for renewal you will get an update on your dashboard which will suggest you to fill out a form given in the ""Latest form"" widget for renewing your account If you are not a full-time NYU faculty member, you will need an NYU faculty member to sponsor your application. This is probably your thesis supervisor, or NYU collaborator. Hit submit, and the request will go to your sponsor to approve (if applicable), and your account will be created, usually within a day of being approved. You will be returned to the dashboard, and now you should see your request in the ""Pending Approvals"" tables. If after a few days you still do not have an account, check with your sponsor - they may have missed a step in the approval process. If you are still stuck, contact us at hpc@nyu.edu for assistance."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account_requesting-an-hpc-account-with-iiq.html,0,"Requesting an HPC Account with IIQ First, check if you already have an account. You can check this by attempting to log in to the cluster, according to the instructions at ""Logging in to the NYU HPC Clusters"". Login to the URL given below, using your netid/password, to create or manage HPC Account Requests: https://identity.it.nyu.edu/ (NYU VPN is required) Upon logging in, an end user’s landing page will look like this NB: if the menu does not appear, select the ""burger"" menu on the top left hand corner: Navigate to Manage Accounts > Request HPC Account: You will be prompted with a form. Continue to fill it out and Submit. When an approver/sponsor/faculty logs in, it will look like this: with “QuickLinks” to both “Bulk HPC Account Request” and “Request HPC Account” and will show any recent or pending approvals or forms."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account_requesting-an-hpc-account-with-iiq.html,1,"If the “Request HPC Account” QuickLink is clicked, the following form appears: The user’s name will be prepopulated, and the forms required fields must be completed (sponsor, reason for request, consent to terms of use). After clicking “Submit” the chosen sponsor will be notified of the request and provisioning will only occur after approval. If the “Bulk HPC Account Request” QuickLink is clicked, the following form appears: The requestor’s name will be prepopulated, and the forms required fields must be completed (sponsor, list of netids, course identifier, reason for request, consent to terms of use). These requests are auto-approved since they are usually submitted by the sponsor themselves or a member of the HPC Admin team."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_accessing-hpc_getting-and-renewing-an-account_requesting-an-hpc-account-with-iiq.html,2,"Approving a Request: When a sponsor logs in to approve a request, clicking on the “Approvals” QuickLink will lead to a list of pending requests: Another way to get to pending approvals is to click on the line item in the “Latest Approvals” section which will lead directly to the approval page. For new HPC Account Requests, the page will look like this: Here, the Approve or Deny button should be clicked, then confirmed, in order to complete the request. For HPC Account Renewals, the page will look like this: Here, all systems should be selected by clicking the check box in the menu bar, and choosing “Select Everything”. Then, the “Select Bulk Action” menu is used to Approve or Reject all items selected. Please note that the line items may span multiple pages and all items must be acted upon in order to complete the request. Clicking “Complete” will complete the request."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_home.html,0,"High Performance Computing NYU High Performance Computing (NYU HPC) provides access to state of the art supercomputer hardware and cloud services to eligible faculty and students across all of NYU Announcements What We Do The NYU High Performance Computing team, within IT's Research Technology division, provides seamless and efficient access to advanced computing resources, outstanding services, and expertise that: Enable NYU scholars to work on the most challenging computational research projects that involve HPC, Big Data Analytics, and Artificial Intelligence Enable and participate in research collaborations Facilitate usage of computing resources, minimize barriers of entry, promote the use of best practices, and develop the computing skills of users by providing training and consultations Facilitate the use of HPC resources in courses Who Can Access NYU HPC Resources NYU HPC resources are available to full-time NYU faculty and research staff."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_home.html,1,"Access for NYU students and external collaborators requires sponsorship from full-time NYU faculty. Full-time NYU faculty and research staff Faculty-sponsored students, postdocs, adjuncts, and other non-full time faculty and research staff Faculty-sponsored external (non-NYU) collaborators Please Note: To apply or renew an NYU HPC user account click here Faculty, students and staff at NYU School of Medicine require the sponsorship of an eligible NYU faculty member at Washington Square to work on the NYU HPC clusters. External (non-NYU) collaborators may require sponsorship for an NYU netid before applying for HPC user account. The process is described here. All sponsored accounts are valid for a period of 12 months, at which point a renewal process is required to continue to use the clusters."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-projects.html,0,"HPC Projects We have introduced a new ""RIT Projects"" workflow enforced by the Slurm job scheduler. This will allow NYU and the Research Computing Team to make data-driven decisions and increase transparancy on utilization . This is an integral part of reporting on research in order to justify future investment in computational tools. The portal can be accessed at https://projects.rit.nyu.edu (VPN required) using your NYU email. The Portal enables users to: Register and manage project(s) Find how to submit jobs associated with project(s) Monitor usage associated with the project PIs can register projects and specify project members. PIs are responsible for the cluster utilization associated with projects. In order to have project associated resources and priority, users need to specify the Slurm account associated with a particular HPC project. If a Slurm account is not specified they will have limited resources. NOTE: The RIT Project Portal also provides access to NYU Private GenAI."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-projects.html,1,"If you would like to use the service please review requirements and apply here, https://www.nyu.edu/life/information-technology/artificial-intelligence-at-nyu/private-ai-generative-pilot.html. Roles at RIT Portal The roles available in RIT Projects: HPC Project Owner Project Manager Project Member Special Resource Approver Usage Viewer Depending on your role, you will see the corresponding interface at the Portal. HPC Project Owner Can register HPC projects. In order to be HPC Project Owner you must be a faculty member. lease get in touch with hpc@nyu.edu to add you to the list of instructors allowed to create an HPC project. Project Manager Project Owners may designate you a Project Manager. You may be a manager for multiple projects and multiple owners. Project Member The project owner or manager may add you to a project. Special Resources Approver Approves access to specific HPC resources. This role is assigned by the RIT team."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-projects.html,2,"If you believe you should be a special resource approver, but you don’t see that interface in the RIT Portal please email hpc@nyu.edu specifying the account and resources. Usage Viewer Administrators for schools, colleges, centers, institutes, and other units have access to an interface displaying the usage of projects that allocate part of the usage to that unit. If you believe you should be an usage viewer but you don’t see that interface - please contact as at hpc@nyu.edu RIT Portal Functionality As Project Member you can find out what slurm account you should use to submit jobs by going to ""Use HPC"" on top menu <add screenshot> For example, if your Slurm account is ""pr_5_general"" add the slurm submission option `--account=pr_5_general` <add screenshot> Within Open OnDemand (https://ood.hpc.nyu.edu), the option will be in ""additional slurm options"" <add screenshot> As Project Owner, you can Register new projects by going to ""Register New Project"" on top menu."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-projects.html,3,"<add screenshots> As Project Owner or Project Manager role, you can Add/Remove Managers and Members <screenshots> Request access to special resources (note: general resource is available by default) Top Menu: ""Project Settings"" -> ""Project resources"" -> ""Request / remove request for resource"" -> ""Choose resource name"" -> ""Request access"" button or ""Withdraw request"" button <add screenshots> Note: After that, you need to email the listed Approver When you get a confirmation - you need to activate the resource for the resource to become available for members to use Top Menu: ""Project Settings"" -> ""Project resources"" -> ""Manage approved resources"" -> ""Choose resource name"" -> Use the checkboxes to specify which resources should be active or not active -> Press button <add screenshots> Archive project. You can reactivate it at a later time."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-projects.html,4,"<why do we want to archive?> Top Menu: ""Project Settings"" -> ""Archive / Reactivate"" -> Select project -> Press button to archive or reactivate project <screenshots> View details on all the projects you can manage/view go to Project Details <add screenshots> Here, you can see Overview: Consolidated view of all the projects Details for the specific project Usage data"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-projects_projects-registration.html,0,"Projects Registration Details When registering a new project on the ""Register new project"" page, you will be asked to provide some information. This page may help you to get some clarity on what is expected. Please note that all of the entries for the project information can be later updated under Project Settings - Update Project info Project name A short project name. One project owner can't have two projects with the same name Project description A brief project description to help involved administrators learn what the project is about List of keywords A list of keywords to help identify trends and analysis School and school allocation If only one school/unit is involved - choose the school/unit and specify 100% If the project is a collaborative effort with contributions (time, budget, etc) coming from more than one school/unit, please specify all of them and put an estimated percentage attribution to each."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-projects_projects-registration.html,1,"Grants If you have multiple grants that belong to this project - specify them all and specify an allocation. If your grant is not listed, please choose [Other] and give details in the project description (if more than one grant should be specified in the description, please list one per line with percentage allocation included at the end of the line) NSF Research Area(s) Research area (s) that best fits your project. More than one can be selected. The list of options is based on the NSF list. Discipline/Strategic Area(s) This list is provided by the Research Administration of NYU. If you can't find one that is a good fit with your project, please choose [Other] and give details in the project description. Publications This field can be updated later when you have publications related to this project Important: Please remember to press the purple button ""Register New Project"" to save your changes in the top part of the page."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems.html,0,"HPC Systems Here is a list of systems managed centrally by the NYU HPC team in NYC and available to all NYU researchers, sponsored students, and eligible collaborators. Greene - HPC Cluster Greene - HPC Cluster A Cluster with > 30K Intel CPU cores and > 360 NVIDIA GPUs and AMD GPUs Storage Storage A GPFS file system with disk space greater than 9 PB. HDR Infiniband interconnect Cloud Computing Cloud Computing The HPC team assists in use of resources for research through Google Cloud Platform"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing.html,0,"Cloud Computing Resources Resources There are various services which can help you perform computations on scalable cloud infrastructure, get a simple computational environment for your work, or host your research applications. Google Cloud Platform There are various services which can help you perform computations on scalable cloud infrastructure, get a simple computational environment for your work, or host your research applications. Google Cloud Platform"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,0,"Getting Started on NYU Dataproc What is Hadoop? Hadoop is an open-source software framework for storing and processing big data in a distributed/parallel fashion on large clusters of commodity hardware. At its core, Hadoop strives to increase processing speed by increasing data locality (i.e., it moves computation to servers where the data is located). There are three components to Hadoop: HDFS (the Hadoop Distributed File System), the Hadoop implementation of MapReduce, and YARN (Yet Another Resource Negotiator; a scheduler). What is Dataproc? Dataproc is a cloud-based Hadoop distribution that is managed by Google. Google administers updates to Dataproc so that it is kept current. Google also packages and maintains additional software that can be run on top of Hadoop. Additionally Dataproc includes other cloud-specific features, such as the ability to automatically add/remove nodes depending upon how busy the cluster is (autoscaling)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,1,"It can also use object storage (GCS) or BigQuery as an alternative to HDFS, and provides integration with BigTable using HBase interfaces. Autoscaling NYU Dataproc is configured to be as cloud-agnostic as possible, and still uses HDFS and non-proprietary HBase components. It does, however, leverage autoscaling. This means that if the cluster hasn't been used for a while, it might take a while for resources to become available (typically 3-5 minutes) as nodes are turned on and NodeManagers register with YARN. During this time, the following warning message will appear and indicate that the cluster is at capacity and resources are not available: WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources This warning will go away after new nodes have been added by autoscaling and more resources are available for YARN applications."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,2,"If it does not go away after more than 10 minutes, please contact the HPC team. Autoscaling is actively monitored, but a duration of more than 10 minutes may indicate a failure of Dataproc's monitoring infrastructure. Currently, NYU Dataproc's autoscaling is configured so that the cluster will have between 3 and 43 nodes depending upon demand. The number of nodes that are currently active can be seen in the YARN web UI. Additionally, percentage of cluster capacity that is used can be seen on the Scheduler page in the lefthand menu in the YARN web UI. Accessing the NYU Dataproc Hadoop Cluster Access to the NYU Dataproc cluster is granted via your NYU Google account. If you are in a class that uses Dataproc, your instructor or TA will request Dataproc access for your NetID. Once this is granted you can log in by navigating to https://dataproc.hpc.nyu.edu/ssh in your web browser."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,3,"After you’ve reached this page you will have access to a browser-based terminal interface where you can run Hadoop commands. If you are having difficulty connecting to a terminal, please make sure that you are not logged in to a non-NYU Google account by clicking the icon displayed in the upper right corner in the Gmail web interface (see here). If you are using Google Chrome, you may also need to switch to your NYU account profile using the instructions here. If you continue to have difficulties connecting via https://dataproc.hpc.nyu.edu/ssh, you can also log in by navigating to https://shell.cloud.google.com/ and running the following command in the terminal that appears: gcloud compute ssh nyu-dataproc-m --project=hpc-dataproc-19b8 --zone=us-central1-f You may need to authorize Google to log in to Dataproc after running the above command. Once logged in, your username will be of the form <your_net_id>_nyu_edu rather than just your NetID (unlike Peel)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,4,"HDFS HDFS stands for Hadoop Distributed File System. HDFS is a highly fault-tolerant file system and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for analyses that use large datasets. File Permissions and Access Control Lists You can share files with others using access control lists (ACLs). An ACL gives you per-file, per-directory and per-user control over who has permission to access files."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,5,"You can see the ACL for a file or directory with the getfacl command: $ hdfs dfs -getfacl /user/<net_id>_nyu_edu/testdir # To modify permissions for files or directories, use setfacl: $ hdfs dfs -setfacl -R -m user:<net_id>_nyu_edu:rwx /user/<net_id>_nyu_edu/testdir $ hdfs dfs -setfacl -R -m default:user:<net_id>_nyu_edu:rwx /user/<net_id>_nyu_edu/testdir To open the subdirectory permission to others, you need to open each higher level directory's navigation permission too: $ hdfs dfs -setfacl -m user:<net_id>_nyu_edu:--x /user/<net_id>_nyu_edu Uploading Data to HDFS from Your Computer Small Transfers You can add smaller files to HDFS by copying them to your local filesystem / non-HDFS home directory and then copying them from there to HDFS. Note that there is a limit on the size of your local filesystem home directory, so you should only perform these steps for smaller amounts of data- for larger datasets you should use the method described in the Large Transfers section."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,6,"Navigate to the command line interface by going to http://dataproc.hpc.nyu.edu/ssh. In the upper right portion of the header banner, select Upload File. Use the web browser dialog to select your file. Once the file is uploaded to your Unix directory, run the following command to copy it into HDFS: hdfs dfs -put /home/<your_netid>_nyu_edu/<path_to_file> <hdfs_path> To retrieve data from HDFS and copy it to your local filesystem home directory, you can use one of the following commands: hdfs dfs -get <hdfs_path> /home/<your_netid>_nyu_edu/<path_to_file> hdfs dfs -copyToLocal <hdfs_path> /home/<your_netid>_nyu_edu/<path_to_file> You can then download data by going to the upper right corner of the window in the command line interface and selecting the Download File option and entering a file path (i.e., /home/<your_netid>_nyu_edu/<path_to_file>)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,7,"To list files in HDFS, use the following command: hdfs dfs -ls Large Transfers To upload large datasets to HDFS, first navigate to the data ingest website at https://dataproc.hpc.nyu.edu/ingest. The data ingest website provides a web interface for temporary cloud-based bucket storage. Any datasets that are uploaded to the data ingest website will remain there for 2 days. Before these 2 days have elapsed, you will need to upload your datasets to your HDFS home directory. To do that you can use the following command after logging in: hadoop distcp gs://nyu-dataproc-hdfs-ingest/<file_or_folder_name> /user/<your_net_id>_nyu_edu You can find the full path to your file/folder in the ingest storage by clicking on it in the web interface and then scrolling down to the gsutil URI field in the Live Object tab. Warning: Data uploaded into the ingest website will be visible to all members of the cluster temporarily."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,8,"If you are uploading files that cannot be shared with all cluster users (e.g., code) please use the alternate method described below. Uploading Data to HDFS from Greene First download gcloud on a desktop computer with a browser by following the instructions here. Ensure that you run the install command described as optional in step 4. Log into Greene and activate the Google Cloud command line interface module: ml load google-cloud-sdk/379.0.0 Then log into Google Cloud by typing the following: gcloud auth login Copy and paste the command that you are given into a terminal application on your desktop and run it. When prompted, type y to proceed. If you are signed into multiple Google accounts, you will then be presented with a browser window where you can choose your account. Select your NYU account. Google will then present a message saying that “Google Cloud SDK wants to access your Google Account”. Click Allow."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,9,"Copy the URL that you are given in the terminal window, and paste it into your Greene session on the line where gcloud asks for it. Type gcloud auth list to verify that you are logged in: [jp6546@hlog-1 ~]$ gcloud auth list Credentialed Accounts ACTIVE ACCOUNT * jp6546@nyu.edu To set the active account, run: $ gcloud config set account `ACCOUNT` Now that you are logged in, use the instructions under the Small Transfers or Large Transfers sections below to upload your data. Small Transfers Run the following commands on Greene to ensure that gcloud knows that you are using it with Dataproc (rather than a different Google Cloud application): gcloud config set project hpc-dataproc-19b8 gcloud config set compute/zone us-central1-f Run the following command on Greene to upload your file to your filesystem home directory on Dataproc: gcloud compute scp MYFILE nyu-dataproc-m:~ If you are prompted to give a passphrase while generating an SSH key, hit enter twice."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,10,"The above commands are the command line equivalent to the upload dialogue at http://dataproc.hpc.nyu.edu/ssh that is mentioned earlier. Your file should now be available within your filesystem home directory on Dataproc. You can then run the following command to get it into HDFS: hdfs dfs -put /home/<your_netid>_nyu_edu/<path_to_file> <hdfs_path> Large Transfers On Greene, run the following to upload a single file to the staging bucket: gsutil cp FILE gs://nyu-dataproc-hdfs-ingest Or run the following to copy a directory: gsutil rsync -r DIRECTORY gs://nyu-dataproc-hdfs-ingest The above commands are the command line equivalents to the data ingest website described earlier. As with the earlier example, run the following from within Dataproc to ingest the dataset into your HDFS home directory: hadoop distcp gs://nyu-dataproc-hdfs-ingest/<file_or_folder_name> /user/<your_net_id>_nyu_edu Computation on Dataproc MapReduce What Is MapReduce?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,11,"MapReduce is a programming model and an associated implementation for processing and generating large datasets with a parallel, distributed algorithm on a cluster. A MapReduce job splits a large dataset into independent chunks and organizes them into key-value pairs for parallel processing. The mapping and reducing functions receive not just values, but (key, value) pairs. Every MapReduce job consists of at least two parts: The Mapper The Reducer Mapping Phase: Takes input as <key,value> pairs, processes them, and produces another set of intermediate <key,value> pairs as output. Reducing Phase: Reducing lets you aggregate values together. A reducer function receives an iterator of input values from an input list. It then combines these values together, returning a single output value. MapReduce Word Count Example Ingest a text file into HDFS."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,12,"In the below example, the hdfs -put command is combined with another Unix command (curl) to send a copy of a Sherlock Holmes book (located at the URL https://www.gutenberg.org/files/1661/1661-0.txt) directly into HDFS: curl -o input.txt https://www.gutenberg.org/files/1661/1661-0.txt hdfs dfs -put input.txt The following command will run an example Word Count job (described in more detail here) with the Sherlock Holmes book as its input. hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount -D mapreduce.job.maps=6 -D mapreduce.job.reduces=2 /user/<netid>_nyu_edu/input.txt /user/<netid>_nyu_edu/output Spark What is Spark? Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,13,"Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. (source: Wikipedia) Launching an Interactive Spark Shell Spark provides an interactive shell that you can use to learn the Spark API and analyze datasets interactively. To connect to Spark Shell from the command line, execute the following command: spark-shell --deploy-mode client --num-executors=1 --driver-memory=1G --executor-memory=1GNote: NYU Dataproc deploys Spark applications in cluster mode by default. The following error indicates that you are trying to deploy an interactive shell, which must use client mode: Exception in thread ""main"" org.apache.spark.SparkException: Cluster deploy mode is not applicable to Spark shells. To resolve this error, either use the command line flag indicated above (--deploy-mode client) or set the spark.submit.deployMode property in your Spark configuration to client."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,14,"More details about the difference between cluster and client mode can be found here. YARN Scheduler YARN is the resource manager and job scheduler in the Hadoop cluster. YARN allows you to use various data processing engines for batch, interactive, and real-time stream processing of data stored in HDFS. Application status and logs Please find the list of current running apps using 'Yarn' script. Running the yarn script without any arguments prints the description for all commands. $ yarn application -listTo kill a currently running app because the submitted app started malfunctioning or in worst case scenario, it's stuck in an infinite loop."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,15,"Get the app ID and then kill it as given below $ yarn application -kill <application_ID>To download application logs for examination on the command line $ yarn logs -applicationId <application_ID>Using Hive Apache Hive is a data warehouse software package that facilitates querying and managing large datasets residing in distributed storage (i.e., HDFS). Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called Hive Query Language (HiveQL or HQL). You can access Hive with the following command: beeline -u jdbc:hive2://localhost:10000 0: jdbc:hive2://localhost:10000> use <netid>_nyu_edu; 0: jdbc:hive2://localhost:10000> show tables; 0: jdbc:hive2://localhost:10000> !quit Closing: 0: jdbc:hive2://localhost:10000 It is important to note that in order to exit properly from a beeline session, you type ""!quit""."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,16,"If you are planning on using SerDe to query/work with JSON files, you will need to run the following code at the Beeline prompt first in order to ensure that the JsonSerDe class is loaded: ADD JAR /usr/lib/hive/lib/hive-hcatalog-core.jar See here for more information. Access to Hive databases on NYU Dataproc is derived from HDFS permissions because we use Storage-Based Authorization . To grant read-only access to a Hive database to someone other than yourself, you can run the following command: hdfs dfs -setfacl -R -m user:<OTHER_PERSON_NETID>_nyu_edu:r-x /user/hive/warehouse/<YOUR_NETID>_nyu_edu.db Outside of NYU, other Hadoop installations may use a different mechanism to share databases with other colleagues– it is common for Hadoop installations to use a SQL style grant/revoke mechanism for sharing databases ( SQL Standards Based Authorization )."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,17,"This mechanism is not used at NYU and it is important to bear in mind that external documentation referring to grant/revoke statements is not applicable to NYU Dataproc. Using Presto (deprecated now) Using Trino Trino is a distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources. To access Trino, you can type the following command: trino Once you are inside, you can reference multiple data sources through catalogs (see here). For instance, you may want to query Hive using Trino."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,18,"You can select a database to use through a fully-qualified database or table name as shown in the code below: trino> show catalogs; Catalog ---------------------- bigquery bigquery_public_data hive mastersql memory system tpcds tpch (8 rows) Query 20240827_132732_00001_ygn47, FINISHED, 2 nodes Splits: 20 total, 20 done (100.00%) 0.17 [0 rows, 0B] [0 rows/s, 0B/s] trino> use hive.<netid>_nyu_edu; USE You can also specify a catalog / data source that you want to use on the command line when you start Trino: trino --catalog=hive Using Conda NYU Dataproc comes with miniconda3 by default. This can be used to manage Python packages within your filesystem home directory. The main difference between using conda on NYU Dataproc and using conda on Peel is that conda is enabled on NYU Dataproc by default, so you do not need to use the module command to load it. See here or here for more information on the conda command."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,19,"Using Jupyter Notebooks Log into the Dataproc cluster and run jupyter-notebook. Do not close the command line interface where jupyter-notebook is running until you're done using Jupyter. From the output produced by jupyter-notebook, obtain the port number that the notebook is running on."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,20,"In the example below, for instance, the notebook is running on port 8888: To access the notebook, open this file in a browser: file:///home/jp6546_nyu_edu/.local/share/jupyter/runtime/nbserver-7866-open.html Or copy and paste one of these URLs: http://localhost:8888/?token=90d9c6297ba2c963cdb998ae374041384bac71c781b18ed1 or http://127.0.0.1:8888/?token=90d9c6297ba2c963cdb998ae374041384bac71c781b18ed1 On an individual workstation that has the gcloud command installed (installation instructions for gcloud can be found here), run the following command (with PORT replaced with the port number from step 2): gcloud compute ssh nyu-dataproc-m --project hpc-dataproc-19b8 --zone us-central1-f -- -N -L PORT:localhost:PORT In our example, from the output in step 2 this command would be as follows: gcloud compute ssh nyu-dataproc-m --project hpc-dataproc-19b8 --zone us-central1-f -- -N -L 8888:localhost:8888 You can then use the URLs from the jupyter-notebook output in step 2 (e.g., http://localhost:8888/?token=90d9c6297ba2c963cdb998ae374041384bac71c781b18ed1) to access your notebook from the workstation."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,21,"When you are done, you can exit the terminals where the jupyter-notebook and the gcloud commands are running. Using Zeppelin Notebooks Log into the Dataproc cluster and run zeppelin start. The terminal will output three pieces of information that you will need later: username, password, and port number."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,22,"The output should look something like this: jp6546_nyu_edu@nyu-dataproc-m:~$ zeppelin start Zeppelin is starting with the following configuration: ------------------------------------------------------- Username: jp6546_nyu_edu Password: REDACTED Port: 64739 Zeppelin start [ OK ] On an individual workstation that has the gcloud command installed (installation instructions for gcloud can be found here), run the following command (with PORT replaced with the port number from step 1): gcloud compute ssh nyu-dataproc-m --project hpc-dataproc-19b8 --zone us-central1-f -- -N -L PORT:localhost:PORT In our example this command would be as follows using the output from step 1 : gcloud compute ssh nyu-dataproc-m --project hpc-dataproc-19b8 --zone us-central1-f -- -N -L 64739:localhost:64739 In a web browser, navigate to localhost:PORT. Log in by clicking the Login button in the upper right corner. Use your credentials from step 1."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_dataproc.html,23,"If you forget the password or the port number at any time, you can run the following commands to retrieve this information: zeppelin get-port zeppelin get-pass When you are finished, run ""zeppelin stop"" to turn off the Zeppelin server."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_gcp-self-managed-projects.html,0,"GCP Self-Managed Projects Google Cloud Platform for Research The NYU Research Cloud team is working to provide NYU scholars with access to the public cloud in Google Cloud Platform (GCP) for their research projects. NYU is a member of the Internet2 Net+ GCP program that allows for community-negotiated GCP terms that provides NYU researchers with benefits that include, amongst other things, price discounts, waivers for data egress fees, and NIH STRIDES initiative benefits. In addition to the I2 Net+ GCP benefits, NYU scholars can enjoy significant discounts in using GCP resources in their research project through a 3 year commitment (started in 2019) NYU made in GCP. The NYU network connects with GCP via Partner Interconnect using Internet2 Cloud Connect (I2CC) service. Why work with the NYU Research Cloud team to deploy your research project on GCP?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_gcp-self-managed-projects.html,1,"NYU researchers who work with the Research Cloud team to deploy projects on GCP may benefit from the following: Discounted rates, lower GCP project cost: Through NYU's participation in the Internet2 Net+ agreement, as well as the 3-year commitment NYU made to using GCP, GCP projects enjoy discounted rates, lowering the cost of the project. The exact discounts depend on the GCP service used in the research project and can vary between 5% and 25%. Free data egress is usually included. GCP Expertise and project setup: The NYU research cloud team can work closely with researchers to Identity and Access Management, groups access, establish billing, etc."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_gcp-self-managed-projects.html,2,"GCP Support cases: Given proper permissions the NYU Research Cloud team can open support cases on behalf of the researchers or directly discuss GCP project issues with GCP experts Cost monitoring, billing alerts, and spending reports: The Research Cloud team has access to additional tools that can provide cost monitoring, switch between billing options and provide spending reports. Research project funding options: The NYU research cloud team can help with providing invoices, internal fund transfers, establishing GCP Billing ids, etc. that can enable NYU researchers to pay for GCP services Getting Started with GCP There are a number of ways you can get started with using GCP in research projects. To start using GCP resources in a research project, request a consultation with the Research Cloud team (via email research-cloud-support@nyu.edu). The Research Cloud team can advise on ways to set up your research project, available discounts, etc."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_gcp-self-managed-projects.html,3,"If a project involves Data Science and Machine Learning, consult with the DS3 team before starting your project on GCP. Please note: Creating a GCP project using your NYU account (netid@nyu.edu) will place the project under the nyu.edu organization, an environment managed by the NYU Research Cloud team. However, your project doesn't automatically qualify for the price discounts and benefits that NYU has negotiated with GCP. For non-NYU work on GCP, please use your personal, non-NYU, Google email when creating GCP projects. GCP Training The NYU Research Cloud team does not currently offer training on how to deploy and utilize resources on GCP in research projects or teaching. If you are new to GCP and you want to learn the GCP fundamentals or if you want to learn how to perform specific tasks on GCP (obtain skill badges), please consider the following resources: Through the Google For Education program, GCP offers training credits and discounts to Students, Faculty, and IT Admins."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_gcp-self-managed-projects.html,4,"To apply for training credits and discounts, please click here. Getting started with Google Cloud Platform offers quick starts and sample projects on GCP. How can I fund my research project on GCP? - GCP Credits for Research GCP Free Tier: Apply for credits using your NYU account (https://cloud.google.com/free/) After credits expire, if you would like to switch to another type of funding and are approved to do so, we will modify your project so it can use other funds Various source of funding for GCP project are possible. Please consider options below and explore other options which may exist for your specific field. One of the following or a combination of them can be used NSF CloudBank GCP seed grant through NYU HPC Departmental funds Apply using this form GCP project request form"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_gcp-self-managed-projects_gcp-for-research-form.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_gcp-self-managed-projects_gcp-for-research-form.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing GCP for Research Form Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_hpc-bursting-to-cloud.html,0,"Google Cloud Bursting Current Bursting Status HPC may provide bursting capabilities to researchers or classes, in some cases, in order to augment the available resources. Bursting is ideal for when you need a large amount of resources for a very short period of time. The way that bursting is made possible is by running a scalable SLURM cluster in the Google Cloud Platform (GCP), which is separate from the on-premise HPC clusters. Bursting is not available to all users and requires advanced approval. In order to get access to these capabilities, please contact hpc@nyu.edu to check your eligibility. Please let us know the amount of storage, total CPUs, Memory, GPU, the number of days you require access, and the estimated total CPU/GPU hours you will use. For reference, please review the GCP cost calculator. Please send a copy of your cost calculation to hpc@nyu.edu as well. To request access to the HPC Bursting capabilities, please complete this form."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_hpc-bursting-to-cloud.html,1,"Running a Bursting Job Note: this is not public, only per request of eligible classes or researchers ssh <NetID>@greene.hpc.nyu.edussh to the class on GCP (burst login node) - anyone can login but you can only submit jobs if you have approval ssh burstStart an interactive job srun --account=hpc --partition=interactive --pty /bin/bashIf you got an error ""Invalid account or account/partition combination specified"" it means your account is not approved to use cloud bursting. Once your files are copied to the bursting instance you can run a batch job from the interactive session. Access to Slurm Partitions In the example above the partition ""interactive"" is used. You can list current partitions by running command sinfoHowever, approval is required to submit jobs to the partitions. Partitions are set up by the resources available to a job, such as the number of CPU, amount of memory, and number of GPUs."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_hpc-bursting-to-cloud.html,2,"Please email hpc@nyu.edu to request access to a specific partition or create a new partition (e.g. 10 CPUs and 64 GB Memory) for more optimal cost/performance of your job. Current Limits 20,000 CPUs available at any given time for all active bursting users Storage Greene's /home and /scratch are mounted (available) at login node of bursting setup. Compute node however, do have independent /home and /scratch. These /home and /scratch mounts are persistent, are available from any compute node and independent from /home and /scratch at Greene. User may need to copy data from Greene's /home or /scratch to GCP mounted /home or /scratch When you run a bursting job the compute nodes will not see those file mounts. This means that you need to copy data to the burst instance. The file systems are independent, so you must copy data to the GCP location. To copy data, you must first start an interactive job."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_hpc-bursting-to-cloud.html,3,"Once started, you can copy your data using scp from the HPC Data Transfer Nodes (greene-dtn). Below is the basic setup to copy files from Greene to your home directory while you are in an interactive bursting job: scp <NetID>@greene-dtn.hpc.nyu.edu:/path/to/files /home/<NetID>/Visualization Workstations The burst cluster includes a partition (nvgrid) that can be used to run graphical applications on NVIDIA GPUs for visualization purposes. You can use this partition by following the instructions below. Add the following to your SSH config file (~/.ssh/config) on your local workstation so that you can log into the burst login node directly: Host burst HostName burst.hpc.nyu.edu User <NetID> ProxyJump <NetID>@greene.hpc.nyu.edu ProxyJump <NetID>@burst.hpc.nyu.edu StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR Log into the burst login node by running ssh <NetID>@burst while on-campus or connected to the VPN."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_hpc-bursting-to-cloud.html,4,"Run the following command on the login node to request an interactive command line session: srun --account=hpc --partition=nvgrid --gres=gpu:p100:1 --pty /bin/bash When your interaction session is active, run the following command to start the VNC (remote desktop) server. If this is the first time you’ve used a visualization node, you will be prompted to set a password to use when you access your remote session: /opt/TurboVNC/bin/vncserver Note the hostname of the node that you are running on. This hostname is displayed in the NODELIST column of the output from the squeue command: [jp6546@b-23-1 ~]$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) … 92727 nvgrid bash jp6546 R 2:55 1 b-23-1 In another terminal on your local machine, run the following command: ssh -N -L 5901:<Hostname>:5901 <NetID>@burst This command will ensure that you can connect to the remote desktop service from your local computer."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_hpc-bursting-to-cloud.html,5,"If you do not already have a VNC remote desktop client installed on your computer, you will need to install one. A list of VNC clients available for various platforms can be found here. Note that Mac OS X comes with a built-in VNC client, which is accessible from the Finder by navigating to Go → Connect to Server and then typing vnc:// at the beginning of the server field. Within your VNC client, connect to localhost:5901 (vnc://localhost:5901 on Mac OS X). You should now be presented with a desktop environment. If you are using any OpenGL-based applications that are started from a terminal, be sure to type vglrun before the command name in order to ensure that the application uses the GPU. After your first time using the nvgrid partition, you can start the remote desktop server non-interactively using the following batch script (although you will need to remember the password that you set in step 3)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_hpc-bursting-to-cloud.html,6,Note that the sleep command should have the length of time that you want the server to run (in seconds) after it (3600 seconds for 1 hour in the example below). #!/bin/bash #SBATCH --gres=gpu:p100:1 #SBATCH --partition=nvgrid #SBATCH --account=hpc #SBATCH --job-name=vnc #SBATCH --time=1:00 #SBATCH --output=slurm_%j.out /opt/TurboVNC/bin/vncserver sleep 3600
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_hpc-bursting-to-cloud_hpc-bursting-request-form.html,0,"HPC Bursting Request Form Please Note: You must be signed into your NYU account to view and complete this form. For a direct link, click here. Please Note: You must be signed into your NYU account to view and complete this form. For a direct link, click here."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_matlab-online.html,0,"MATLAB Online NYU students/faculty/staff can use MATLAB online. How to access How to access Create account using your NYU email at https://matlab.mathworks.com/ Login at https://matlab.mathworks.com/ Available resources Available resources Generally, specifications and limitations can be found here You can check characteristics of the VM by running in MATLAB console !free -g !lscpu At the moment of writing, this command returns 125 GB RAM and 16 CPUs One can use MATLAB drive to upload/store data (up to 5GB)"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_nih-strides.html,0,"NIH STRIDES Introduction The NIH Science and Technology Research Infrastructure for Discovery, Experimentation, and Sustainability (STRIDES) Initiative allows NIH to explore the use of cloud environments to streamline NIH data use by partnering with commercial providers (GCP/AWS)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_nih-strides.html,1,"The NIH STRIDES Initiative: Provides biomedical researchers with access to advanced, cost-effective cloud computing infrastructure, tools, and resources Enable researchers to work collaboratively in the cloud by establishing an interconnected ecosystem of biomedical research data Equip researchers with emerging cloud solutions for data management & computation to enable experimentation and innovation The benefits of participating in NIH STRIDES program include: Pre-negotiated favorable pricing for cloud services Access to training to help researchers harness the power of the cloud Receive opportunities for professional service engagements to help drive success Receive guidance for best practices in areas such as data storage, governance, and controlled access Enrolling to the NIH STRIDES initiative NYU has enrolled in the NIH STRIDES initiative in December 2020 by signing an agreement with Carahsoft, GCP's billing and administrative partner."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_nih-strides.html,2,"Thus NIH-funded NYU researchers with an active NIH award may take advantage of the STRIDES Initiative for their NIH-funded research projects. The NYU High Performance Computing (HPC) team works closely with Burwood Group, a GCP reseller, to provide access to GCP resources for NYU researchers who are approved to participate in the NIH STRIDES initiative. NYU researchers who wish to participate must follow the steps outlined below. Step 1: Email the NYU HPC Cloud team (hpc-cloud@nyu.edu) answers to the following questions or submit the ServiceLink form. NIH Institution/Center/Office: (e.g. National Cancer Institute, national Institute of Environmental Health Studies, etc.) NIH Grant Number: Program and Project Name: *This identifies the project locally, and those conducting the research. It is a component of the full account name. Example <org>.<project name> Estimated GCP spend: estimate of what the consumption will be (We want this to set up some budgeting alerts."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_nih-strides.html,3,"It doesn’t limit the project). Billing Admin Name: NYU High Perfomance Computing GCP Billing Admins Billing Admin Email: hpc-gcp-billing-admins@nyu.edu Primary Contact: Name of Principal Investigator Primary Contact email: Email for the person listed in #7 Program Officer Name (First and Last Name) *This is the NIH Program Officer or Lab Chief working with the project. This person should give consensus to cloud adoption from an NIH programmatic side and should approve cloud funding for the project. This person will receive reports on usage and cost. Program Officer Email: email for the person listed in #8 NYU Chartfield: Please provide an NYU chartfield that can used to fund the GCP project. You can still apply for NIH STRIDES initiative benefits even without providing an NYU chartfield. However, you may choose to provide an NYU chartfield if you plan to continue using the GCP project after the NIH project funding ends."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_nih-strides.html,4,"Step 2: PIs email the NYU HPC Cloud team confirming that they agree to the following statement: ""PI <enter Full Name> agrees to receive the applicable billing discounts established by the NIH STRIDES Initiative, provided that such services exclusively support NIH-funded research activities. If any services billed hereunder do not exclusively support NIH-funded research activities PIs will be in breach of this agreement"" Step 3: Once the NYU HPC team receives the answers to the above questions (Step 1) and the PIs agreement (Step 2), they will work with Burwood Group and NIH to confirm the project eligibility for NIH STRIDES initiative. Burwood will respond to the PI (listed in questions #7 and 8 above) about the decision. Step 4: If the request is approved, Burwood will issue a quote for the amount specified in Step 1, Item #4 ""Estimated GCP Spend"" above."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_nih-strides.html,5,"Step 5: Once the Purchase Order (PO) has been issued, the NYU HPC Research Cloud team will create a GCP project under the nyu.edu org and provide the PI with access to the GCP project resources (VMs, storage, etc.) During the project, Burwood group will be billing monthly towards the PO. More information about the NIH STRIDES Initiative Read more About the NIH STRIDES Initiative Questions for joined NIH STRIDES INITIATIVE & Internet2 CALL – JUNE 4, 2020 Contact For general questions about Research Cloud/GCP please email the NYU HPC Research Cloud team: hpc-cloud@nyu.edu To learn more about the NIH STRIDES Initiative, email the team at STRIDES@nih.gov"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_cloud-computing_penguin-on-demand.html,0,"Penguin On Demand General Info Users of NYU HPC have access to Penguin On Demand (POD) cluster, providing a bare-metal HPC computing environment in the cloud (more information). The cluster provides access to 160 AMD GPUs for NYU researchers, 20 of which are dedicated solely to NYU use. If interested in using POD, please email hpc@nyu.edu with your inquiry. About In 2020, AMD and technology partner Penguin Computing Inc. announced the donation of a petascale supercomputer to NYU as part of the broad effort to combat COVID-19 and future pandemics. AMD also announced it would contribute access to a cloud based system powered by AMD EPYC and AMD Radeon Instinct processors located on-site at Penguin Computing, providing remote super computing capabilities for selected researchers around the world via Penguin On Demand (POD)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene.html,0,"Greene The NYU Greene high performance computing (HPC) cluster is a powerful system that bolsters research across a wide range of disciplines, from genomics and biomolecular genetics to the political ramifications of social media behavior to artificial intelligence. Highlights 350 in the June 2022 edition of the Top500 list (CPU only, 2.088 petaflops, Linpack) The overall performance of Greene (CPUs and GPUs) exceeds 4 petaflops (10 times more powerful than its predecessor, the NYU HPC Prince cluster)NVIDIA and AMD GPUs cards Greene is a general purpose cluster that supports a variety of job types and sizes, including jobs requiring multiple CPU cores, multiple GPU cards, terabytes of memory, to single core serial jobs. Greene went into production on November 18th, 2020 when the NYU President Andrew Hamilton announced the new cluster in a ribbon-cutting ceremony. Greene is Green! Most of the cluster compute nodes are water-cooled using Lenovo's Neptune liquid cooling technology."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene.html,1,"The remaining nodes are air-cooled and are deployed in a heat-containment area, reducing the need for ambient air cooling in the data center. The combination of the liquid cooling technology, the heat containment arrangement, and the Power Utilization Efficiency (PUE) of the data center makes Greene an efficient and environmentally-friendly cluster - in short, Greene is green. The Greene cluster, as all other new NYU HPC clusters, is located in a new space (5,000 sq ft) in a data center colocation facility."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_best-practices.html,0,"Best Practices on Greene Scheduling jobs in an efficient and fair manner can be a challenging task in a multi-user environment. Here we provide some recommendations. What resources to ask for (and how much)? When asking for compute resources in the batch script, never ask for more than you need. There are two important reasons listed below. Imagine you need only 2 CPUs but request 10. You will wait longer for job to start: It is easier for scheduler to find 2 CPUs than 10 You will make other users to not be able to run jobs: 8 cores will be sitting idle and no other users will be able to use them. The same argument applies to other types of resources: RAM, GPUs (and potentially other 'TRES' in SLURM terminology). Very often, even for parallel codes, it makes sense to request fewer CPU cores. Main example of this is -- your parallel job doesn't scale well."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_best-practices.html,1,"For example, imagine that using 12 cores instead of 4, you can reduce execution time from 1 hour (on 4 cores) to 45 minutes (on 12 cores). Yes, 45 minutes is faster, but you are using 3 times as many resources. Additionally, it may happen that your job with 4 cores will only wait in the queue for 5 minutes, but the 12-core job would need to wait for 20 minutes. This completely offsets all the gains from using 8 more cores. In the end you are to judge, what is really needed, but you should always make this type of considerations when creating a job script. We recommend you don't specify a specific GPU unless needed. If it isn't very important to run your job on a particular GPU device (for example a powerful V100), then request any GPU with #SBATCH --gres=gpu:1And only when you are absolutely sure that you need a V100, use #SBATCH --gres=gpu:v100:1. V100s are often in high demand and average wait time for these GPUs is higher than for any other type of GPU devices."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_best-practices.html,2,"It may be (and very often is) beneficial to run your job on a less powerful accelerator, but spend less time waiting in the queue for your job to start. Do NOT run CPU heavy jobs on login nodes Login nodes designed use Logging In Running low-compute operations like copying files, installing packages, etc Submitting compute jobs using Slurm (see Getting Started) Important: check if you are wasting RAM and CPU! Another useful command that allows you to better understand how resources were utilized by completed jobs is seff: [~]$ seff 8932105Job ID: 8932105Cluster: greeneUser/Group: NetID/GROUPIDState: COMPLETED (exit code 0)Cores: 1CPU Utilized: 02:22:45CPU Efficiency: 99.99% of 02:22:46 core-walltimeJob Wall-clock time: 02:22:46Memory Utilized: 2.18 GBMemory Efficiency: 21.80% of 10.00 GBThis example shows statistics on a completed job, that was ran with a request of 1 cpu core and 10Gb of RAM."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_best-practices.html,3,"While CPU utilization was 100%, RAM utilization was very poor -- only 2.2GB out of requested 10GB was used. This job's batch script should definitely be adjusted to something like #SBATCH --mem=2250MB Check resources usage of a currently running job Run job. When it is scheduled you can use squeue to figure out node name, where job is scheduled. From login node, do ssh <node-name>top -u $USERTake a look how fully you use CPUs and how much RAM your jobs are using. For a GPU job also run nvidia-smiTake a look how much GPU processing power your job is using. It may happen that your code does not scale well, and it is better to use 1 or 2 GPUs instead of 4 You can also take a look at GPU memory utilization. RAM specification in sbatch file is for CPU RAM, not GPU memory! Please request only as much RAM as your job needs! Is my job scalable? How efficiently I use multiple CPUs/GPUs Every code is different. Test it! Run your job with 1 CPU, then 2 CPUs, then 4 CPUs, etc. Make a plot!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_best-practices.html,4,"How much improvements you get from adding more CPUs? Run your job with 1 GPu, then 2 GPUs, then 4 GPUs, etc. Make a plot! How much improvements you get from adding more GPUs? Most of our nodes don't have NVlink, which may make scaling across GPUs much less efficient comparing to systems which do Why my jobs are queued? To understand why your job is waiting in the queue you can run squeue -j <JobID> -o ""%.18i %.9P %.8j %.8u %.8T %.10M %.9l %.6D %R""Last column of the output would indicate a reason. You can find out more about squeue output format from man squeue."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_best-practices.html,5,"The column ""NODELIST(REASON)"" in the end is job status due to the reason(s), which can be : Priority: higher priority jobs exist Resources: waiting for resources to become available BeginTime: start time not reached yet Dependency: wait for a depended job to finish QOSMaxCpuPerUserLimit: number of CPU cores limit is reached JobArrayTaskLimit: limit of tasks in the JobArray is reached For a more complete list of possible values of REASON, please refer to man squeue under the section JOB REASON CODES. Limits on resources you can request Within SLURM there are multiple limits defined on different levels and applied to different objects. Some of the important limits are listed here. Number of jobs per user 2000 Job lifetime Limited to 7 days (168 hours), but you can request an extension by emailing HPC team (hpc@nyu.edu). CPU, GPU, RAM These limits depend on the time you request for the job: ""short queue"" (under 48 hours, or 2 days) or ""long queue"" (under 168 hours, or 7 days)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_best-practices.html,6,"These limits may be updated by the HPC team, based on the cluster usage patterns. In order to obtain the up to date numbers you have to run the following command In the output look at MaxWall to determine queue you are interested in (short or long). Now look under MaxTRESPU to find limits for CPU, RAM, GPU. Here is an example of output you may get MaxWall MaxTRESPU Name----------- ---------------------------------------- ---------- normal 2-00:00:00 cpu=3000,mem=6000G cpu48 7-00:00:00 cpu=1000,mem=2000G cpu168 2-00:00:00 gres/gpu=24 gpu48 7-00:00:00 gres/gpu=4 gpu168 04:00:00 cpu=48,gres/gpu=4 interact gres/gpu=96 gpuamd 12:00:00 cpu=20000,mem=10000G cpulowHow many CPU cores per GPU These limits are frequently updated by the HPC team, based on the cluster usage patterns. Due to this, the numbers below are not exact and should only be used as general guidelines."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_best-practices.html,7,"Here are some of these limits: | # gpus | max_cpus | max_memory | gpu type = ""V100"" |--------+----------+------------|| 1 | 20 | 200 || 2 | 24 | 300 || 3 | 44 | 350 || 4 | 48 | 369 | gpu type = ""rtx8000"" |--------+----------+------------|| 1 | 20 | 200 || 2 | 24 | 300 || 3 | 44 | 350 || 4 | 48 | 369 | gpu type = ""a100"" |--------+----------+------------|| 1 | 28 | 250 || 2 | 32 | 300 || 3 | 60 | 400 || 4 | 64 | 490 | gpu type = ""mi50"" |--------+----------+------------|| 1 | 48 | 200 || 2 | 72 | 300 || 3 | 76 | 350 || 4 | 80 | 370 || 5 | 84 | 400 || 6 | 88 | 430 || 7 | 92 | 460 || 8 | 96 | 490 ||--------+----------+------------| From this table you can for example see, that a job asking for 8 V100 GPUs will not be queued. Another example is that requests for 2 V100s and 48 cores will also not be granted. How to find more information on my jobs?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_best-practices.html,8,"Some other useful SLURM commands that can help to get information about running and pending jobs are # detailed information for a job:scontrol show jobid -dd <jobid># show status of a currently running job# (see 'man sstat' for other available JOB STATUS FIELDS)sstat --format=TresUsageInMax%80,TresUsageInMaxNode%80 -j <JobID> --allsteps# get stats for completed jobs # (see 'man sacct' for other JOB ACCOUNTING FIELDS)sacct -j <jobid> --format=JobID,JobName,MaxRSS,Elapsed# the same information for all jobs of a user:sacct -u <username> --format=JobID,JobName,MaxRSS,ElapsedHow busy is the cluster? Please refer to the Systems Status page to see the number of jobs in the queue and other metrics. Often your jobs are queued for a simple reason -- cluster is very busy and there aren't enough resources available. Best way to submit large number of similar jobs The correct way to submit such jobs is to use the array job functionality of SLURM. This reduces load on the scheduler system."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_best-practices.html,9,"Don't make your own loops to do this kind of work. You can find a detailed description on how to submit such jobs here. Web scraping, Data mining from websites If you need to do web scraping, don't do that on Greene. It is not allowed due to security concerns and waste of CPU resources (most of the time a job would spend on downloading files, instead of using CPU). Please contact us, and we will advise on a better workflow for your project. Error handling We recommend using #!/bin/bash -e instead of plain #!/bin/bash, so that the failure of any command within the script will cause your job to stop immediately rather than attempting to continue on with an unexpected environment or erroneous intermediate data. It also ensures that your failed jobs show a status of FAILED in sacct output. When a batch job is finished it produces an exit code (among other useful data)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_best-practices.html,10,"To view the error code of the job you can use: sacct -b -j <JobID>When reaching out to the HPC team asking for help with failing jobs, it is useful to find an exit code from the job at question. Check efficiency of your jobs Please check on this page SSH Issues Some people may experience connection warnings while connection to Greene, and connections being terminated too soon. This can be addressed by entering the following into ~/ssh/config # Increase alive intervalHost greene.hpc.nyu.edu dtn.hpc.nyu.edu StrictHostKeyChecking no ServerAliveInterval 60 ForwardAgent yes StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERRORMore information on SSH can be found on the SSH Tunneling page."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started-amd-nodes.html,0,"Getting Started with AMD nodes Getting Maximum Performance from AMD GPUs AMD nodes are dedicated to AI/GPU workloads. Do not run CPU-only jobs on AMD nodes. Using AMD nodes In order to work with AMD nodes please login to Greene. Please refer to the Accessing HPC page to read about other options. SSH using Terminal or Command Prompt Simply open a terminal (Linux, Mac) or the Command Prompt (Windows 10) and enter the commands: ssh <NetID>@gw.hpc.nyu.edu ## you can skip this step if you are on the NYU Network or using the NYU VPNssh <NetID>@greene.hpc.nyu.edu NOTE: When you are asked to enter password, just type it (letters will not be displayed), and then hit ""Enter"" Submitting a Job to AMD nodes To submit a job to the AMD GPUs, you submit a job the same way you would on Greene, with one major change: specify --gres=gpu:mi50:1 (or another appropriate AMD GPU model) Use Singularity images containing pre-installed software."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started-amd-nodes.html,1,"You can use overlay to install additional packages as described here Software/Modules The modules on the cluster are built for Intel CPUs, not AMD, so do not use the modules that are installed. We recommend using Singularity and setting up an Anaconda environment to manage your packages. For information on setting up a conda environment please see our reference page. The usable Singularity containers are located at /scratch/work/public/singularity/hudson/images and begin with ""rocm4.X"" - other Singularity containers are compiled for NVIDIA GPUs and should not be used. To learn how to install python packages using singularity and overlay files please read Python packages Use versions of packages that are designed for AMD (ROCm). When installing, make sure the installation process will compile packages from source, instead of installing a wheel or conda precompiled package."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started-amd-nodes.html,2,"As an example, those who uses ternsorFlow or PyTorch can install the following packages pip install torch torchvision==0.10.1 -f https://download.pytorch.org/whl/rocm4.2/torch_stable.html pip install tensorflow-rocm Minimal PyTorch Example For a simple example you can use the overlay image with ROCm versions of PyTorch and TensorFlow installed - overlay-10GB-400K-rocm-tensorflow-pytorch.ext3 OVERLAY_FILE=/scratch/work/public/examples/amd-getting-started/overlay-10GB-400K-rocm-tensorflow-pytorch.ext3SINGULARITY_IMAGE=/scratch/work/public/singularity/hudson/images/rocm4.2-ubuntu20.04.sifsingularity exec --overlay $OVERLAY_FILE $SINGULARITY_IMAGE /bin/bashsource /ext3/miniconda3/bin/activate Using your Singularity Container in a SLURM Batch Job Below is an example script of how to call a python script, in this case torch-test.py, from a SLURM batch job using your new Singularity image torch-test.py: (for convenience it is available at /scratch/work/public/examples/amd-getting-started/torch-test.py ) #!/bin/env pythonimport torch print(torch.__file__)print(torch.__version__) # How many GPUs are there?print(torch.cuda.device_count()) # Get the name of the current GPUprint(torch.cuda.get_device_name(torch.cuda.current_device())) # Is PyTorch using a GPU?print(torch.cuda.is_available()) Now we will write the SLURM job script, run-test.SBATCH, that will start our Singularity Image and call the torch-test.py script."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started-amd-nodes.html,3,"run-test.SBATCH: (for convenience it is available at /scratch/work/public/examples/amd-getting-started/run-test.SBATCH ) #!/bin/bash#SBATCH --nodes=1#SBATCH --ntasks-per-node=1#SBATCH --cpus-per-task=1#SBATCH --time=1:00:00#SBATCH --mem=2GB#SBATCH --gres=gpu:mi50:1 ## USE HUDSON CLUSTER#SBATCH --job-name=torch module purge singularity exec \ --overlay /scratch/work/public/examples/amd-getting-started/overlay-10GB-400K-rocm-tensorflow-pytorch.ext3:ro \ /scratch/work/public/singularity/hudson/images/rocm4.2-ubuntu20.04.sif \ /bin/bash -c ""source /ext3/env.sh; \ python /scratch/work/public/examples/amd-getting-started/torch-test.py"" Run the run-test.SBATCH script sbatch /scratch/work/public/examples/amd-getting-started/run-test.SBATCHCheck your SLURM output for results, an example is shown below cat slurm-3752662.out# example output: # /ext3/miniconda3/lib/python3.8/site-packages/torch/__init__.py # 1.9.1+rocm4.2 # 1 # VEGA # True Software installation To learn how to install python packages using singularity and overlay files please read page Singularity with Miniconda Note: An example above is using an overlay image with has being created by installing pip install torch torchvision==0.10.1 -f https://download.pytorch.org/whl/rocm4.2/torch_stable.html pip install tensorflow-rocm One can find up to date versions of those packages here PyTorch (go to Install section, choose Linux - ROCm) AMD GPU Tutorials AMD Tutorials (COVID-19 HPC Fund- Hackathon Trainings)"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started.html,0,"Getting Started on Greene Accessing the Greene Cluster There are several ways one can use to access the cluster. We only show one example below. Please refer to the Accessing HPC page to read about other options. SSH using Terminal or Command Prompt Simply open a terminal (Linux, Mac) or the Command Prompt (Windows 10) and enter the commands: ssh <NetID>@gw.hpc.nyu.edu ## you can skip this step if you are on the NYU Network or using the NYU VPNssh <NetID>@greene.hpc.nyu.edu NOTE: When you are asked to enter password, just type it (letters will not be displayed), and then hit ""Enter"" Computations on Greene There are several types of jobs that can be run on the Greene cluster. The traditional type of workload for HPC is called a ""batch job"" - where a job is sent to the system to execute a function or program without further user input or interaction. Users can submit batch jobs to the SLURM scheduler (a software which manages the jobs queue on Greene)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started.html,1,"For other job types please read here. Writing Your Code Using a text editing program such as vim, nano, or emacs, create a python script. Below is an example called 'hello-world.py': import osprint(""Hello from the compute node"", os.uname()[1])The script loads the os library in order to print out the name of the server that the script is run on. Step 1. Allocating Resources Using SLURM To run our python script we need to submit a batch job to SLURM to allocate the necessary compute resources. SLURM expects file in the following format in order to execute the job. The file contains both commands specific for SLURM to interpret as well as programs for it execute."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started.html,2,"Below is a simple example of a batch job to run the python script we just created, the file is named ""hello-python.sbatch"": #!/bin/bash #SBATCH --nodes=3 # requests 3 compute servers #SBATCH --ntasks-per-node=2 # runs 2 tasks on each server #SBATCH --cpus-per-task=1 # uses 1 compute core per task #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=python-hello-world #SBATCH --output=python-hello-world.out module purge module load python/intel/3.8.6 python ./hello-world.py You then submit the job with the following command: $ sbatch hello-python.sbatchThe command will result in the job queuing as it awaits resources to become available (which varies on the number of other jobs being run on the cluster). You can see the status of your jobs with the following command: $ squeue -u $USERThere are many more ways how you can allocate resources in SLURM. Please read here for more details."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started.html,3,"Reading the Job Output Once the job has completed, you can read the output of your job in the python-hello-world.out file. This is where logs regarding the execution of your job can be found, including errors or system messages. You can print the contents to the screen from the directory containing the output file with the following command: $ cat python-hello-world.outPython natively will only run on a single CPU core. This is why the output contains only one printed statement, even though we requested several nodes and 2 tasks per node. Your output may look something like the following: Hello from the compute node cs240.nyu.clusterWe can make our script use all the resources allocated by SLURM. We can accomplish this by parallelizing the batch script, which we will do in the next step. Step 2. Running Your Script on Multiple Nodes/Cores After we allocated multiple nodes resources (Step 1), we can run scripts on multiple nodes."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started.html,4,"One of the most popular software tools to run code on multiple nodes/cores at HPC is ""Open MPI"". Here is a simple example of using Open MPI with the job we ran before #!/bin/bash #SBATCH --nodes=3 # requests 3 compute servers #SBATCH --ntasks-per-node=2 # runs 2 tasks on each server #SBATCH --cpus-per-task=1 # uses 1 compute core per task #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=python-hello-world #SBATCH --output=python-hello-world.out module purge module load python/intel/3.8.6 module load openmpi/intel/4.0.5 # NOTE: we are loading ""openmpi"" module here srun python ./hello-world.py # NOTE: we can use ""srun"" or ""mpirun"" in front of python command The mpirun command will execute our hello-world.py script using each requested node/core."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started.html,5,"You can see it in the output: Hello from the compute node cs240.nyu.clusterHello from the compute node cs239.nyu.cluster Hello from the compute node cs240.nyu.cluster Hello from the compute node cs239.nyu.cluster Hello from the compute node cs241.nyu.cluster Hello from the compute node cs241.nyu.cluster Using mpirun, the batch job ran the hello-world.py script on three nodes on two separate CPU cores. Step 3. Parallelizing the Script Now you can modify your program, so it can leverage available resources. As an example, in Python, you can use package mpi4py."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started.html,6,"First install package by executing in terminal module load python/intel/3.8.6 python3 -m venv ./test_venv source ./test_venv/bin/activate module load openmpi/intel/4.0.5 pip install mpi4py Create a new python file 'hello-world_parallel.py' (more examples can be found here) import osfrom mpi4py import MPIcomm = MPI.COMM_WORLDrank = comm.Get_rank()size = comm.Get_size()host = os.uname()[1]print(f""hello from process {rank} on host {host}"") data = (rank+1)**2data_collected = comm.gather(data, root=0) if rank == 0: sum = 0 for i in range(len(data_collected)): sum = sum + data_collected[i] print(f""on rank {rank} I've calculated sum = {sum}"") MPI.Finalize() # compare the result to:# sum([i**2 for i in range(N+1)])# where N is a number of MPI ranks you are using And modify batch file #!/bin/bash #SBATCH --job-name=pythonMPIexample #SBATCH --nodes=4 #SBATCH --ntasks-per-node=2 #SBATCH --cpus-per-task=1 #SBATCH --mem=2GB #SBATCH --time=01:00:00 module purge module load python/intel/3.8.6 module load openmpi/intel/4.0.5 source /scratch/<NetID>/test_venv/bin/activate srun /scratch/<NetID>/test_venv/bin/python ./hello-world_parallel.py GPU Jobs To request one GPU, use SBATCH directives in job script #SBATCH --gres=gpu:1#SBATCH --gres=gpu:v100:1 ## To request specific GPU (v100 or rtx8000) We recommend to use Singularity images containing pre-installed software for GPU jobs."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started.html,7,"You can use overlay to install additional packages as described here. Here is an example with an overlay image containing Miniconda and PyTorch 'overlay-10GB-400K-tensorflow-pytorch.ext3' srun --pty -c 2 --mem=10GB --gres=gpu:rtx8000:1 /bin/bash # request an RTX8000 GPU nodeSINGULARITY_IMAGE=/scratch/work/public/singularity/ubuntu-20.04.1.sif OVERLAY_FILE=/scratch/work/public/examples/greene-getting-started/overlay-15GB-500K-pytorch.ext3## Note: this overlay file has PyTorch installed using command ## conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia ## Run interactively on the GPU node with singularity exec --nvsingularity exec --nv --overlay $OVERLAY_FILE $SINGULARITY_IMAGE /bin/bashsource /ext3/miniconda3/bin/activate ## Exit the Singularity sessionSingularity> exit ## Exit interactive sessionexit Now we can run SLURM job script 'run-test.SBATCH', that will start our Singularity Image and call the 'torch-test.py' script."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started.html,8,"Note: If you started an interactive job with singularity above, exit it first. Further Examples/Instructions There are other kinds of jobs that can be run on Greene like interactive jobs and jobs arrays. The links below give further information on the usage of SLURM. Examples available on Greene /scratch/work/public/examplesGraphical User Interface (GUI) Tools (using Open OnDemand) Open OnDemand is a tool that allows users to launch Graphical User Interfaces (GUIs) based applications are accessible without modifying your HPC environment. Login Before you can use Open OnDemand, you need to login to cluster at least once using terminal (this will create home directory) - otherwise OOD won't connect You can log into the Open OnDemand interface at https://ood.hpc.nyu.edu. Once logged in, select the Interactive Apps menu, select the desired application, and submit the job based on required resources and options."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_getting-started.html,9,"Software available through Open OnDemand Jupyter Notebook with Conda environments and Singularity: instructions Several IDEs/Apps are available, and can be accessed using ""Interactive Apps"" menu item Software You can view software available on Greene as well as instructions on installing new software at the Greene Software page. Best Practices Please read"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_hardware-specs.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_hardware-specs.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing Hardware Specs Note: hover a mouse over a cell with a black triangle to see more details Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,0,"Software on Greene Software Overview There are different types of software packages available Use ""module avail"" command to see preinstalled software. This includes the licensed software listed below Singularity Containers You can find those already built and ready to use, at location /scratch/work/public/singularity/ For more information on running software with Singularity, click here.Python/R/Julia packages can be installed by a user If you need another linux program installed, please contact us at hpc@nyu.edu Software and Environment Modules Lmod, an Environment Module system, is a tool for managing multiple versions and configurations of software packages and is used by many HPC centers around the world."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,1,"With Environment Modules, software packages are installed away from the base system directories, and for each package, an associated modulefile describes what must be altered in a user's shell environment - such as the $PATH environment variable - in order to use the software package. The modulefile also describes dependencies and conflicts between this software package and other packages and versions. To use a given software package, you load the corresponding module. Unloading the module afterwards cleanly undoes the changes that loading the module made to your environment, thus freeing you to use other software packages that might have conflicted with the first one."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,2,"Below is a list of modules and their associated functions: module unload <module-name> : unload a module module show <module-name> : see exactly what effect loading the module will have with module purge : remove all loaded modules from your environment module load <module-name> : load a module module whatis <module-name> : Find out more about a software package module list : check which modules are currently loaded in your environment module avail : check what software packages are available module help <module-name> : A module file may include more detailed help for the software package Package Management for R, Python, & Julia, and Conda in general Examples of software usage on Greene Examples can be found under /scratch/work/public/examples/ and include the following alphafold amd GPUs comsol c-sharp crystal17 fluent gaussian hadoop-streaming julia jupyter notebooks knitro lammps matlab mathematica namd orca quantum-espresso R sas schrodinger Singularity slurm spark stata squashfs trinity vnc vscode xvfb Accessing Datasets with Singularity Licensed Software SCHRODINGER Schrödinger provides a complete suite of software solutions with the latest advances in pharmaceutical research and computational chemistry."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,3,"The NYU New York campus has a limited number of licenses for the Biologics Suite (ConfGen, Epik, Jaguar, Jaguar pKa, MacroModel, Prime, QSite, SiteMap), BioLuminate and the Basic Docking Suite. NOTE: Schrödinger can be used for non-commercial, academic purposes ONLY. Using SCHRODINGER on HPC Cluster To load Schrodinger module execute $ module load schrodinger/2021-1Using SCHRODINGER on NYU Lab Computers Request your account at: https://www.schrodinger.com/request-account Download the software at: https://www.schrodinger.com/downloads/releases Contact NYU-HPC team to request your license file. These license servers are accessible from NYU subnet."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,4,"Please see the following links for installation of the license file: https://www.schrodinger.com/kb/377238 https://www.schrodinger.com/license-installation-instructions To check licenses status # module load schrodinger/2021-1 # load schrodinger if not already loaded# licadmin STAT # licutil -jobs## For example: [wang@cs001 ~]$ licutil -jobs######## Server /share/apps/schrodinger/schrodinger.licProduct & job type JobsBIOLUMINATE 10BIOLUMINATE, Docking 1BIOLUMINATE, Shared 10CANVAS 50COMBIGLIDE, Grid Generation 11COMBIGLIDE, Library Generation 50COMBIGLIDE, Protein Prep 11COMBIGLIDE, Reagent Prep 1EPIK 11GLIDE, Grid Generation 11GLIDE, Protein Prep 11GLIDE, SP Docking 1GLIDE, XP Descriptors 1GLIDE, XP Docking 1IMPACT 11JAGUAR 5JAGUAR, PKA 5KNIME 50LIGPREP, Desalter 1LIGPREP, Ionizer 3511LIGPREP, Ligparse 1LIGPREP, Neutralizer 1LIGPREP, Premin Bmin 1LIGPREP, Ring Conf 1LIGPREP, Stereoizer 1LIGPREP, Tautomerizer 1MACROMODEL 5MACROMODEL, Autoref 5MACROMODEL, Confgen 5MACROMODEL, Csearch Mbae 5MAESTRO, Unix 1000MMLIBS 3511PHASE, CL Phasedb Confsites 1PHASE, CL Phasedb Convert 1PHASE, CL Phasedb Manage 1PHASE, DPM Ligprep Clean Structures 1PHASE, DPM Ligprep Generate Conformers 5PHASE, MD Create sites 1PRIME, CM Build Membrane 2PRIME, CM Build Structure 2PRIME, CM Edit Alignment 2PRIME, CM Struct Align 18PRIME, Threading Search 2QSITE 5SITEMAP 10 Schrodinger Example Files Example SBATCH jobs and outputs are available to review here: /scratch/work/public/examples/schrodinger/COMSOL COMSOL is a problem-solving simulation environment, enforcing compatibility guarantees consistent multiphysics models."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,5,"COMSOL Multiphysics is a general-purpose software platform, based on advanced numerical methods, for modeling and simulating physics-based problems. The package is cross-platform (Windows, Mac, Linux). The COMSOL Desktop helps you organize your simulation by presenting a clear overview of your model at any point. It uses functional form, structure, and aesthetics as the means to achieve simplicity for modeling complex realities. NOTE: This license is for academic use only with Floating Network Licensing in nature i.e., authorized users are allowed to use the software on desktops. Please contact hpc@nyu.edu for the license. However, COMSOL is also available on NYU HPC cluster Greene. In order to check what Comsol licenses are available on Greene use comsol_licenses command in your terminal session. Several versions of COMSOL are available on the HPC cluster."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,6,"To use COMSOL on the Greene HPC cluster, please load the relevant module in your batch job submission script: module load comsol/5.6.0.280To submit a COMSOL job in a parallel fashion, running on multiple processing cores, follow below steps. Create a directory on ""scratch"" as given below. Copy example files to your newly created directory cp /scratch/work/public/examples/comsol/test-input.mph /scratch/<net_id>/example/ Edit the slurm batch script file (run-comsol.sbatch) to match your case (for example chance location of the run directory). Once the slurm batch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file for detail output information. MATHEMATICA Mathematica is a general computing environment with organizing algorithmic, visualization, and user interface capabilities. The many mathematical algorithms included in Mathematica make computation easy and fast."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,7,"To run Mathematica on the Greene HPC cluster, please load the relevant module in your batch job submission script: module load mathematica/12.1.1Note: In the example below the module is loaded already in the sbatch script. To submit a batch Mathematica job for running in a parallel mode on multiple processing cores, follow below steps. Create a directory on ""scratch"" as given below. Copy example files to your newly created directory. cp /scratch/work/public/examples/mathematica/basic/run-mathematica.sbatch /scratch/<net_id>/example Edit the slurm batch script file (run-mathematica.sbatch) to match your case (for example chance location of the run directory). Once the sbatch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file generated."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,8,"SAS SAS is a software package which enables programmers to perform many tasks, including: Information retrieval Data management Report writing & graphics Statistical analysis and data mining Business planning Forecasting and decision support Operations research and project management Quality improvement Applications development Data warehousing (extract, transform, load) Platform independent and remote computing. There are licenses for 2 CPUs on the HPC Cluster. Running a parallel SAS job on HPC cluster (Greene): To submit a SAS job for running on multiple processing elements, follow below steps. Create a directory on ""scratch"": Copy example files to your newly created directory. Submit as shown below. After successful completion of job, verify output log file generated. MATLAB MATLAB is a technical computing environment for high performance numeric computation and visualization."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,9,"MATLAB integrates numerical analysis, matrix computation, signal processing, and graphics in an easy to use environment without using traditional programming. MATLAB on personal computers and laptops NYU has a Total Academic Headcount (TAH) license which provides campus-wide access to MATLAB, Simulink, and a variety of add-on products. All faculty, researchers, and students (on any NYU campus) can use MATLAB on their personal computers and laptops and may go to the following site to download the NYU site license software free of charge. https://www.mathworks.com/academia/tah-portal/new-york-university-618777.html MATLAB can be used for non-commercial, academic purposes. There are several versions of Matlab available on the cluster and the relevant version can be loaded. module load matlab/2020bmodule load matlab/2021a In order to run MATLAB interactively on the cluster, start an interactive slurm job, load the matlab module and launch an interactive matlab session in the terminal."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,10,"Mathworks has provided a Greene Matlab User Guide that presents useful tips and practices for using Matlab on the cluster. STATA Stata is a command and menu-driven software package for statistical analysis. It is available for Windows, Mac, and Linux operating systems. Most of its users work in research. Stata's capabilities include data management, statistical analysis, graphics, simulations, regression and custom programming. Running a parallel STATA job on HPC cluster (Greene): To submit a STATA job for running on multiple processing elements, follow below steps. Create a directory on ""scratch"": cd /scratch/<net_id>/example Copy example files to your newly created directory. Submit using sbatch. After successful completion of job, verify output log file generated. GAUSSIAN Gaussian uses basic quantum mechanic electronic structure programs."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,11,"This software is capable of handling proteins and large molecules using semi-empirical, ab initio molecular orbital (MO), density functional, and molecular mechanics calculations. The NYU Gaussian license only covers PIs at the Washington Square Park campus. We will grant access to you after verifying your WSP affiliation. For access, please email hpc@nyu.edu. Running a parallel Gaussian job on HPC cluster (Greene): To submit a Gaussian job for running on multiple processing elements, follow below steps. Create a directory on ""scratch"": Once the sbatch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file generated. Knitro Knitro is a commercial software package for solving large scale mathematical optimization problems."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software.html,12,"Knitro is specialized for nonlinear optimization, but also solves linear programming problems, quadratic programming problems, systems of nonlinear equations, and problems with equilibrium constraints. The unknowns in these problems must be continuous variables in continuous functions; however, functions can be convex or nonconvex. Knitro computes a numerical solution to the problem—it does not find a symbolic mathematical solution. Knitro versions 9.0.1 and 10.1.1 are available. Running a parallel Knitro job on HPC cluster (Greene): To submit a Knitro job for running on multiple processing elements, follow below steps. Create a directory on ""scratch"": cd /scratch/<net_id>/example Copy example files to your newly created directory. There is no sample sbatch script available for knitro. After creating your own sbatch script you can execute it as follows:"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_conda-environments-python-r.html,0,"Conda Environments (Python, R) What is Conda? Package, dependency and environment management for any language—Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN, and more. https://docs.conda.io/en/latest/ Conda provides a great way to install packages that are already compiled, so you don't need to go over the long compilation process. If a package you need is not available, you can install it (and compile it when needed) using pip (Python) or install.packages (R). Reproducibility note: One of the ways to ensure the reproducibility of your results is to have an independent conda environment in the directory of each project (one of the options shown below). This will also keep conda environment files away from your /home/$USER directory. Advantages/disadvantages of using Conda Advantages A lot of pre-compiled packages (fast and easy to install) Note for Python: pip also offers pre-compiled packages (wheels). List can be found here https://pythonwheels.com/."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_conda-environments-python-r.html,1,"However, Conda has a significantly larger number of pre-compiled packages. Compiled packages use highly efficient Intel Math Kernel Library (MKL) library Disadvantages Conda does not take advantage of packages already installed in the system (while virtualenv and venv does - link) As you will see below, you may need to do additional steps to keep track of all installed packages (including those installed by pip and/or install.packages) Initializing Conda Load anaconda module module purgemodule load anaconda3/2020.07 Conda init can create problems with package installation, so we suggest using source activate instead of conda activate, even though conda activate is considered a best practice by the Anaconda developers. Automatic deletion of your files This page describes the installation of packages on /scratch."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_conda-environments-python-r.html,2,"One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged (read more). Thus you can consider the following options Reinstall your packages if some of the files get deleted You can do this manually You can do this automatically. For example, within a workflow of a pipeline software like Nextflow Pay for ""Research Project Space"" - read more here Use Singularity and install packages within a corresponding overlay file - read more here Python Load anaconda module module purgemodule load anaconda3/2020.07 IMPORTANT: keep your program/project in /scratch and create conda environment using '-p' parameter."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_conda-environments-python-r.html,3,"This will keep all the files inside the project's directory, instead of putting in in your /home/$USER conda create -p ./penv python=3 ## environment will be created in project directoryconda activate ./penv Also, you need to create a symbolic link, so conda will download files for packages to be installed into scratch, not your home directory."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_conda-environments-python-r.html,4,"mkdir /home/<NetID>/.condamkdir /scratch/<NetID>/conda_pkgsln -s /scratch/<NetID>/conda_pkgs /home/<NetID>/.conda/pkgsInstall pre-compiled packages available in conda: https://anaconda.org/anaconda/repo conda install -c anaconda pandasOther packages may be installed (and compiled when needed) using pip pip install <package_name>Important: conda and packages install by default to ~/.local/lib/python<version> If you did use 'pip install --user' to install some packages (without conda or other virtual environment), they will be available in ~/.local/lib/python<version> The primary takeaway: Let say you have tornado v.6 installed in ~/.local/lib/python<version>, and tornado v.5 installed by ""conda install"". When you will do ""conda activate"" you will have tornado v.6 available!! Not v.5!!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_conda-environments-python-r.html,5,"(this behaviour is the same for packages installed by to ~/.local/lib/python<version> before or after you create your conda environment) pip freeze will give v.6 conda list will give v.5 Solution To overcome this, do ""export PYTHONNOUSERSITE=True"" after conda activate R Load anaconda module module load anaconda3/2020.07IMPORTANT: keep your program/project in /scratch and create conda environment using '-p' parameter."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_conda-environments-python-r.html,6,"This will keep all the files inside the project's directory, instead of putting them in your /home/$USER conda create -p ./renv r=3.5 ## environment will be created in project directory## OR conda create -c conda-forge -p ./penv r-base=3.6.3 ## environment will be created in project directory conda activate ./renv Install pre-compiled packages available in conda: https://docs.anaconda.com/anaconda/packages/r-language-pkg-docs/ conda install -c r r-dplyrOther packages may be installed (and compiled) using install.packages() install.packages(""<package_name>"")Reproducibility Packages installed only using conda Save a list of packages (so you are able to report environment in publication, and to restore/reproduce env on another machine at any time) # saveconda list --export > requirements.txt # restore conda create -p ./penv --file requirements.txt Important: This will not list packages installed by pip or install.packages() If you installed extra packages using pip (Python) In this you can use export PYTHONNOUSERSITE=True ## to ingnore packages in ~/.local/lib/python<version># saveconda list --export > conda_requirements.txtpip freeze > pip_requirements.txt# restoreconda create -p ./penv --file conda_requirements.txtpip install -r pip_requirements.txtNote: alternatively, you can use conda env export > all_requirements.txt, which will save both: packages installed by conda and by pip."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_conda-environments-python-r.html,7,"However, this may fail if your conda environment is created as a sub-directory of your project's directory (which we recommend) Installed extra packages using install.packages? (R) Usecase: You need packages not availalbe in conda channels, and want to use install.packages. Command ""conda list --export"" will not include packages installed by ""install.packages"". So, do not use 'conda install' at all."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_conda-environments-python-r.html,8,"To have reproducibility in this case you need to use Conda and renv together, as described below Conda + pakcrat: specific version of R and install.packages (R) use conda to install version of R you need do not use 'conda install' at all use renv install all the packages using install.packages use renv as described here to keep track of the environment In order for conda + renv to work, you need to add following steps: After you activate conda AND before loading R export R_RENV_DEFAULT_LIBPATHS=<path_to_project_directory>/renv/lib/x86_64-conda_cos6-linux-gnu/<version>/ Start R and execute .libPaths(c(.libPaths(), Sys.getenv(""R_RENV_SYSTEM_LIBRARY""))) Use conda env in a batch script The part of the batch script which will call the command shall look like (replace <path_to_env> to an appropriate value) Python Single node #!/bin/bash#SBATCH --job-name=test#SBATCH --nodes=1#SBATCH --cpus-per-task=1#SBATCH --ntasks-per-node=4#SBATCH --mem=8GB#SBATCH --time=1:00:00module purge;module load anaconda3/2020.07;export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK;source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh;conda activate ./penv;export PATH=./penv/bin:$PATH;python python_script.pyMultiple nodes, using MPI mpiexec --mca bash -c ""module purge;export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; module load anaconda3/2020.07; source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh; conda activate ./penv; export PATH=./penv/bin:$PATH; python python_script.py"" R (conda packages only) #!/bin/bash#SBATCH --job-name=test#SBATCH --nodes=1#SBATCH --cpus-per-task=1#SBATCH --ntasks-per-node=4#SBATCH --mem=8GB#SBATCH --time=1:00:00module purge;module load anaconda3/2020.07;export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK;source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh;conda activate ./renv;export PATH=./renv/bin:$PATH;Rscript r_script.RMultiple nodes, using MPI mpiexec --mca bash -c ""module purge;export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK;module load anaconda3/2020.07;source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh;conda activate ./renv;export PATH=./renv/bin:$PATH;Rscript r_script.R""R (conda with renv combination) In this case, when you use sbatch you would activate conda in sbatch script, and R script will pickup packages installed in renv module purgemodule load anaconda3/2020.07 source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh conda activate ./renv Rscript test.R"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_llama-on-hpc.html,0,"Llama on HPC Llama 2 Llama 2 You can find weights of the pre-trained Llama models inside directory /vast/work/public/ml-datasets/llama-2 In order to access it, you need to accept ""Llama 2 Community License Agreement"" on this page: https://ai.meta.com/resources/models-and-libraries/llama-downloads/ In order to get access you need to accept both Llama 2 & Llama Chat Code Llama After you accept the agreement and get the confirmation please make screenshot of that and send it to the HPC team at hpc@nyu.edu."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_open-ondemand-ood-with-condasingularity.html,0,"Open OnDemand (OOD) with Conda/Singularity Open OnDemand is a tool that allows users to launch Graphical User Interfaces (GUIs) based applications are accessible without modifying your HPC environment. You can log into the Open OnDemand interface at https://ood.hpc.nyu.edu. Once logged in, select the Interactive Apps menu, select the desired application, and submit the job based on required resources and options. OOD + Singularity + conda This page describes how to use your Singularity with conda environment in Open OnDemand (OOD) GUI at Greene. Log Into Greene via the Terminal The following commands must be run from the terminal. Information on accessing via the terminal can be found at the Accessing HPC page."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_open-ondemand-ood-with-condasingularity.html,1,"Preinstallation Warning If you have initialized Conda in your base environment (your prompt on Greene may show something like (base) [NETID@log-1 ~]$) then you must first comment out or remove this portion of your ~/.bashrc file: The above code automatically makes your environment look for the default shared installation of Conda on the cluster and will sabotage any attempts to install packages to a Singularity environment. Once removed or commented out, log out and back into the cluster for a fresh environment. Prepare Overlay File mkdir /scratch/$USER/my_envcd /scratch/$USER/my_envcp -rp /scratch/work/public/overlay-fs-ext3/overlay-15GB-500K.ext3.gz . gunzip overlay-15GB-500K.ext3.gz Above we used the overlay file ""overlay-15GB-500K.ext3.gz"" which will contain all of the installed packages. There are more optional overlay files. You can find instructions on the following pages: Singularity with Miniconda, Squash File System and Singularity ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_open-ondemand-ood-with-condasingularity.html,2,"Launch Singularity Environment for Installation singularity exec --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif /bin/bashAbove we used the Singularity OS image ""cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif "" which provides the base operating system environment for the conda environment. There are other Singularity OS images available at /scratch/work/public/singularity Launching Singularity with the --overlay flag mounts the overlay file to a new directory: /ext3 - you will notice that when not using Singularity /ext3 is not available. Be sure that you have the Singularity prompt (Singularity>) and that /ext3 is available before the next step: Singularity> ls -lah /ext3total 8.5Kdrwxrwxr-x. 2 root root 4.0K Oct 19 10:01 .drwx------."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_open-ondemand-ood-with-condasingularity.html,3,"29 root root 8.0K Oct 19 10:01 ..Install Miniforge to Overlay File wget --no-check-certificate https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.shsh Miniforge3-Linux-x86_64.sh -b -p /ext3/miniforge3 Next, create a wrapper script at /ext3/env.sh touch /ext3/env.shecho '#!/bin/bash' >> /ext3/env.shecho 'unset -f which' >> /ext3/env.shecho 'source /ext3/miniforge3/etc/profile.d/conda.sh' >> /ext3/env.shecho 'export PATH=/ext3/miniforge3/bin:$PATH' >> /ext3/env.shecho 'export PYTHONPATH=/ext3/miniforge3/bin:$PATH' >> /ext3/env.shYour /ext3/env.sh file should now contain the following: #!/bin/bashsource /ext3/miniforge3/etc/profile.d/conda.shexport PATH=/ext3/miniforge3/bin:$PATHexport PYTHONPATH=/ext3/miniforge3/bin:$PATH The wrapper script will activate your conda environment, to which you will be installing your packages and dependencies."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_open-ondemand-ood-with-condasingularity.html,4,"Next, activate your conda environment with the following: source /ext3/env.shInstall Packages to Miniforge Environment Now that your environment is activated, you can update and install packages conda config --remove channels defaultsconda update -n base conda -y conda clean --all --yes conda install pip --yesconda install ipykernel --yes # Note: ipykernel is required to run as a kernel in the Open OnDemand Jupyter Notebooks To confirm that your environment is appropriately referencing your Miniforge installation, try out the following: unset whichwhich conda# output: /ext3/miniforge3/bin/conda which python # output: /ext3/miniforge3/bin/python python --version # output: Python 3.8.5 which pip # output: /ext3/miniforge3/bin/pip Now use either conda install or pip to install your required python packages to the Miniforge environment. To install larger packages, like Tensorflow, you must first start an interactive job with adequate compute and memory resources to install packages."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_open-ondemand-ood-with-condasingularity.html,5,"The login nodes restrict memory to 2GB per user, which may cause some large packages to crash. srun --cpus-per-task=2 --mem=10GB --time=04:00:00 --pty /bin/bash# wait to be assigned a node singularity exec --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif /bin/bash source /ext3/env.sh# activate the environment After it is running, you’ll be redirected to a compute node. From there, run singularity to setup on conda environment, same as you were doing on login node. Configure iPython Kernels To create a kernel named my_env copy the template files to your home directory."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_open-ondemand-ood-with-condasingularity.html,6,"mkdir -p ~/.local/share/jupyter/kernelscd ~/.local/share/jupyter/kernelscp -R /share/apps/mypy/src/kernel_template ./my_env # this should be the name of your Singularity envcd ./my_envls#kernel.json logo-32x32.png logo-64x64.png python # files in the ~/.local/share/jupyter/kernels directory To set the conda environment, edit the file named 'python' in /.local/share/jupyter/kernels/my_env/. The python file is a wrapper script that the Jupyter notebook will use to launch your Singularity container and attach it to the notebook. At the bottom of the file we have the template singularity command."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_open-ondemand-ood-with-condasingularity.html,7,"singularity exec $nv \ --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:ro \ /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif \ /bin/bash -c ""source /ext3/env.sh; $cmd $args""WARNING: If you used a different overlay (/scratch/$USER/my_env/overlay-15GB-500K.ext3 shown above) or .sif file (/scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif shown above), you MUST change those lines in the command above to the files you used. Edit the default kernel.json file by setting PYTHON_LOCATION and KERNEL_DISPLAY_NAME using a text editor like nano/vim. { ""argv"": [ ""PYTHON_LOCATION"", ""-m"", ""ipykernel_launcher"", ""-f"", ""{connection_file}"" ], ""display_name"": ""KERNEL_DISPLAY_NAME"", ""language"": ""python""}to { ""argv"": [ ""/home/<Your NetID>/.local/share/jupyter/kernels/my_env/python"", ""-m"", ""ipykernel_launcher"", ""-f"", ""{connection_file}"" ], ""display_name"": ""my_env"", ""language"": ""python""}Update the ""<Your NetID>"" to your own NetID without the ""<>"" symbols."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_open-ondemand-ood-with-condasingularity.html,8,"Launch an Open OnDemand Jupyter Notebook Configure and Launch your Notebook Select kernel Once configured and launched, kernels can be selected in the ""New"" dropdown or within the notebook under the kernel menu. Please note that your notebook view may look slightly different depending on available directories and environments, as well as if you choose the lab or traditional notebook view."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_python-packages-with-virtual-environments.html,0,"Python packages with Virtual Environments In order to be able to install new Python packages and make your work reproducible, please use virtual environments. There is more than one way to create a private environment in Python. Create project directory and load Python module ## Find python version you need module avail python ## created directory for your project and cd there mkdir /scratch/$USER/my_project cd /scratch/$USER/my_project ## load python module (different versions available) module load python/intel/3.8.6 Automatic deletion of your files This page describes the installation of packages on /scratch. One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged (read more)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_python-packages-with-virtual-environments.html,1,"Thus you can consider the following options Reinstall your packages if some of the files get deleted You can do this manually You can do this automatically. For example, within a workflow of a pipeline software like Nextflow Pay for ""Research Project Space"" - read more here Use Singularity and install packages within a corresponding overlay file - read more here Create virtual environment It is advisable to create private environment inside the project directory. This boosts reproducibility and does not use space in /home/$USER virtualenv virtualenv is a tool to create isolated Python environments Since Python 3.3, a subset of it has been integrated into the standard library under the venv module."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_python-packages-with-virtual-environments.html,2,"Note: you may need to install virtualenv first, if it is not yet installed (instructions) Now create new virtual environment in current directory Empty OR inherit all packages from those installed on HPC already (and available in PATH after you load python module) ## Create an EMPTY virtual environmentvirtualenv venv ## Create an virtual environment that inherits system packagesvirtualenv venv --system-site-packages venv venv is package shipped with Python3. It provides subset of options available in virtualenv tool (link). python3 -m venv venvCreate new virtual environment in current directory Empty OR inherit all packages from those installed on HPC already (and available in PATH after you load python module) mkdir /scratch/$USER/my_project cd /scratch/$USER/my_project ##EMPTY ## (use venv command to create environment called ""venv"") python3 -m venv venv ## Inhering all packages python3 -m venv venv --system-site-packages Install packages."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_python-packages-with-virtual-environments.html,3,"Keep things reproducible ## activatesource venv/bin/activate## install packagespip install <package you need>## If package was inherited, but you want to install it in your own env anywaypip install <package you need> --ignore-installed## export list of packages (to report together with paper and/or to reproduce environment on another computer)pip freeze > requirements.txt## restorepip install -r requirements.txtClose an Activated Virtual Environment If you have activated a virtual environment, you can exit it with the following command: deactivateUse with sbatch When you use this env in sbatch script, please use module purge;source venv/bin/activate; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; python python_script.py If you use mpi mpiexec bash -c ""module purge;source venv/bin/activate; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; python python_script.py"""
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv.html,0,"R Packages with renv You may use the renv R package to create a personal R Project environment for R packages. Documentation on renv can be found on the RStudio site. Setup Say your R code is in directory /scratch/$USER/projects/project1 cd /scratch/$USER/projects/project1module purge module load r/gcc/4.1.2 R Automatic deletion of your files This page describes the installation of packages on /scratch. One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged (read more). Thus you can consider the following options Reinstall your packages if some of the files get deleted You can do this manually You can do this automatically."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv.html,1,"For example, within a workflow of a pipeline software like Nextflow Pay for ""Research Project Space"" - read more here Use Singularity and install packages within a corresponding overlay file - read more here Cache directory setup By default, renv will cache package installation files to your home directory (most likely either in ~/.local/share/renv or ~/.cache/R/renv/ or something similar). To avoid filling up your home directory, we advise to set up path to alternative cache directory (otherwise your home directory may fill up quickly) Create directory Put the following into .Renviron file withing the R project directory (/scratch/$USER/projects/project1 in this example) Init renv The renv package is already installed for module r/gcc/4.1.2."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv.html,2,"You need to install it yourself if you use other R module version ## Do this if renv is not available (already installed for r/gcc/4.1.2)# install.packages(""renv"") ## By default this will install renv package into a sub-directory within your home directory## init renv in project's directoryrenv::init(""."") Restart R for renv to take effect. Once you start R, your renv environment will be loaded automatically. * Project '/scratch/$USER/projects/project1' loaded. [renv 0.14.0] Check You can check your library paths with the .libPaths() command You can check where the cache is set with the following: Add/remove, etc. packages Install a package, such as reshape2. Below we can see it is not yet installed and then install it. Rlibrary(reshape2) Error in library(reshape2) : there is no package called ‘reshape2’install.packages(""reshape2"") Note: you must be in the project1 directory for renv to load your project and the appropriate personal environment that you have created."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv.html,3,"If you want to copy your environment to a new location, use the bundle package, as shown below. Test R file print(""hello"")renv::restore()library(reshape2)names(airquality) <- tolower(names(airquality))head(airquality)aql <- melt(airquality)print(""hello again"")For testing run it as srun --pty /bin/bashRscript test.R Note: your '.Rprofile' file will include line source(""renv/activate.R"") The file will output the following: [1] ""hello""* The library is already synchronized with the lockfile."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv.html,4,"ozone solar.r wind temp month day1 41 190 7.4 67 5 12 36 118 8.0 72 5 23 12 149 12.6 74 5 34 18 313 11.5 62 5 45 NA NA 14.3 56 5 56 28 NA 14.9 66 5 6No id variables; using all as measure variables[1] ""hello again""Clean up Keep only the packages that you use in this particular project (not all the packages available on the system) R # launch Rrenv::clean() # remove packages not recorded in the lockfile from the target libraryRecommended Workflow The general workflow when working with renv is: Call renv::init() to initialize a new project-local environment with a private R library, Work in the project as normal, installing and removing new R packages as they are needed in the project, Call renv::snapshot() to save the state of the project library to the lockfile (called renv.lock), By default, renv::snapshot() will only capture packages listed in your R scripts within the R Project. For more options read the renv::snapshot() documentation."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv.html,5,"Continue working on your project, installing and updating R packages as needed. If needed, call renv::restore() to revert to the previous state as encoded in the lockfile if your attempts to update packages introduced some new problems. The renv::init() function attempts to ensure the newly-created project library includes all R packages currently used by the project. It does this by crawling R files within the project for dependencies with the renv::dependencies() function. The discovered packages are then installed into the project library with the renv::hydrate() function, which will also attempt to save time by copying packages from your user library (rather than reinstalling from CRAN) as appropriate. Calling renv::init() will also write out the infrastructure necessary to automatically load and use the private library for new R sessions launched from the project root directory."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv.html,6,"This is accomplished by creating (or amending) a project-local .Rprofile with the necessary code to load the project when the R session is started. If you’d like to initialize a project without attempting dependency discovery and installation – that is, you’d prefer to manually install the packages your project requires on your own – you can use renv::init(bare = TRUE) to initialize a project with an empty project library. Use with sbatch When you launch a job with sbatch, R will check if there is renv directory, and if renv is on it will pick up packages, installed using renv in the current directory. Before you launch sbatch job, you need to make sure your project renv environment is ready, as outlined in the previous section. Store and Share your R Project's R version and R Package Versions Reproduce Environment If you already have file renv.lock or bundle file skip step 1 1."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv.html,7,"In the original location (your own laptop for example) go to project directory and execute (Make sure the whole path to project directory and names of your script files don't have empty spaces!) R# install.packages(""renv"") ## if neededrenv::init()renv::snapshot()2. Take file renv.lock and copy it to a new location for the project 3. At the new location - restore environment: go to directory of the project and execute. (Make sure version of R is the same) ## Reproduce environmentmodule purge module load r/gcc/4.1.2 R renv::restore() renv::init() renv will install/compile what is needed on any system (Linux, Windows, etc). You can share your code with other researchers no matter what system they use. However, you should be careful that the same version of R is used between systems."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv.html,8,"What to save/publish/commit with Git In order to have your work reproducible by you or/and others, save and/or commit your code in git, please including renv.lock (which lists all packages and versions that you use including the version of R) Migrating from Packrat The renv package has replaced the now deprecated Packrat package. The renv::migrate() function makes it possible to migrate projects from Packrat to renv. See the ?migrate documentation for more details. In essence, calling renv::migrate(""<project path>"") will be enough to migrate the Packrat library and lockfile such that they can then be used by renv."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv_rstudio-r-python-reproducible-way.html,0,"RStudio + R + Python Reproducible Environments This page will be useful for those who enjoys working in RStudio, with majority of code written in R, but also utilizing python packages. Information here extend that presented on the page describing the use of renv on cluster: this page. It is recommended you read that page first. Here you will see how to use renv to track both R packages and Python packages. In this workflow renv will automatically create conda environment inside R project directory to handle Python packages. This workflow assumes that you have R and Anaconda installed locally (if you work on cluster you can rely on R and anaconda modules). Setting Up Environments Locally Check that you have installed R and Anaconda."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv_rstudio-r-python-reproducible-way.html,1,"For R make sure you have a version you want selected in RStudio (Tools > Global Options > R version) Create new RStudio project: RStudio -> new project -> new directory install.packages(renv)renv::activate()renv::use_python(type = ""conda"")On the Cluster mkdir my_r_projectcd my_r_projectmodule load r/gcc/4.3.3# module load anaconda3/2024.02 # you can omit this step if you specify reticulate.conda_binary as shown belowR install.packages(renv)renv::activate() Make anaconda available in the project. The path you specify depends on which anaconda version in the system you are using. In order to do that, add the following line into .Rprofile file. Close and open R project. options(reticulate.conda_binary = ""/share/apps/anaconda3/2020.07/condabin/conda"") Now you can specify you are going to use python packages, and that those packages will be handled with anaconda."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv_rstudio-r-python-reproducible-way.html,2,"The following command will create conda environment withing renv directory renv::use_python(type = ""conda"") Installing Packages and Making Snapshots Install R packages as usual Install python packages using. Remember to change cache directory if applicable (as described in renv page). Make snapshot of current package versions renv::snapshot()snapshot command will create two files in the project directory renv.lock - This file will contain info about all the R packages used, R version info, info on conda env environment.yml - this file will contain info about all the conda packages installed. Commit those files to git together with your script files Restoring an Environment Assuming you have renv.lock and environment.yml file in the project directory you would use `renv::restore()` command. This will install R packages and conda packages (including python itself)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv_rstudio-r-python-reproducible-way.html,3,"You need to make sure that R version you use is the same as the one in renv.lock This should work nicely as long as you restore at the same OS, at which renv::snapshot() was created (Windows, Linux, macOS) Restoring on a Different OS In case you or your collaborators are going to reproduce the same environment on a different OS (for example Widnows -> Linux or Linux -> Windows), you may need to change content of this file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv_rstudio-r-python-reproducible-way.html,4,"In particular Recreate environment.yml file using commands: conda activate renv/python/condaenvs/renv-python conda env export --no-build > environment.yml This will overwrite the one created by renv::snapshot(), which does include build information, which is specific to OS, and may create problems for environment setup on another OS change name parameter within the file to null change prefix parameter within file to appropriate value For Linux and MacOS ./renv/python/condaenvs/renv-pythonFor Windows .\renv\python\condaenvs\renv-python Some conda packages are only available on Linux or only on Windows. While executing restoration command renv::restore() you may see that some conda packages are not available. One of the most likely reasons is that those packages are specific to operating system. Examples: vs2015, ucrt, vc are Windows specific; libnsl, libgfortran5 are Linux specific."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_r-packages-with-renv_rstudio-r-python-reproducible-way.html,5,"Thus, you may need to delete those lines from the file for renv::restore() to work properly Using with RStudio on Cluster You can use Open OnDemand, open RStudio and create RStudio project the same way you would do locally: either by creating a project in a new directory, or by cloning from the GitHub repository. Additional Reading These pages may be useful: https://mpn.metworx.com/packages/renv/0.12.5/reference/use_python.html https://rstudio.github.io/reticulate/articles/python_packages.html"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,0,"Singularity with Conda What is Singularity? Singularity is a free, cross-platform and open-source program that creates and executes containers on the HPC clusters. Containers are streamlined, virtualized environments for specific programs or packages. Singularity is an industry standard tool to utilize containers in HPC environments. Containers allow for the support of highly specific environments and further increase scientific reproducibility and portability. Using Singularity containers, researchers can work in the reproducible containerized environments of their choice can easily tailor them to their needs."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,1,"Using Singularity Overlays for Miniforge (Python & Julia) Preinstallation Warning If you have initialized Conda in your base environment (your prompt on Greene may show something like (base) [NETID@log-1 ~]$) then you must first comment out or remove this portion of your ~/.bashrc file: The above code automatically makes your environment look for the default shared installation of Conda on the cluster and will sabotage any attempts to install packages to a Singularity environment. Once removed or commented out, log out and back into the cluster for a fresh environment. Miniforge Environment PyTorch Example Conda environments allow users to create customizable, portable work environments and dependencies to support specific packages or versions of software for research. Common conda distributions include Anaconda, Miniconda and Miniforge. Packages are available via ""channels"". Popular channels include ""conda-forge"" and ""bioconda""."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,2,"In this tutorial we shall use Miniforge which sets ""conda-forge"" as the package channel. Traditional conda environments, however, also create a large number of files that can cut into quotas. To help reduce this issue, we suggest using Singularity, a container technology that is popular on HPC systems. Below is an example of how to create a pytorch environment using Singularity and Miniforge. Create a directory for the environment mkdir /scratch/<NetID>/pytorch-examplecd /scratch/<NetID>/pytorch-exampleCopy an appropriate gzipped overlay images from the overlay directory. You can browse available images to see available options ls /scratch/work/public/overlay-fs-ext3In this example we use overlay-15GB-500K.ext3.gz as it has enough available storage for most conda environments. It has 15GB free space inside and is able to hold 500K files You can use another size as needed. Choose a corresponding Singularity image."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,3,"For this example we will use the following image /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sifFor Singularity image available on nyu HPC greene, please check the singularity images folder /scratch/work/public/singularity/For the most recent supported versions of PyTorch, please check the PyTorch website. Launch the appropriate Singularity container in read/write mode (with the :rw flag) singularity exec --overlay overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bashThe above starts a bash shell inside the referenced Singularity Container overlayed with the 15GB 500K you set up earlier. This creates the functional illusion of having a writable filesystem inside the typically read-only Singularity container."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,4,"Now, inside the container, download and install miniforge to /ext3/miniforge3 wget --no-check-certificate https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.shbash Miniforge3-Linux-x86_64.sh -b -p /ext3/miniforge3# rm Miniforge3-Linux-x86_64.sh # if you don't need this file any longer Next, create a wrapper script /ext3/env.sh using a text editor, like nano. touch /ext3/env.shnano /ext3/env.shThe wrapper script will activate your conda environment, to which you will be installing your packages and dependencies."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,5,"The script should contain the following: #!/bin/bashunset -f which source /ext3/miniforge3/etc/profile.d/conda.sh export PATH=/ext3/miniforge3/bin:$PATH export PYTHONPATH=/ext3/miniforge3/bin:$PATH Activate your conda environment with the following: source /ext3/env.shIf you have the ""defaults"" channel enabled, please disable it with conda config --remove channels defaultsNow that your environment is activated, you can update and install packages: conda update -n base conda -yconda clean --all --yesconda install pip -yconda install ipykernel -y # Note: ipykernel is required to run as a kernel in the Open OnDemand Jupyter Notebooks To confirm that your environment is appropriately referencing your Miniforge installation, try out the following: unset -f whichwhich conda# output: /ext3/miniforge3/bin/conda which python # output: /ext3/miniforge3/bin/python python --version # output: Python 3.8.5 which pip # output: /ext3/miniforge3/bin/pip exit# exit Singularity Install packages You may now install packages into the environment with either the pip install or conda install commands."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,6,"First, start an interactive job with adequate compute and memory resources to install packages. The login nodes restrict memory to 2GB per user, which may cause some large packages to crash. srun --cpus-per-task=2 --mem=10GB --time=04:00:00 --pty /bin/bash# wait to be assigned a node singularity exec --overlay overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash source /ext3/env.sh# activate the environment After it is running, you’ll be redirected to a compute node. From there, run singularity to setup on conda environment, same as you were doing on login node. We will install PyTorch as an example: pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116pip3 install jupyter jupyterhub pandas matplotlib scipy scikit-learn scikit-image Pillow For the latest versions of PyTorch please check the PyTorch website."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,7,"You can see the available space left on your image with the following commands: find /ext3 | wc -l# output: should be something like 45445du -sh /ext3 # output should be something like 4.9G /ext3 Now, exit the Singularity container and then rename the overlay image. Typing 'exit' and hitting enter will exit the Singularity container if you are currently inside it."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,8,"You can tell if you're in a Singularity container because your prompt will be different, such as showing the prompt 'Singularity>' exitmv overlay-15GB-500K.ext3 my_pytorch.ext3 Test your PyTorch Singularity Image singularity exec --overlay /scratch/<NetID>/pytorch-example/my_pytorch.ext3:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash -c 'source /ext3/env.sh; python -c ""import torch; print(torch.__file__); print(torch.__version__)""'#output: /ext3/miniforge3/lib/python3.8/site-packages/torch/__init__.py #output: 1.8.0+cu111 Note: the end ':ro' addition at the end of the pytorch ext3 image starts the image in read-only mode. To add packages you will need to use ':rw' to launch it in read-write mode."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,9,"Using your Singularity Container in a SLURM Batch Job Below is an example script of how to call a python script, in this case torch-test.py, from a SLURM batch job using your new Singularity image torch-test.py: #!/bin/env pythonimport torch print(torch.__file__) print(torch.__version__) # How many GPUs are there? print(torch.cuda.device_count()) # Get the name of the current GPU print(torch.cuda.get_device_name(torch.cuda.current_device())) # Is PyTorch using a GPU? print(torch.cuda.is_available()) Now we will write the SLURM job script, run-test.SBATCH, that will start our Singularity Image and call the torch-test.py script."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,10,"run-test.SBATCH: #!/bin/bash#SBATCH --nodes=1#SBATCH --ntasks-per-node=1#SBATCH --cpus-per-task=1#SBATCH --time=1:00:00#SBATCH --mem=2GB#SBATCH --gres=gpu#SBATCH --job-name=torch module purge singularity exec --nv \ --overlay /scratch/<NetID>/pytorch-example/my_pytorch.ext3:ro \ /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif\ /bin/bash -c ""source /ext3/env.sh; python torch-test.py"" You will notice that the singularity exec command features the '--nv flag' - this flag is reguired to pass the CUDA drivers from a GPU to the Singularity container."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,11,"Run the run-test.SBATCH script sbatch run-test.SBATCHCheck your SLURM output for results, an example is shown below cat slurm-3752662.out# example output: # /ext3/miniforge3/lib/python3.8/site-packages/torch/__init__.py # 1.8.0+cu111 # 1 # Quadro RTX 8000 # True Optional: Convert ext3 to a compressed, read-only squashfs filesystem Singularity images can be compressed into read-only squashfs filesystems to conserve space in your environment. Use the following steps to convert your ext3 Singularity image into a smaller squashfs filesystem."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,12,"srun -N1 -c4 singularity exec --overlay my_pytorch.ext3:ro /scratch/work/public/singularity/centos-8.2.2004.sif mksquashfs /ext3 /scratch/<NetID>/pytorch-example/my_pytorch.sqf -keep-as-directory -processors 4Here is an example of the amount of compression that can be realized by converting: ls -ltrsh my_pytorch.*5.5G -rw-r--r-- 1 wang wang 5.5G Mar 14 20:45 my_pytorch.ext32.2G -rw-r--r-- 1 wang wang 2.2G Mar 14 20:54 my_pytorch.sqfNotice that it saves over 3GB of storage in this case, though your results may vary. Use a squashFS Image for Running Jobs You can use squashFS images similarly to the ext3 images."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,13,"singularity exec --overlay /scratch/<NetID>/pytorch-example/my_pytorch.sqf:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash -c 'source /ext3/env.sh; python -c ""import torch; print(torch.__file__); print(torch.__version__)""'#example output: /ext3/miniforge3/lib/python3.8/site-packages/torch/__init__.py #example output: 1.8.0+cu111 Adding Packages to a Full ext3 or squashFS Image If the first ext3 overlay image runs out of space or you are using a squashFS conda enviorment, but need to install a new package inside, please copy another writable ext3 overlay image to work together."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,14,"Open the first image in read only mode cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz .gunzip overlay-2GB-100K.ext3.gz singularity exec --overlay overlay-2GB-100K.ext3 --overlay /scratch/<NetID>/pytorch-example/my_pytorch.ext3:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu-22.04.2.sif /bin/bash source /ext3/env.sh pip install tensorboard Note: Click here for information on how to configure your conda environment. Please also keep in mind that once the overlay image is opened in default read-write mode, the file will be locked. You will not be able to open it from a new process. Once the overlay is opened either in read-write or read-only mode, it cannot be opened in RW mode from other processes either. For production jobs to run, the overlay image should be open in read-only mode. You can run many jobs at the same time as long as they are run in read-only mode."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,15,"In this ways, it will protect the computation software environment, software packages are not allowed to change when there are jobs running. Julia Singularity Image Singularity can be used to set up a Julia environment. Create a directory for your julia work, such as /scratch/<NetID>/julia, and then change to your home directory. An example is shown below. mkdir /home/<NetID>/juliacd /home/<NetID>/julia Copy an overlay image, such as the 2GB 100K overlay, which generally has enough storage for Julia packages."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,16,"Once copied, unzip to the same folder, rename to julia-pkgs.ext3 cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz .gunzip overlay-2GB-100K.ext3.gz mv overlay-2GB-100K.ext3 julia-pkgs.ext3 Copy the following wrapper script in the directory cp -rp /share/apps/utils/julia-setup/* .Now launch writable Singularity overlay to install packages module purgemodule load knitro/12.3.0 module load julia/1.5.3 ~/julia/my-julia-writable using Pkg Pkg.add(""KNITRO"") Pkg.add(""JuMP"") Now exit from the container to launch a read only version to test (example below) ~/julia/my-julia_ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type ""?"" for help, ""]?"" for Pkg help."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,17,"| | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.5.3 (2020-11-09) _/ |\__'_|_|_|\__'_| | Official https://julialang.org/ release |__/ | julia> using Pkg julia> using JuMP, KNITRO julia> m = Model(with_optimizer(KNITRO.Optimizer)) A JuMP Model Feasibility problem with: Variables: 0 Model mode: AUTOMATIC CachingOptimizer state: EMPTY_OPTIMIZER Solver name: Knitro julia> @variable(m, x1 >= 0) x1 julia> @variable(m, x2 >= 0) x2 julia> @NLconstraint(m, x1*x2 == 0) x1 * x2 - 0.0 = 0 julia> @NLobjective(m, Min, x1*(1-x2^2)) julia> optimize!(m) You can make the above code into a julia script to test batch jobs."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,18,"Save the following as test-knitro.jl using Pkgusing JuMP, KNITROm = Model(with_optimizer(KNITRO.Optimizer))@variable(m, x1 >= 0)@variable(m, x2 >= 0)@NLconstraint(m, x1*x2 == 0)@NLobjective(m, Min, x1*(1-x2^2))optimize!(m)You can add additional packages with commands like the one below (NOTE: Please do not install new packages when you have Julia jobs running, this may create issues with your Julia installation) ~/julia/my-julia-writable -e 'using Pkg; Pkg.add([""Calculus"", ""LinearAlgebra""])'Run a SLURM job to test with the following sbatch command (e.g."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,19,"julia-test.SBATCH) #!/bin/bash#SBATCH --nodes=1#SBATCH --ntasks-per-node=1#SBATCH --cpus-per-task=1#SBATCH --time=1:00:00#SBATCH --mem=2GB#SBATCH --job-name=julia-test module purgemodule load julia/1.5.3module load knitro/12.3.0 ~/julia/my-julia test-knitro.jl Then run the command with the following: sbatch julia-test.SBATCHOnce the job completes, check the SLURM output (example below) cat slurm-1022969.out======================================= Academic License (NOT FOR COMMERCIAL USE) Artelys Knitro 12.3.0======================================= Knitro presolve eliminated 0 variables and 0 constraints. datacheck: 0hessian_no_f: 1par_numthreads: 1 Problem Characteristics ( Presolved)-----------------------Objective goal: MinimizeObjective type: generalNumber of variables: 2 ( 2) bounded below only: 2 ( 2) bounded above only: 0 ( 0) bounded below and above: 0 ( 0) fixed: 0 ( 0) free: 0 ( 0)Number of constraints: 1 ( 1) linear equalities: 0 ( 0) quadratic equalities: 0 ( 0) gen."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,20,nonlinear equalities: 1 ( 1) linear one-sided inequalities: 0 ( 0) quadratic one-sided inequalities: 0 ( 0) gen. nonlinear one-sided inequalities: 0 ( 0) linear two-sided inequalities: 0 ( 0) quadratic two-sided inequalities: 0 ( 0) gen. nonlinear two-sided inequalities: 0 ( 0)Number of nonzeros in Jacobian: 2 ( 2)Number of nonzeros in Hessian: 3 ( 3) Knitro using the Interior-Point/Barrier Direct algorithm. Iter Objective FeasError OptError ||Step|| CGits -------- -------------- ---------- ---------- ---------- ------- 0 0.000000e+00 0.000e+00 WARNING: The initial point is a stationary point and only the first order optimality conditions have been verified. EXIT: Locally optimal solution found.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,21,"Final Statistics----------------Final objective value = 0.00000000000000e+00Final feasibility error (abs / rel) = 0.00e+00 / 0.00e+00Final optimality error (abs / rel) = 0.00e+00 / 0.00e+00# of iterations = 0 # of CG iterations = 0 # of function evaluations = 1# of gradient evaluations = 1# of Hessian evaluations = 0Total program time (secs) = 1.03278 ( 1.014 CPU time)Time spent in evaluations (secs) = 0.00000 =============================================================================== Using CentOS 8 for Julia (for Module Compatibility) Building on the previous Julia example, this will demonstrate how to set up a similar environment using the Singularity CentOS 8 image for additional customization."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,22,"Using the CentOS 8 overlay allows for the loading of modules installed on Greene, such as Knitro 12.3.0 Copy overlay image cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz .gunzip overlay-2GB-100K.ext3.gzmv overlay-2GB-100K.ext3 julia-pkgs.ext3The path in this example is /scratch/<NetID>/julia/julia-pkgs.ext3 To use modules installed into /share/apps you can make two directories mkdir julia-compiled julia-logsNow, in this example, the absoulte paths are as follows /scratch/<NetID>/julia/julia-compiled/scratch/<NetID>/julia/julia-logsTo launch Singularity with overlay images in writable mode to install packages singularity exec \ --overlay /scratch/<NetID>/julia/julia-pkgs.ext3 \ --bind /share/apps \ --bind /scratch/<NetID>/julia/julia-compiled:/ext3/pkgs/compiled \ --bind /scratch/<NetID>/julia/julia-logs:/ext3/pkgs/logs \ /scratch/work/public/apps/greene/centos-8.2.2004.sif \ /bin/bashImplement a wrapper script /ext3/env.sh #/bin/bashexport JULIA_DEPOT_PATH=/ext3/pkgs # this changes the default installation path to the environmentsource /opt/apps/lmod/lmod/init/bashmodule use /share/apps/modulefilesmodule purgemodule load knitro/12.3.0module load julia/1.5.3 Load julia via the wrapper script and check that it loads properly source /ext3/env.shwhich julia# example output: /share/apps/julia/1.5.3/bin/juliajulia --version# example output: julia version 1.5.3Run julia to install packages julia> using Pkg> Pkg.add(""KNITRO"")> Pkg.add(""JuMP"")Set up a similar test script like the test-knitro.jl script above."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,23,"Name it test.jl: using Pkgusing JuMP, KNITROm = Model(with_optimizer(KNITRO.Optimizer))@variable(m, x1 >= 0)@variable(m, x2 >= 0)@NLconstraint(m, x1*x2 == 0)@NLobjective(m, Min, x1*(1-x2^2))optimize!(m)Now implement a wrapper script named julia into ~/bin, the overlay image is in readonly mode #!/bin/bashargs=''for i in ""$@""; do i=""${i//\\/\\\\}"" args=""$args \""${i//\""/\\\""}\""""done module purge singularity exec \ --overlay /scratch/<NetID>/julia/julia-pkgs.ext3:ro \ --bind /share/apps \ --bind /scratch/<NetID>/julia/julia-compiled:/ext3/pkgs/compiled \ --bind /scratch/<NetID>/julia/julia-logs:/ext3/pkgs/logs \ /scratch/work/public/apps/greene/centos-8.2.2004.sif \ /bin/bash -c ""source /ext3/env.shjulia $args"" Make the wrapper executable chmod 755 ~/bin/juliaTest your installation with a SLURM job example."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,24,"The following code has been put into a file called test-julia-centos.SBATCH #!/bin/bash#SBATCH --nodes=1#SBATCH --ntasks-per-node=1#SBATCH --cpus-per-task=1#SBATCH --time=1:00:00#SBATCH --mem=2GB#SBATCH --job-name=julia-testmodule purge julia test.jl Run the above with the following: sbatch test-julia-centos.SBATCHRead the output (example below) cat slurm-764085.outInstalling New Julia Packages Later Implement another writable julia-writable with overlay image writable in order to install new Julia packages later cd /home/<NetID>/bincp -rp julia julia-writable#!/bin/bash args=''for i in ""$@""; do i=""${i//\\/\\\\}"" args=""$args \""${i//\""/\\\""}\""""done module purge singularity exec \ --overlay /scratch/<NetID>/julia/julia-pkgs.ext3 \ --bind /share/apps \ --bind /scratch/<NetID>/julia/julia-compiled:/ext3/pkgs/compiled \ --bind /scratch/<NetID>/julia/julia-logs:/ext3/pkgs/logs \ /scratch/work/public/apps/greene/centos-8.2.2004.sif \ /bin/bash -c ""source /ext3/env.shjulia $args"" Check the writable image which julia-writable#example output: ~/bin/julia-writableInstall packages to the writable image julia-writable -e 'using Pkg; Pkg.add([""Calculus"", ""LinearAlgebra""])'If you do not need host packages installed in /share/apps, you can work with Singularity OS image /scratch/work/public/singularity/ubuntu-20.04.1.sifdownload Julia installation package from https://julialang-s3.julialang.org/bin/linux/x64/1.5/julia-1.5.3-linux-x86_64.tar.gz install Julia to /ext3, setup PATH properly."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_software_singularity-with-miniconda.html,25,It will be easy to move to other servers in future.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_storage-specs.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_storage-specs.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing Storage Hardware description Detailed hardware description can be found on the HPC Storage page Mounted Storage Systems Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_system-status.html,0,Greene System Status Note: To be able to see the panels below you need to be within NYU network (use VPN if you are not on campus) Resources Allocation and Queue (AMD nodes not included) Resources Allocation and Queue (AMD nodes not included) Resources Allocation and Queue by partitions (AMD nodes not included) Resources Allocation and Queue by partitions (AMD nodes not included) Resources Allocation and Queue on AMD nodes Resources Allocation and Queue on AMD nodes Plese look here Check efficiency of your jobs Check efficiency of your jobs Please look here
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_greene_system-status_amd-system-status.html,0,AMD nodes System Status Note: To be able to see the panels below you need to be within NYU network (use VPN if you are not on campus) Note: To be able to see the panels below you need to be within NYU network (use VPN if you are not on campus)
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage.html,0,"HPC Storage The NYU HPC clusters are served by a General Parallel File System (GPFS) cluster. The NYU HPC team supports data storage, transfer, and archival needs on the HPC clusters, as well as collaborative research services like the Research Project Space (RPS). Team is also piloting all flash VAST storage. Highlights Highlights 9.5 PB Total GPFS Storage Up to 78 GB per second read speeds Up to 650k input/output operations per second (IOPS) Research Project Space (RPS) RPS volumes provide working spaces for sharing data and code amongst project or lab members"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_best-practices.html,0,"Best Practices on HPC Storage User Quota Limits and the myquota command All users have quote limits set on HPC fie systems. There are several types of quota limits, such as limits on the amount of disk space (disk quota), number of files (inode quota) etc. The default user quota limits on HPC file systems are listed here. Running out of quota causes a variety of issues such as running user jobs being interrupted or users being unable to finish the installation of packages under their home directory. One of the common issues users report is running out of inodes in their home directory. This usually occurs during software installation, for example installing conda environment under their home directory. Users can check their current utilization of quota using the myquota command. The myquota command provides a report of the current quota limits on mounted file systems, the user's quota utilization, as well as the percentage of quota utilization."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_best-practices.html,1,"In the following example the user who executes the myquota command is out of inodes in their home directory. The user inode quota limit on the /home file system 30.0K inodes and the user has 33000 inodes, thus 110% of the inode quota limit. $ myquotaHostname: log-1 at Sun Mar 21 21:59:08 EDT 2021 Filesystem Environment Backed up? Allocation Current Usage Space Variable /Flushed?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_best-practices.html,2,"Space / Files Space(%) / Files(%) /home $HOME Yes/No 50.0GB/30.0K 8.96GB(17.91%)/33000(110.00%) /scratch $SCRATCH No/Yes 5.0TB/1.0M 811.09GB(15.84%)/2437(0.24%) /archive $ARCHIVE Yes/No 2.0TB/20.0K 0.00GB(0.00%)/1(0.00%)/vast $VAST No/Yes 2.0TB/5.0M 0.00GB(0.00%)/1(0.00%) Users can find out the number of inodes (files) used per subdirectory under their home directory ($HOME), by running the following commands: $cd $HOME$ for d in $(find $(pwd) -maxdepth 1 -mindepth 1 -type d | sort -u); do n_files=$(find $d | wc -l); echo $d $n_files; done/home/netid/.cache 1507/home/netid/.conda 2/home/netid/.config 2/home/netid/.ipython 11/home/netid/.jupyter 2/home/netid/.keras 2/home/netid/.local 24185/home/netid/.nv 2/home/netid/.sacrebleu 46/home/netid/.singularity 1/home/netid/.ssh 5/home/netid/.vscode-server 7216Large number of small files In case your dataset or workflow requires to use large number of small files, this can create a bottleneck due to read/write rates."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_best-practices.html,3,"Please refer to our page on working with a large number of files to learn about some of the options we recommend to consider. Installing Python packages Your home directory has relatively small number of inodes. In case you would create conda or python environment in you home directory, this can eat up all the inodes. Please review best practices for managing packages under the Package Management section of the Greene Software Page."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,0,"Data Management Introduction to HPC Data Management The NYU HPC Environment provides access to a number of file systems to better serve the needs of researchers managing data during the various stages of the research data lifecycle (data capture, analysis, archiving,, etc.). Each HPC file system comes with different features, policies, and availability. In addition, a number of data management tools are available that enable data transfers and data sharing, recommended best practices, and various scenarios and use cases of managing data in the HPC Environment. Multiple public data sets are available to all users of the HPC environment, such as a subset of The Cancer Genome Atlas (TCGA), the Million Song Database, ImageNet, and Reference Genomes. Below is a list of file systems with their characteristics and a summary table."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,1,"Reviewing the list of available file systems and the various Scenarios/Use cases that are presented below, can help select the right file systems for a research project. As always, if you have any questions about data storage in the HPC environment, you can request a consultation with the HPC team by sending email to hpc@nyu.edu. Data Security Warning Moderate Risk Data - HPC Approved The HPC Environment has been approved for storing and analyzing Moderate Risk research data, as defined in the NYU Electronic Data and System Risk Classification Policy. High Risk research data, such as those that include Personal Identifiable Information (PII) or electronic Protected Health Information (ePHI) or Controlled Unclassified Information (CUI) should NOT be stored in the HPC Environment. Please note that only the Office of Sponsored Projects (OSP) and Global Office of Information Security (GOIS) are empowered to classify the risk categories of data."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,2,"High Risk Data - Secure Research Data Environments (SRDE) Approved Because the HPC system is not approved for High Risk data, we recommend using an approved system like the Secure Research Data Environments (SRDE). Data Storage options in the HPC Environment User Home Directories Every individual user has a home directory (under /home/$USER, environment variable $HOME) for permanently storing code and important configuration files. Home Directories provide limited storage space (50 GB) and inodes (files) 30,000 per user. Users can check their quota utilization using the myquota command. User home directories are backed up daily and old files under $HOME are not purged. The User home directories are available on all HPC clusters (Greene) and on every cluster node (login nodes, compute nodes) as well as and Data Transfer Node (gDTN). Please Note: Avoid changing file and directory permissions in your home directory to allow other users to access files."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,3,"User Home Directories are not ideal for sharing files and folders with other users. HPC Scratch of Research Project Space (RPS) are better file systems for sharing data. One of the common issues that users report regarding their home directories is running out of inodes, i.e. the number of files stored under their home exceeds the inode limit, which by default is set to 30,000 files. This typically occurs when users install software under their home directories, for example, when working with Conda and Julia environments, that involve many small files. To find out the current space and inode quota utilization and the distribution of files under your home directory, please see: Understanding user quota limits and the myquota command."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,4,"Working with Conda environments: To avoid running out of inode limits in home directories, the HPC team recommends setting up conda environments with Singularity overlay images HPC Scratch The HPC scratch file system is the HPC file system where most of the users store research data needed during the analysis phase of their research projects. The scratch file system provides temporary storage for datasets needed for running jobs. Files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged. Every user has a dedicated scratch directory (/scratch/$USER) with 5 TB disk quota and 1,000,000 inodes (files) limit per user. The scratch file system is available on all nodes (compute, login, etc.) on Greene as well as Data Transfer Node (gDTN). Please Note: There are No Back ups of the scratch file system."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,5,"Files that were deleted accidentally or removed due to storage system failures CAN NOT be recovered. Since there are no back ups of HPC Scratch file system, users should not put important source code, scripts, libraries, executables in /scratch. These important files should be stored in file systems that are backed up, such as /home or Research Project Space (RPS). Code can also be stored in a git repository. Old file purging policy on HPC Scratch: All files on the HPC Scratch file system that have not been accessed for more than 60 days will be removed. It is a policy violation to use scripts to change the file access time. Any user found to be violating this policy will have their HPC account locked. A second violation may result in your HPC account being turned off. To find out the user's current disk space and inode quota utilization and the distribution of files under your scratch directory, please see: Understanding user quota Limits and the myquota command."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,6,"Once a research project completes, users should archive their important files in the HPC Archive file system. HPC Vast The HPC Vast all-flash file system is the HPC file system where users store research data needed during the analysis phase of their research projects, particuarly for high I/O data that can bottleneck on the scratch file system. The vast file system provides temporary storage for datasets needed for running jobs. Files stored in the HPC vast file system are subject to the HPC Vast old file purging policy: Files on the /vast file system that have not been accessed for 60 or more days will be purged. Every user has a dedicated vast directory (/vast/$USER) with 2 TB disk quota and 5,000,000 inodes (files) limit per user. The vast file system is available on all nodes (compute, login, etc.) on Greene as well as Data Transfer Node (gDTN). Please Note: There are No Back ups of the vastsc file system."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,7,"Files that were deleted accidentally or removed due to storage system failures CAN NOT be recovered. Since there are no back ups of HPC Vast file system, users should not put important source code, scripts, libraries, executables in /vast. These important files should be stored in file systems that are backed up, such as /home or Research Project Space (RPS). Code can also be stored in a git repository. Old file purging policy on HPC Vast: All files on the HPC Vast file system that have not been accessed for more than 60 days will be removed. It is a policy violation to use scripts to change the file access time. Any user found to be violating this policy will have their HPC account locked. A second violation may result in your HPC account being turned off. To find out the user's current disk space and inode quota utilization and the distribution of files under your vast directory, please see: Understanding user quota Limits and the myquota command."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,8,"Once a research project completes, users should archive their important files in the HPC Archive file system. HPC Research Project Space The HPC Research Project Space (RPS) provides data storage space for research projects that is easily shared amongst collaborators, backed up, and not subject to the old file purging policy. HPC RPS was introduced to ease data management in the HPC environment and eliminate the need of having to frequently copying files between Scratch and Archive file systems by having all projects files under one area. These benefits of the HPC RPS come at a cost. The cost is determined by the allocated disk space and the number of files (inodes). For detailed information about RPS see: HPC Research Project Space HPC Work The HPC team makes available a number of public sets that are commonly used in analysis jobs. The data sets are available Read-Only under /scratch/work/public. For some of the datasets users must provide a signed usage agreement before accessing."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,9,"Public datasets available on the HPC clusters can be viewed on the Datasets page. HPC Archive Once the Analysis stage of the research data lifecycle has completed, HPC users should tar their data and code into a single tar.gz file and then copy the file to their archive directory (/archive/$USER). The HPC Archive file system is not accessible by running jobs; it is suitable for long-term data storage. Each user has access to a default disk quota of 2TB and 20,000 inode (files) limit. The rather low limit on the number of inodes per user is intentional. The archive file system is available only on login nodes of Greene. The archive file system is backed up daily."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,10,"Here is an example tar command that combines the data in a directory named my_run_dir under $SCRATCH and outputs the tar file in the user's $ARCHIVE: tar cvf $ARCHIVE/simulation_01.tar -C $SCRATCH my_run_dir NYU (Google) Drive Google Drive (NYU Drive) is accessible from the NYU HPC environment and provides an option to users who wish to archive data or share data with external collaborators who do not have access to the NYU HPC environment. Currently (March 2021) there is no limit on the amount of data a user can store on Google Drive and there is no cost associated with storing data on Google Drive (although we hear rumors that free storage on Google Drive may be ending soon). However, there are limits to the data transfer rate in moving to/from Google Drive. Thus, moving many small files to Google Drive is not going to be efficient. Please read the Instructions on how to use cloud storage within the NYU HPC Environment."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management.html,11,HPC Storage Mounts Comparison Table Data Management Subpages Data Transfers Sharing Data on HPC Link to Sharing Data on HPC Page Research Project Space Link to Research Project Space Page
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers.html,0,"Data Transfers Introduction The main tools to transfer data to/from HPC systems Linux tools like scp and rsync Please use Data transfer nodes Note: while one can transfer data while on login nodes, it is considered a bad practice Globus rclone to/from cloud storage like NYU (Google) Drive OpenOnDemand Other tools Data-Transfer nodes Attached to the NYU HPC cluster Greene, the Greene Data Transfer Node (gDTN) are nodes optimized for transferring data between cluster file systems (e.g. scratch) and other endpoints outside the NYU HPC clusters, including user laptops and desktops. The gDTNs have 100-Gb/s Ethernet connections to the High Speed Research Network (HSRN) and are connected to the HDR Infiniband fabric of the HPC clusters. The HPC cluster filesystems include /home, /scratch, /archive and the HPC Research Project Space are available on the gDTN."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers.html,1,"The Data-Transfer Node (DTN) can be access in a variety of ways From NYU-net and the High Speed Research Network: use SSH to the DTN hostname gdtn.hpc.nyu.edu From the Greene cluster (e.g., the login nodes): the hostname can be shortened to gdtn For example, to log in to a DTN from the Greene cluster, to carry out some copy operation, and to log back out, you can use a command sequence like: rsync ... logout Via specific tools like Globus (see below) Linux & Mac Tools scp and rsync Please transfer data using Data-Transfer nodes Sometimes these two tools are convenient for transferring small files. Using the DTNs does not require to set up an SSH tunnel; use the hostname dtn.hpc.nyu.edu for one-step copying."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers.html,2,"See below for examples of commands invoked on the command line on a laptop running a Unix-like operating system: scp HMLHWBGX7_n01_HK16.fastq.gz jdoe55@dtn.hpc.nyu.edu:/scratch/jdoe55/rsync -av HMLHWBGX7_n01_HK16.fastq.gz jdoe55@dtn.hpc.nyu.edu:/scratch/jdoe55/In particular, rsync can also be used on the DTNs to copy directories recursively between filesystems, e.g. (assuming that you are logged in to a DTN), rsync -av /scratch/username/project1 /rw/sharename/where username would be your user name, project1 a directory to be copied to the Research Workspace, and sharename the name of a share on the Research Workspace (either your NetID or the name of a project you're a member of). Windows Tools File Transfer Clients Windows 10 machines may have the Linux Subsystem installed, which will allow for the use of Linux tools, as listed above, but generally it is recommended to use a client such as WinSCP or FileZilla to transfer data."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers.html,3,"Additionally, Windows users may also take advantage of Globus to transfer files. Tunneling One can set up tunnels to copy data. Please read this page Globus Globus is the recommended tool to use for large-volume data transfers. It features automatic performance tuning and automatic retries in cases of file-transfer failures. Data-transfer tasks can be submitted via a web portal. The Globus service will take care of the rest, to make sure files are copied efficiently, reliably, and securely. Globus is also a tool for you to share data with collaborators, for whom you only need to provide the email addresses. The Globus endpoint for Greene is available at ""nyu#greene"". The endpoint ""nyu#prince"" has been retired. Please find detailed instructions here rclone rclone - rsync for cloud storage, is a command line program to sync files and directories to and from cloud storage systems such as Google Drive, Amazon Drive, S3, B2 etc. rclone is available on DTNs."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers.html,4,"Please see the documentation for how to use it. Open OnDemand One can use Open OnDemand interface to upload data. However, please use it only for small data! Other tools Please use Data-Transfer nodes while moving large data FDT FDT stands for ""Fast Data Transfer"". It is a command line application written in Java. With the plugin mechanism, FDT allows users to load user-defined classes for Pre- and Post-Processing of file transfers. Users can start their own server processes. If you have use cases for FDT, visit the download page to get fdt.jar to start. Please contact hpc@nyu.edu for any questions."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_globus.html,0,"Globus The Globus project aims at providing powerful tools for scientific data management, to help researchers to focus on their domain subjects and solve data intensive research problems. Globus has been grown maturely to enable grid computing by connecting computing resources distributed globally across organizational boundary. Universities, national laboratories and computing facilities are using services of Globus. Transferring data between endpoints Endpoint A globus Endpoint is a data transfer location, a location where data can be moved to or from using Globus transfer, sync and sharing service. An endpoint can either be a personal endpoint (on a user’s personal computer) or a server endpoint (located on a server, for use by multiple users). For more info see: https://www.globus.org/how-it-works Collection A collection is a named set of files (or blobs), hierarchically organized in folders."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_globus.html,1,"Data Sharing How to share data using Globus is described: https://docs.globus.org/how-to/share-files/ The first step in transferring data is to get a Globus account at https://www.globus.org/. Click on ""Log in"" at upper right corner. Select ""New York University"" from the pull-down menu and click on ""Continue"". Enter your NYU NetID and password in the familiar screen, and hit ""LOGIN"" then go through the Multi-Factor Authentication. The ""File Manager"" panel should come up as the following image. In order to be able to transfer files, you will need to specify two Collections. A collection is defined on top of an endpoint. We can search for a collection using an endpoint name. The Server Endpoint on the NYU HPC storage is nyu#greene ."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_globus.html,2,"Server and Personal Endpoints The NYU HPC Server Endpoint: nyu#greene Globus Connect Server is already installed on the NYU HPC cluster creating a Server Endpoint named nyu#greene, that is available to authorized users (users with a valid HPC account) using Globus. If you want to move data to or from your computer and the NYU HPC cluster, you need to install Globus Connect Personal on your computer, thus creating an Personal Endpoint on your computer. Moving data between Server Endpoints If you plan to transfer data between Server Endpoints, such as between the NYU server endpoint nyu#greene and a server endpoint at another institution, you do not need to install Globus Connect Personal on your computer. Creating a Personal Endpoint on your computer This needs to be done only once on your personal computer. After clicking ""Transfer or Sync to..."", click ""Search"" on the upper right side. Then follow the link ""Install Globus Connect Personal""."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_globus.html,3,"More information about Globus Connect Personal and download links for Linux, Mac and Windows can be found at: https://www.globus.org/globus-connect-personal Transfer files between your Personal Endpoint and NYU nyu#greene To transfer files you need to specify two collections (endpoints). Specify one of them as ""Greene scratch directory"", or ""Greene archive directory"" or ""Greene home directory"". The other endpoint is the one created for your personal computer (e.g. My Mac Laptop) if it is involved in the transfer. When you first use the Greene directory collection, authentication/consent is required for the Globus web app to manage collections on this endpoint on your behalf. When writing to your Greene archive directory, please pay attention that there is a default inode limit of 20K per user. When the second Endpoint is chosen to be your personal computer, your computer home directory content will show up."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_globus.html,4,"Now select directory and files (you may select multiple files when clicking on file names while pressing down ""shift"" key), click one of the two blue Start buttons to indicate the transfer direction. After clicking the blue Start button, you should see a message indicating a transfer request has been submitted successfully, and a transfer ID is generated. Globus file transfer service takes care of the actual copying. When the transfer is done, you should receive an email notification. Click ""ACTIVITY"" on the Globus portal, select the transfer you want to check, a finished transfer should look like the following: Small file download from web browsers Globus support HTTPS access to data. To download a small file from your web browser, select a file and right-click your mouse, then click 'Download' at the popup menu. Additional info can be found at this page https://docs.globus.org/how-to/get-started/. Feel free to send any question. Good luck!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_transferring-cloud-storage-data-with-rclone.html,0,"Transferring Cloud Storage Data with rclone Transferring files to and from Google Drive with RCLONE Having access to Google Drive from the HPC environment provides an option to archive data and even share data with collaborators who have no access to the NYU HPC environment. Other options to archiving data include the HPC Archive file system and using Globus to share data with collaborators. Access to Google Drive is provided by rclone - rsync for cloud storage - a command line program to sync files and directories to and from cloud storage systems such as Google Drive, Amazon Drive, S3, B2 etc."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_transferring-cloud-storage-data-with-rclone.html,1,"rclone is available on Greene cluster as a module, the module currently (November 2022) is rclone/1.60.1 For more details on how to use rclone to sync files to Google Drive, please see: https://rclone.org/drive/ rclone can be invoked in one of the three modes: Please try with these options: rclone --transfers=32 --checkers=16 --drive-chunk-size=16384k --drive-upload-cutoff=16384k copy source:sourcepath dest:destpathThis option works great for file sizes 1Gb+ to 250GB. Keep in mind that there is a rate limiting of 2 files/sec for upload into Google Drive. Small file transfers don’t work that well. If you have many small jobs, please tar the parent directory of such folders and splits the tar file into 100GB chunks and uploads then into Google Drive."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_transferring-cloud-storage-data-with-rclone.html,2,"rclone Configuration You need to configure rclone before you will be able to move files between the HPC Environment and Google Drive There are specific instruction on the rclone web site: https://rclone.org/drive/ Step 1: Login to Greene: Follow instructions to log into the Greene HPC cluster. Step 2: Load the rclone module Step 3: Configure rclone Configuring rclone and setting up remote access to your Google Drive, using the command: $ rclone configThis will try to open the config files and you will see the below content: You can select one of the options (here we show how to set up a new remote) 2021/03/23 18:10:29 NOTICE: Config file ""/home/netid/.config/rclone/rclone.conf"" not found - using defaultsNo remotes found - make a new onen) New remotes) Set configuration passwordq) Quit confign/s/q> nname> remote1Type of storage to configure.Enter a string value."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_transferring-cloud-storage-data-with-rclone.html,3,"Press Enter for the default ("""").Choose a number from below, or type in your own value 1 / 1Fichier \ ""fichier"" 2 / Alias for an existing remote \ ""alias"" 3 / Amazon Drive \ ""amazon cloud drive"" 4 / Amazon S3 Compliant Storage Provider (AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Minio, Tencent COS, etc) \ ""s3"" 5 / Backblaze B2 \ ""b2"" 6 / Box \ ""box"" 7 / Cache a remote \ ""cache"" 8 / Citrix Sharefile \ ""sharefile"" 9 / Dropbox \ ""dropbox""10 / Encrypt/Decrypt a remote \ ""crypt""11 / FTP Connection \ ""ftp"" 12 / Google Cloud Storage (this is not Google Drive) \ ""google cloud storage""13 / Google Drive \ ""drive"" 14 / Google Photos \ ""google photos""............37 / premiumize.me \ ""premiumizeme""38 / seafile \ ""seafile""Storage> 13** See help for drive backend at: https://rclone.org/drive/ **Google Application Client IdSetting your own is recommended.See https://rclone.org/drive/#making-your-own-client-id for how to create your own.If you leave this blank, it will use an internal key which is low performance.Enter a string value."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_transferring-cloud-storage-data-with-rclone.html,4,"Press Enter for the default ("""").client_id> Just Hit EnterOAuth Client SecretLeave blank normally.Enter a string value. Press Enter for the default ("""").client_secret> Just Hit EnterScope that rclone should use when requesting access from drive.Enter a string value. Press Enter for the default ("""").Choose a number from below, or type in your own value 1 / Full access all files, excluding Application Data Folder. \ ""drive"" 2 / Read-only access to file metadata and file contents. \ ""drive.readonly"" / Access to files created by rclone only. 3 | These are visible in the drive website. | File authorization is revoked when the user deauthorizes the app. \ ""drive.file"" / Allows read and write access to the Application Data folder. 4 | This is not visible in the drive website. \ ""drive.appfolder"" / Allows read-only access to file metadata but 5 | does not allow any access to read or download file content."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_transferring-cloud-storage-data-with-rclone.html,5,"\ ""drive.metadata.readonly""scope> 1ID of the root folderLeave blank normally.Fill in to access ""Computers"" folders (see docs), or for rclone to usea non root folder as its starting point.Enter a string value. Press Enter for the default ("""").root_folder_id> Just Hit EnterService Account Credentials JSON file pathLeave blank normally.Needed only if you want use SA instead of interactive login.Leading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`.Enter a string value. Press Enter for the default ("""").service_account_file> Just Hit EnterEdit advanced config? (y/n)y) Yesn) No (default)y/n> nRemote configUse auto config?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_transferring-cloud-storage-data-with-rclone.html,6,"* Say Y if not sure * Say N if you are working on a remote or headless machiney) Yes (default)n) Noy/n> nPlease go to the following link: https://accounts.google.com/o/oauth2/auth?access_type=offline&client_id= CUT AND PASTE The URL ABOVE INTO A BROWSER ON YOUR LAPTOP/DESKTOP Log in and authorize rclone for accessEnter verification code> ENTER VERIFICATION CODE HEREConfigure this as a team drive?y) Yesn) No (default)y/n> n--------------------[remote1]type = drivescope = drivetoken = {""access_token"":"", removed ""}--------------------y) Yes this is OK (default)e) Edit this remoted) Delete this remotey/e/d> yCurrent remotes:Name Type==== ====remote1 drivee) Edit existing remoten) New remoted) Delete remoter) Rename remotec) Copy remotes) Set configuration passwordq) Quit confige/n/d/r/c/s/q> qStep 4: Sample commands: $ rclone lsd remote1:Transferring files to Google Drive, using the command below: $ rclone copy <source_folder> <remote_name>:<name_of_folder_on_gdrive>It looks something like below: $ rclone copy /home/user1 remote1:backup_home_user1Step 5: The files are transferred and you can find the files on your Google Drive."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_data-transfers_transferring-cloud-storage-data-with-rclone.html,7,Note: Rclone only copies new files or files different from the already existing files on Google Drive.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_research-project-space-rps.html,0,"Research Project Space (RPS) Description Research Project Space (RPS) volumes provide working space for sharing data and code amongst project or lab members. RPS directories are built on the same parallel file system (GPFS) like HPC Scratch. They are mounted on the cluster Compute Nodes, and thus they can be accessed by running jobs. RPS directories are backed up and there is no old file purging policy. These features of RPS simplify the management of data in the HPC environment as users of the HPC Cluster can store their data and code on RPS directories and they do not need to move data between the HPC Scratch and the HPC Archive file systems. Due to limitations of the underlying parallel file system, the total number of RPS volumes that can be created is limited. There is an annual cost associated with RPS. The disk space and inode usage in RPS directories do not count towards quota limits in other HPC file systems (Home, Scratch, and Archive)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_research-project-space-rps.html,1,"Calculating RPS Costs The PI should estimate the cost of the RPS volume by taking into account storage size and number of inodes (files). The cost is calculated annually. Costs are divided into the total space, in terabytes, and the number of inodes, in blocks of 200,000. 1 TB of Storage Cost: $100 200,000 inodes Cost: $100 An initial RPS volume request must include both storage space and inodes. Modifications of existing RPS volumes can include just Storage or just inode adjustments. An initial request includes 1TB and 200,000 inodes for an annual cost of $200. Example RPS Requests Requests can include more storage or files, as needed, such as 1TB and 400,000 inodes or 2TB and 200,000 inodes. Both of the previous examples would cost $300, since they are requesting an incremental increase of storage or inodes, respectively."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_research-project-space-rps.html,2,"This would be the breakdown of the examples listed above: 1 TB ($100) + 400,000 inodes ($200) = $300 2 TB ($200) + 200,000 inodes ($100) = $300 Submitting an RPS volume Request or Modification Step 1: Decide the size (in TB) and number of inodes (files) that is needed for one year The minimum size of an RPS request (to create a new RPS volume or extend an existing one) is 1TB of space. If this is a new/first request, you must purchase both storage and inodes. A typical request includes 200,000 inodes per TB of storage. Before submitting an RPS request (request for a new RPS volume or extending the size of an existing volume) PIs should estimate the growth of their data (in terms of storage space and number of files) during the entire year, rather than submitting a request based on their data storage needs at the time of the request."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_research-project-space-rps.html,3,"Step 2: Determine the cost of the request Determine the total annual cost of the request and the contact info of the School/Department/Center finance person. Step 3: Verify that the project PI has a valid HPC user account The PI administers the top level RTS directory and grants access to other users. Thus the PI must have a valid HPC user account at the time of request. Please note that the HPC user account of NYU faculty never expires and thus does not need to be renewed every year. If the PI does not have an HPC account, please request one here. Step 4: The PI submits the request to the HPC team via email Only PIs can submit RPS requests by contacting the HPC team via email (hpc@nyu.edu). Please include in the request the size (TB and number of inodes), and the contact information of the School/Department/Center finance person. The HPC team will review the request and will contact the PI with any questions."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_research-project-space-rps.html,4,"If the request is approved, the HPC team will create (or adjust) the RPS volume with the PI's HPC user account as the owner of the RPS directory. An invoice will be generated by the IT finance team. Current HPC RPS Stakeholders HPC RPS Stakeholders (as of February 2021) FAQs Data Retention and Backups How long can I keep the lab data in RPS? For as long as the lab pays for the RPS resources. Even if the current HPC cluster retires, the RPS volumes will be transferred to the next cluster How can I find out how much of the storage and inodes have I used in my lab RPS volume TBD What kind of backups are provided? Backups are done once a day (daily incremental). Backups are kept for 30 days. This means that if something was deleted more than 30 days ago, it won't be in the back ups and thus it won't be recoverable. Where are backups stored? RPS backups are stored on public cloud (AWS S3 Storage buckets). Billing and Payments What happens if I do not pay my bill?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_research-project-space-rps.html,5,"If the invoice is not paid for more than 60 days, the lab RPS directory will be 'tar'-ed and copied to an archival area. If 60 more days pass and the invoice is still not paid the tar files will be deleted. Can I pay for RPS using a credit card? Unfortunately we're unable to process credit card payments Can I pay for multiple years instead of paying every year? Yes, we can arrange for multiyear agreement"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_sharing-data-on-hpc.html,0,"Sharing Data on HPC Introduction To share files on the cluster with other users, we recommend using file access control lists (FACL) for a user to share access to their data with others. FACL mechanism allows a fine-grained control access to any files by any users or groups of users. We discourage users from setting '777' permissions with chmod, because this can lead to data loss (by a malicious user or unintentionally, by accident). Linux commands getfacl and setfacl are used to view and set access. ACL mechanism, just like regular Linux POSIX, allows three different levels of access control: Read (r) Write (w) eXecute (x) This level of access can be granted to user (owner of the file) group (owner group) other (everyone else) ACL allows to grant the same type access without modifying file ownership and without changing POSIX permissions. Viewing ACL Use getfacl to retrieve access permissions for a file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_sharing-data-on-hpc.html,1,"$ getfacl myfile.txt# file: myfile.txt # owner: ab123 # group: users user::rw- group::--- other::--- The example above illustrates that in most cases ACL looks just like the chmod-based permissions: owner of the file has read and write permission, members of the group and everyone else have no permissions at all."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_sharing-data-on-hpc.html,2,"Setting ACL Modify access permissions Use setfacl: # general syntax:$ setfacl [option] [action/specification] file # most important options are # -m to modify ACL # -x to remove ACL # -R to apply the action recursively (apply to everything inside the directory) # To set permissions for a user (user is either the user name or ID):$ setfacl -m ""u:user:permissions"" <file/dir> ## To set permissions for a group (group is either the group name or ID):$ setfacl -m ""g:group:permissions"" <file/dir> # To set permissions for others: $ setfacl -m ""other:permissions"" <file/dir> # To allow all newly created files or directories to inherit entries from the parent directory (this will not affect files which will be copied into the directory afterwards):$ setfacl -dm ""entry"" <dir> # To remove a specific entry:$ setfacl -x ""entry"" <file/dir> # To remove the default entries:$ setfacl -k <file/dir> # To remove all entries (entries of the owner, group and others are retained):$ setfacl -b <file/dir> Important: Give Access to Parent Directories in the Path When you would like to set ACL to say /a/b/c/example.out, you also need to set appropriate ACLs to all the parent directories in the path."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_sharing-data-on-hpc.html,3,"If you want to give read/write/execute permissions for the file /a/b/c/example.out, you would also need to give at least r-x permissions to the directories: /a, /a/b, and /a/b/c.Remove All ACL Entries # setfacl -b abcCheck ACLs # getfacl abc# file: abc # owner: someone # group: someone user::rw- group::r-- other::r-- You can see with 'ls -l' if a file has extended permissions set with setfacl: the '+' in the last column of the permissions field indicates that this file has detailed access permissions via ACLs: $ ls -latotal 304 drwxr-x---+ 18 ab123 users 4096 Apr 3 14:32 . drwxr-xr-x 1361 root root 0 Apr 3 09:35 .. -rw------- 1 ab123 users 4502 Mar 28 22:27 my_private_file -rw-r-xr--+ 1 ab123 users 29 Feb 11 23:18 dummy.txt Flags Please read 'man setfacl' for possible flags."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_sharing-data-on-hpc.html,4,"For example: '-m' - modify '-x' - remove '-R' - recursive (apply ACL to all content inside a directory) '-d' - default (set given settings as default - useful for a directory - all the new content inside in the future will have given ACL) Examples File ACL Example Set all permissions for user johnny to file named abc: # setfacl -m ""u:johnny:rwx"" abcCheck permissions: # getfacl abc# file: abc # owner: someone # group: someone user::rw- user:johnny:rwx group::r-- mask::rwx other::r-- Change permissions for user johnny: # setfacl -m ""u:johnny:r-x"" abcCheck permissions: # getfacl abc# file: abc # owner: someone # group: someone user::rw- user:johnny:r-x group::r-- mask::r-x other::r-- Directory ACL Example Let's say alice123 wants to share directory /scratch/alice123/shared/researchGroup/group1 with user bob123 ## Read/execute access to /scratch/alice123setfacl -m u:bob123:r-x /scratch/alice123 ## Read/execute access to /scratch/alice123/shared setfacl -m u:bob123:r-x /scratch/alice123/shared ## Read/execute access to /scratch/alice123/shared/researchGroup setfacl -m u:bob123:r-x /scratch/alice123/shared/researchGroup ## Now I can finally can give access to directory /scratch/alice123/shared/researchGroup/group1 setfacl -Rm u:bob123:rwx /scratch/alice123/shared/researchGroup/group1 Note: user bob123 will be able to see content of the following directories /scratch/alise123/ /scratch/alise123/shared /scratch/alise123/shared/researchGroup/ /scratch/alise123/shared/researchGroup/group1"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_squash-file-system-and-singularity.html,0,"Squash File System and Singularity View available datasets on the Datasets page. Working with Datasets Working with Datasets Writable ext3 overlay images have conda environments installed inside, Singularity can work with squashFS for fixed datasets, such as the coco datasets. /scratch/work/public/ml-datasets/coco/coco-2014.sqf/scratch/work/public/ml-datasets/coco/coco-2015.sqf/scratch/work/public/ml-datasets/coco/coco-2017.sqfsingularity exec \--overlay /scratch/wang/zzz/pytorch1.8.0-cuda11.1.ext3:ro \--overlay /scratch/work/public/ml-datasets/coco/coco-2014.sqf:ro \--overlay /scratch/work/public/ml-datasets/coco/coco-2015.sqf:ro \--overlay /scratch/work/public/ml-datasets/coco/coco-2017.sqf:ro \/scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif /bin/bash If you have many tiny files as fixed datasets, please make squashFS files to work with Singularity."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_data-management_squash-file-system-and-singularity.html,1,"Here is an example Make a temporary folder in /state/partition1, it is a folder in local hard drive on each computer node Unzip files there, for example Change access permissions in case we'll share files with others Convert to a single squashFS file on host http://www.iitk.ac.in/LDP/HOWTO/SquashFS-HOWTO/mksqoverview.html Copy this file to /scratch To test, files are in /imagenet-example inside Singularity container Singularity> find /imagenet-example | wc -l 1303 Singularity> find /state/partition1/sw77/imagenet-example | wc -l 1303 To delete the tempoary folder on host"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_datasets.html,0,"Datasets General The HPC team makes available a number of public sets that are commonly used in analysis jobs. The data sets are available Read-Only under /scratch/work/public/ml-datasets/ /vast/work/public/ml-datasets/ We recommend to use version stored at /vast (when available) to have better read performance Note: For some of the datasets users must provide a signed usage agreement before accessing Format Many datasets are available in the form of '.sqf' file, which can be used with Singularity."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_datasets.html,1,"For example, in order to use coco dataset, one can run the following commands $ singularity exec \ --overlay /<path>/pytorch1.8.0-cuda11.1.ext3:ro \ --overlay /vast/work/public/ml-datasets/coco/coco-2014.sqf:ro \ --overlay /vast/work/public/ml-datasets/coco/coco-2015.sqf:ro \ --overlay /vast/work/public/ml-datasets/coco/coco-2017.sqf:ro \ /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif find /coco | wc -l 532896 Data Sets COCO Dataset About data set: https://cocodataset.org/ Common Objects in Context (COCO) is a large-scale object detection, segmentation, and captioning dataset."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_datasets.html,2,"Dataset is available under /scratch /scratch/work/public/ml-datasets/coco/coco-2014.sqf /scratch/work/public/ml-datasets/coco/coco-2015.sqf /scratch/work/public/ml-datasets/coco/coco-2017.sqf /vast /vast/work/public/ml-datasets/coco/coco-2014.sqf /vast/work/public/ml-datasets/coco/coco-2015.sqf /vast/work/public/ml-datasets/coco/coco-2017.sqf ImageNet and ILSVRC About data set: ImageNet (image-net.org) ImageNet is an image dataset organized according to the WordNet hierarchy (Miller, 1995). Each concept in WordNet, possibly described by multiple words or word phrases, is called a “synonym set” or “synset”. ImageNet populates 21,841 synsets of WordNet with an average of 650 manually verified and full resolution images. As a result, ImageNet contains 14,197,122 annotated images organized by the semantic hierarchy of WordNet (as of August 2014). ImageNet is larger in scale and diversity than the other image classification datasets (https://arxiv.org/abs/1409.0575)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_datasets.html,3,"Note: WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept (https://wordnet.princeton.edu/) ILSVRC (subset of ImageNet) ILSVRC uses a subset of ImageNet images for training the algorithms and some of ImageNet’s image collection protocols for annotating additional images for testing the algorithms (https://arxiv.org/abs/1409.0575). The name comes from 'ImageNet Large Scale Visual Recognition Challenge (ILSVRC)'. Competition was moved to Kaggle (http://image-net.org/challenges/LSVRC/2017/) What is included (https://arxiv.org/abs/1409.0575)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_datasets.html,4,"1000 object classes approximately 1.2 million training images 50 thousand validation images 100 thousand test images Size of data is about 150 GB (for train and validation) Dataset is available under /scratch/work/public/ml-datasets/imagenet /vast/work/public/ml-datasets/imagenet Get access to Data New York University does not own this dataset. Please open the ImageNet site, find the terms of use (http://image-net.org/download), copy them, replace the needed parts with your name, send us an email including the terms with your name - thereby confirming you agree to the these terms. Once you do this, we can grant you access to the copy of the dataset on the cluster."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_datasets.html,5,Millions Songs About data set: https://labrosa.ee.columbia.edu/millionsong/ Dataset is available under /scratch/work/public/MillionSongDataset /vast/work/public/ml-datasets/millionsongdataset/ Twitter Decahose About data set: https://developer.twitter.com/en/docs/twitter-api/enterprise/decahose-api/overview/decahose NYU has a subscription to Twitter Decahose - 10% random sample of the realtime Twitter Firehose through a streaming connection Data are stored in GCP cloud (BigQuery) and on HPC clusters Greene and Peel (Parquet format). Please contact Megan Brown at The Center for Social Media & Politics to get access to data and learn the tools available to work with it.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_datasets.html,6,"On cluster dataset is available under (given that you have permissions) /scratch/work/twitter_decahose/ ProQuest Congressional Record About data set: ProQuest Congressional Record The ProQuest Congressional Record text-as-data collection consists of machine-readable files capturing the full text and a small number of metadata fields for a full run of the Congressional Record between 1789 and 2005. Metadata fields include the date of publication, subjects (for issues for which such information exists in the ProQuest system), and URLs linking the full text to the canonical online record for that issue on the ProQuest Congressional platform. A total of 31,952 issues are available. Dataset is available under: /scratch/work/public/proquest/ C4 About data set: c4 | TensorFlow Datasets A colossal, cleaned version of Common Crawl's web crawl corpus."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_datasets.html,7,"Based on Common Crawl dataset: https://commoncrawl.org Dataset is available under /scratch/work/public/ml-datasets/c4 /vast/work/public/ml-datasets/c4 GQA About data set: GQA: Visual Reasoning in the Real World (stanford.edu) Question Answering on Image Scene Graphs Dataset is available under /scratch/work/public/ml-datasets/gqa /vast/work/public/ml-datasets/gqa MJSynth About data set: Visual Geometry Group - University of Oxford This is synthetically generated dataset which found to be sufficient for training text recognition on real-world images This dataset consists of 9 million images covering 90k English words, and includes the training, validation and test splits used in the author's work (archived dataset is about 10 GB) Dataset is available under /vast/work/public/ml-datasets/mjsynth open-images-dataset About data set: Open Images Dataset – opensource.google A dataset of ~9 million varied images with rich annotations The images are very diverse and often contain complex scenes with several objects (8.4 per image on average)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_datasets.html,8,"It contains image-level labels annotations, object bounding boxes, object segmentations, visual relationships, localized narratives, and more Dataset is available under /scratch/work/public/ml-datasets/open-images-dataset /vast/work/public/ml-datasets/open-images-dataset Pile About data set: The Pile (eleuther.ai) The Pile is a 825 GiB diverse, open source language modeling data set that consists of 22 smaller, high-quality datasets combined together. Dataset is available under /scratch/work/public/ml-datasets/pile /vast/work/public/ml-datasets/pile Waymo open dataset About data set: Open Dataset – Waymo The field of machine learning is changing rapidly. Waymo is in a unique position to contribute to the research community with some of the largest and most diverse autonomous driving datasets ever released. Dataset is available under /vast/work/public/ml-datasets/waymo_open_dataset_v_1_2_0_individual_files"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_hardware-specs.html,0,"Hardware Specs The NYU HPC clusters are served by a General Parallel File System (GPFS) storage cluster. GPFS is a high-performance clustered file system software developed by IBM that provides concurrent high-speed file access to applications executing on multiple nodes of clusters. GPFS Configuration The NYU HPC cluster storage runs on Lenovo Distributed Storage Solution DSS-G hardware: 2x DSS-G 202 116 Solid State Drives (SSDs) 464TB raw storage 2x DSS-G 240 668 Hard Disk Drives (HDDs) 9.1PB raw storage Performance Read Speed: 78 GB per second read speeds Write Speed: 42 GB per second write speeds I/O Performance: up to 650k input/output operations per second (IOPS) Flash Tier Storage (VAST) An all flash file system, using VAST Flash storage, is now available on Greene. Flash storage is optimal for computational workloads with high I/O rates. For example, If you have jobs to run with huge amount of tiny files, VAST may be a good candidate."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_hardware-specs.html,1,"If you and your lab members are interested, please reach out to hpc@nyu.edu for more information. NVMe interface Total size: 778 TB Access: /vast is available for all users to read and available to approved users to write data. Research Project Space (RPS) Research Project Space (RPS) volumes provide working spaces for sharing data and code amongst project or lab members. RPS directories are available on the Greene HPC cluster. There is no old-file purging policy on RPS. RPS is backed up. There is a cost per TB per year and inodes per year for RPS volumes. More information on the Research Project Space is available page."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_hardware-specs.html,2,"Data Transfer Nodes Specs (gDTN) Node type: Lenovo SR630 Number of nodes: 2 CPU: 2x Intel Xeon Gold 6244 8C 150W 3.6GHz Processor Memory: 192GB (total) - 12x 16GB DDR4, 2933MHz Local disk: 1x 1.92TB SSD Infiniband interconnect: 1x Mellanox ConnectX-6 HDR100 /100GbE VPI 1-Port x16 PCIe 3.0 HCA Ethernet connectivity to the NYU High-Speed Research Network ( HSRN ): 200Gbit - 1x Mellanox ConnectX-5 EDR IB/100GbE VPI Dual-Port x16 PCIe 3.0 HCA"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_storage-status.html,0,"HPC Storage System Status Allocation and Utilization data Allocation and Utilization data Below you can find data for the following file system mounts GPFS file system: /home, /scratch, /archive VAST file system: /vast HDFS file system of Hadoop cluster Peel Note: To be able to see the panels below you need to be within NYU network (use VPN if you are not on campus)"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_system-status.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_hpc-storage_system-status.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing 404 The page you have entered does not exist Go to site home Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_peel_peel-system-status.html,0,Peel System Status Cloudera Manager Cloudera Manager The Cloudera Manager is an administrator tool that you can view the cluster status. You can access the Cloudera Manager in read-only mode by logging in with 'nyu' as both username and password. The NYU VPN is required for access. Yarn WebUI Yarn WebUI The Yarn WebUI lists all current and recently executed applications. SparkUI for each application is accessible through their corresponding link on the page. Resources Allocation and Utilization Resources Allocation and Utilization Note: for file system (HDFS) utilization data please refer to Storage System Status page Note: To be able to see the panels below you need to be within NYU network (use VPN if you are not on campus)
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_retired-clusters.html,0,"Retired Clusters Hudson The Hudson cluster was not decommissioned, but rather incorporated into the Greene cluster in April, 2023. In 2020, AMD and technology partner Penguin Computing Inc donated AMD GPU nodes in order to support COVID-19 research. This included a donation to NYU, which became the Hudson AMD cluster. Hudson consisted of 20 compute nodes (servers), each equipped with an AMD EPYC Rome 7642 processor (having 48 processing cores), 512 gigabytes (GB) of host memory, 8 MI-50 32GB graphics processing units (GPUs) and 2 terabyte (TB) of local solid state disk (SSD) for data storage. As of April, 2023, these nodes have been incorporated into the Greene cluster in order to consolidate the expanding AMD computational ecosystem at NYU. Peel The Peel Hadoop cluster was decommissioned on January 3rd, 2023. Peel was replaced by Dataproc on the Google Cloud Platform."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_retired-clusters.html,1,"Cloudera CDH 6.3.4 (Hadoop 3.0.0 with Yarn) 18 compute nodes CPU cores: 864 total (2x24-core per node) Total RAM: 6.9 TB (384 GB per node) HDFS total: 1.1 PB (64 TB per node) Prince The HPC Cluster Prince was decommissioned on February 1st, 2021 and replaced by the Greene cluster. Prince was a production cluster for a majority of computing tasks, like general computations, numerical simulations, AI, and more. CPU cores: 10,084 NVIDIA GPUs: 220 Total RAM: 58 TB Disk space: 2.3 PB Lustre and BeeGFS scratch file systems Infiniband interconnect Dumbo The Dumbo Hadoop cluster was decommissioned on February 22nd, 2021. Dumbo was replaced by the Peel cluster. Cloudera CDH 5.15.0 (Hadoop 2.6.0 with Yarn) 44 compute nodes CPU cores: 704 total (2x8-core per node) Total RAM: 5.6 TB (128 GB per node) HDFS total: 1.4 PB Brooklyn The Brooklyn Research Cluster (BRC) was decommissioned on April 1st, 2021."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_hpc-systems_retired-clusters.html,2,"OpenStack Software Used to create Bare metal machines, Virtual Machines (VMs), run containers, and Kubernetes clusters 25 servers CPU cores: 28 each server GPU NVIDIA: 4 each server RAM: 256 Gb each Disk space: 300 TB Ceph storage system Mercer The Mercer cluster was decommissioned on May 19th, 2017."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_services.html,0,Services Compute Compute Available High Performance Computing Clusters: Storage & Data Management Storage & Data Management Supported Research Initiatives Supported Research Initiatives Support & Research Consultation Support & Research Consultation Cloud Computing Cloud Computing Resources for Classes Resources for Classes Education / Tutorials Education / Tutorials
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support.html,0,"Training & Support HPC Training HPC Training Introductory Videos Introductory Videos Watch videos prepared by students like you Classes Offered by Library Classes Offered by Library Please find the topic you are interested in, date of the class, and sign up: All classes offered via the library calendar system can be viewed here (or in calendar view) HPC-specific classes can be viewed here XSEDE workshops XSEDE workshops Workshops on High Performance Computing topics More info: XSEDE HPC Workshop Calendar Tutorials (offered by HPC team) Tutorials (offered by HPC team) Support Support See the Support page for assistance and consultation requests."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics.html,0,"General HPC Topics In general, HPC is a vast and complicated area of knowledge. Within this section we provide information on some common topics. If you would like to see additional topics, please email at hpc@nyu.edu. Note: If you have written your own documentation that you believe would be helpful to other users, please let us know. We can evaluate if it would be a useful addition to the existing documentation."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips.html,0,"AI at HPC: Tips Intel Python GREENE cluster uses Intel CPUs, and thus your code can benefit from Intel distribution of Python. Look at their benchmarks and see if this would be useful for you. Intel distribution of Python together with library daal4py may make your ML code much faster! In order to experiment with that on cluster use module use /share/apps/intel/19.1.2/intelpython3/binNote: command below will show available environmental modules for python build with intel compilers. However this is not Intel distribution of Python module avail python/intel/3.8.6Tips on handling large data Parallel execution General approach You have several options to make your computations parallel Independent tasks If you can separate your large job to several independent tasks - do that, and submit a corresponding number of jobs."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips.html,1,"Grid Search across parameter space If you may do the grid search on independent nodes ML/Data processing algorithms supporting multiple CPUs on single node ML/Data processing algorithms supporting multiple CPUs on multiple nodes This usually requires an algorithm to be compiled with MPI Splitting data Some algorithms support splitting data, performing computations on independent chunks of data and then combining results Splitting features Some algorithms support splitting by features: performing computations on independent subsets for features and then combining results Python It is always good to look at options listed in official documentation: here and here. If you use Python thread-based parallelism, note that Python uses GIL (Global Interpreter Lock) - at any one time only a single thread can acquire a lock for a Python object. If you use threads, access to global objects will jump from one thread to another."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips.html,2,"In case you use Python multiprocessing: there is no access to global variables at all. Objects will be pickled and passed by scheduler to independent processes. There are other tools addressing the parallelization task like Ray, mpi4py, dask-mpi. We have examples for a couple of options you may find useful: R A well written overview of parallelization options for R is presented here https://www.glennklockwood.com/data-intensive/r/ Note: It is important to specify number of cores requested for R correctly A wrong way No matter how many cores you requested in SLURM, this will instruct R to use all the cores on a node (20, 28, or even 40). A job that requests a number of CPUs but uses more or fewer cores than requested will be detected by monitoring tools and then killed An acceptable method: Also set 1 as value for OMP_NUM_THREADS. Some R algorithms will still try to use more cores if OMP_NUM_THREADS is not setup."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips.html,3,"Thus, please do # inside R scriptSys.setenv(OMP_NUM_THREADS = “1”) Spark Spark provides an alternative method to distribute computations across multiple CPUs and multiple nodes. Spark comes with Spark ML - a collection of spark ML algorithms accessibly using pyspark and sparklyr. You can read more information here: Spark, MLib, PySpark, sparklyr Some may find useful algorithms designed by H2O, which can be executed with sparkling water (H2O on top of Spark): Sparkling Water Example: Spark interactive: Scala, Python, R"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_dask-example.html,0,"DASK Example DASK enables scalable analytics in Python. More details here Prepare Conda environment Create conda environment with Python version of your liking. Activate it module purgemodule load anaconda3/2020.07conda create -p $(pwd)/cenv python=3.7conda activate <path_to_env>Install mpi4py DASK rely on mpi4py to distribute jobs across many nodes. This is python package shall be compiled (so, don't use 'conda install', use 'pip install') only after mpicc made available (more info). # Load MPImodule load openmpi/intel/4.1.1# Compile mpy4pipip install mpi4py# You can also compile daskpip install daskpip install dask distributed --upgradepip install dask_mpiInstall conda packages Conda packages come precompiled and thus are fast to install. They use MKL libraries and thus are efficient. conda install numpy pandas scikit-learnRUN using sbatch SVM example file 'send' Please change '<path_to_cenv>' to actual value Run execution using 'sbatch send'. Note: below we request 2*3=6 CPUs."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_dask-example.html,1,"However for workers we will have 4 (2 less) #!/bin/bash#SBATCH --job-name=dask_joblib#SBATCH --nodes=2#SBATCH --cpus-per-task=1#SBATCH --ntasks-per-node=3#SBATCH --mem=8GB#SBATCH --time=1:00:00path_to_cenv=""<path_to_cenv>""module purgemodule load openmpi/intel/4.1.1mpiexec --mca mpi_warn_on_fork 0 bash -c ""module purge; openmpi/intel/4.1.1; module load anaconda3/2020.07; source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh; conda activate ""$path_to_cenv""; export PATH=""$path_to_cenv/bin:$PATH""; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; python -u mpi_gird_search.py""file mpi_gird_search.py SKLEARN example based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html Please notice interface='ib0', which is specifying to use infiniband (fast connection between nodes) instead of default (ethernet) ## ACTIVATE DASKimport osfrom dask_mpi import initializeinitialize(interface='ib0', local_directory = os.getenv('TMPDIR'))from dask.distributed import Client, progressif __name__ == '__main__': client = Client() # https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.metrics import classification_report from sklearn.svm import SVC print(__doc__) # Loading the Digits dataset digits = datasets.load_digits() # To apply an classifier on this data, we need to flatten the image, to # turn the data in a (samples, feature) matrix: n_samples = len(digits.images) X = digits.images.reshape((n_samples, -1)) y = digits.target # Split the dataset in two equal parts X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.5, random_state=0) # Set the parameters by cross-validation tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}, {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}] scores = ['precision', 'recall'] for score in scores: print(""# Tuning hyper-parameters for %s"" % score) print() clf = GridSearchCV( SVC(), tuned_parameters, scoring='%s_macro' % score ) clf.fit(X_train, y_train) print(""Best parameters set found on development set:"") print() print(clf.best_params_) print() print(""Grid scores on development set:"") print() means = clf.cv_results_['mean_test_score'] stds = clf.cv_results_['std_test_score'] for mean, std, params in zip(means, stds, clf.cv_results_['params']): print(""%0.3f (+/-%0.03f) for %r"" % (mean, std * 2, params)) print() print(""Detailed classification report:"") print() print(""The model is trained on the full development set."") print(""The scores are computed on the full evaluation set."") print() y_true, y_pred = y_test, clf.predict(X_test) print(classification_report(y_true, y_pred)) print() # Note the problem is too easy: the hyperparameter plateau is too flat and the # output model is the same for precision and recall with ties in quality.Dask jobqueue This is a note, aimed to save your time."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_dask-example.html,2,"There is a way to run interactive multi-worker jobs: https://jobqueue.dask.org/en/latest/, which is not supported at our cluster. This approach relies on slurm's ""--ntasks"" parameter which is not allowed to use on NYU HPC cluster"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_independent-r-tasks-example.html,0,"Independent R Tasks Example If your jobs can run independently on different nodes, the example below may give you a fast start Here we will implement: Multiple independent jobs on different nodes. Parallel execution within each node. Slurm provides a job array mechanism. * submit.sbatch #!/bin/bash#SBATCH --job-name=my-job#SBATCH --nodes=1#SBATCH --cpus-per-task=4#SBATCH --mem=5GB#SBATCH --time=24:00:00#SBATCH --array=1-8# env. variable SLURM_ARRAY_TASK_ID will be availablemodule purgemodule load r/gcc/4.1.0R --no-save -q -f main.RThis will submit 8 jobs, each using 4 cpus and 1 node * main.R example based on https://www.glennklockwood.com/data-intensive/r/foreach-parallelism.html We will run multiple independent jobs on different nodes. Parallel execution within each node. Some of our users have to have independent directory structure for every job, and thus we will do that in this example Let us assume your job files are inside directory R-test."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_independent-r-tasks-example.html,1,"We will use the following script, which will copy your whole project directory, and then run an sbatch job: * bash script submitting multiple jobs (say 'run_experiment.sh') ########################################################################### * create file run_experiment.sh in directory containing R-test directory# * make it executable:# chmod +x run_experiment.sh# * run desired job number# ./run_experiment.sh 2########################################################################### experiment numberexperiment=$1if [ -z ""$experiment"" ]; then echo ""Please provide number of the experiment after the command call"" exit 0fi## check if directory already existsif [ -d ""R-test-$experiment"" ];then echo ""Directory R-test-$experiment already exists."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_independent-r-tasks-example.html,2,"Exiting"" exit 0ficp -r R-test R-test-$experimentcd R-test-$experimentsbatch submit.sbatch* batch file ('submit.sbatch') with parameter #!/bin/bash#SBATCH --job-name=my_job#SBATCH --nodes=1#SBATCH --cpus-per-task=4#SBATCH --mem=5GB#SBATCH --time=24:00:00module purgemodule load r/gnu/3.5.1R --no-save -q -f main.R* R script accepting parameter The above scripts were based on this example. Make sure your R script uses relative paths, when you are using this approach. (You actually want to use relative paths all the time, to make your projects movable/reproducible/better_manageable with R projects, renv, github/bitbucket, etc.) data <- read.csv('dataset.csv')library(future)n_cores <- availableCores()library(foreach)library(doMC)registerDoMC(n_cores)results <- foreach( i = c(25,25,25,25) ) %dopar% { kmeans( x=data, centers=4, nstart=i )}write.csv(results, file = ""results.RData"")"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_joblib-example.html,0,"Joblib Example You can always read more detail on official page https://joblib.readthedocs.io/en/latest/ Affinity issue Affinity issue On Greene, you have to do this trick to address problem with affinity (more info) Start Python python3Execute the following in your Python session ## see current affinityimport osos.sched_getaffinity(0)## we see only one CPU# reset affinity - read about affinity in man tasksetos.system(""taskset -p 0xFFFFFFFF %d"" % os.getpid())# checkos.sched_getaffinity(0)Quit Python exit()Examples to run on Greene: Examples to run on Greene: Collection of examples is available on the developer's website: Parallel Processes This will run on several CPUs inside one node You can pass as a parameter only objects which can be pickled. Or use this approach def myfunc: i*2Parallel(n_jobs=2)(delayed(myfunc)(i) for i in range(10))Parallel Threads This will run using many thread on one CPU only You can pass any objects present in Python. There is no need to pickle them."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_joblib-example.html,1,"def myfunc: i*2Parallel(n_jobs=2, prefer=""threads"")(delayed(myfunc)(i) for i in range(10))"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_large-number-of-small-files.html,0,"Large Number of Small Files Motivation Many datasets contain a large number of files (for example ImageNet contains 14 million images, with ~150 GB size). How to deal with this data? How to store it? How to use for computations? Long-term storage of data is not an issue - an archive like tar.gz can handle this pretty well. However, when you want to use data in computations, the performance may depend on how you handle the data on disk. Here are some ideas you can try and evaluate performance for your own project Squash file system to be used with Singularity Please read here Use jpg/png files on disk One option is to store image files (like png or jpg) on the disk and read from disk directly. An issue with this approach, is that many linux file system can hold only a limited number of files."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_large-number-of-small-files.html,1,"# One can open greene cluster and run the following command$ df -ih /scratch/Filesystem Inodes IUsed IFree IUse% Mounted on10.0.0.40@o2ib:10.0.0.41@o2ib:/scratch1 1.6G 209M 1.4G 14% /scratchThis shows us that the total number of files '/scratch' can hold (currently) is about 1.6 G. This looks like a large number. But let us translate this into number of datasets like ImageNet (14 mil images) -> 100 datasets like that would almost fully occupy Total possible slots for files! This is a problem! And even if you can ignore this on your own PC, on HPC, there is a limit of files each user can put on /scratch to prevent such problems. This is the reason why you just can't extract all those files in '/scratch' SLURM_TMPDIR Another option would be to start SLURM job and extract everything into $SLURM_TMPDIR. This can work, but would require to do un-tar every time you run SLURM command."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_large-number-of-small-files.html,2,"SLURM_RAM_TMPDIR You can also use the custom-made RAM mapped disk using #SLURM_RAM_TMPDIR while submitting the job. In this case when you start a job you first un-tar your files to $SLURM_RAM_TMPDIR and then read from there. This basically requires you to use 2*(size of the data) size of RAM just to hold the data. Binary files (pickle, etc) Store data in some binary file (say pickle in Python) which you load fully when you start SLURM job. This option may require a lot of RAM - thus you may have to wait a long time for scheduler to find resources for your job. Also this approach would not work on regular PC without so much RAM, and thus your scripts are not transferable. Container files, one-file databases Special containers, which allow to either load data fast fully or access chosen elements without loading the whole dataset into RAM. SQLite If you have structured data, a good option would be to use SQLite."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_large-number-of-small-files.html,3,"Please refer to this page for more information HDF5 One can think about HDF5 file as a ""container file"" (database of a sort), which holds a lot of objects inside. HDF5 files do not have a file size limitation, and can hold huge number of objects inside, providing fast read/write access to those objects. It is easy to learn how to subset data and load to RAM only to those data objects that you need. This format is extensively in Academia and Industry across the world - look here More info: book (free with NYU email) hdf5 supports reading and writing in parallel, so you can use several nodes reading from the same file. More info: https://support.hdfgroup.org/HDF5/PHDF5/ LMDB LMDB (Lightning Memory-Mapped Database) is a light-weight, high-speed embedded database for key-value data. Essentially, this is a large file sitting on the disk that contains a lot of smaller objects inside. This is a memory-mapped database meaning, file can be larger than RAM."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_large-number-of-small-files.html,4,"OS is responsible for managing the pages (like caching frequently uses pages). For practical use it means: say you have 10 GB of RAM, and LMDB file of 100 GB. When you connect to this file, OS may deside to load 5GB to RAM, and the rest 95GB will be attached as virtual memory. PRINCE does not have limit for virtual memory. Of course, if your RAM is larger than LMDB file, this database will perform the best, as OS will have enough resources to keep what is needed directly in RAM. Note: when you write key-value pairs to LMDB they have to be byte-encoded. For example, if you use Python you can use: for string 'st.encode()', for np.array use 'ar.tobytes()', or in general pickle.dumps() Problem with large number of files: LMDB uses B Tree, which has O(long n) complexity for search."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_large-number-of-small-files.html,5,"Thus, when number of elements in LMDB becomes really big, search of specific element slows down considerably More info: LMDB supports reading by many readers and many parallel thread from the same file Formats inside HDF5/LMDB: binary, numpy, other.. One can store data in different way inside LMDB or HDF5. For example we can store binary representation of jpeg, or we can store python numpy array. In the first case file can be read from any language, in the second - only from Python. We can also store objects from other languages - for example tibble in R Other formats There are other formats like Bcolz, Zarr, and others. Some examples can be found here. Example Code A benchmarking of various ways of reading data was performed on now retired Prince HPC cluster. You can find the code used to perform that benchmarking and the results at this repository. For those of you interested in using multiple cores for data reading, this code example below may be useful."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_large-number-of-small-files.html,6,Multiple cores on the same node are used. Parallelization is based on joblib Python module
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_llm-on-hpc.html,0,"LLM on HPC Here we provide an example of how one can run a Large-language model (LLM) on NYU Greene cluster Prepare environment Install Singularity using instructions here (choose 50 Gb overlay file) After launching the Singularity container in read/write mode, install transformers library using pip Prepare script Create a python script using the following code from sections 1-9 and save it in a file called huggingface.py Importing necessary modules import torch import numpy as np from transformers import AutoTokenizer, AutoModel Create a list of reviews ""What is the monthly premium for Medicare Part B?"", ""How do I terminate my Medicare Part B (medical insurance)?"", ""How do I sign up for Medicare?"", ""Can I sign up for Medicare Part B if I am working and have health insurance through an employer?"", ""How do I sign up for Medicare Part B if I already have Part A?""] Choose the model name from huggingface’s model hub and instantiate the model and tokenizer object for the given model."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_llm-on-hpc.html,1,"We are setting output_hidden_states as True as we want the output of the model to not only have loss, but also the embeddings for the sentences. tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModel.from_pretrained(model_name, output_hidden_states=True) Create the ids to be used in the model using the tokenizer object. We set the return_tensors as “pt” as we want to return the pytorch tensor of the ids. ids = tokenizer(texts, padding=True, return_tensors=""pt"") Set the device to cuda, and move the model and the tokenizer to cuda as well."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_llm-on-hpc.html,2,"Since, we will be extracting embeddings, we will only be performing a forward pass of the model and hence we will set the model to validation mode using eval(): model.eval() Performing the forward pass and storing the output tuple in out: out = model(**ids) Extracting the embeddings of each review from the last layer: For the purpose of classification, we are extracting the CLS token which is the first embedding in the embedding list for each review: We can check the shape of the final sentence embeddings for all the reviews. The output should look like torch.Size([6, 768]), where 6 is the batch size as we input 6 reviews as shown in step 2b, and 768 is the embedding size of the RoBERTa model used."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_llm-on-hpc.html,3,"Prepare Sbatch file After saving the above code in a script called huggingface.py, create a file called run.SBATCH with the the following code: module purge if [ -e /dev/nvidia0 ]; then nv=""--nv""; fi singularity exec $nv \ --overlay /scratch/netID/my_env/overlay-50G-10M.ext3:rw \ /scratch/work/public/singularity/cuda11.2.2-cudnn8-devel-ubuntu20.04.sif \ /bin/bash -c ""source /ext3/env.sh; python /scratch/netID/path-to-script/huggingface.py"" Run the run.SBATCH file sbatch run.SBATCHThe output can be found in huggingface.out Acknowledgements Instructions are developed and provided by Laiba Mehnaz, a member of AIfSR"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_r-python-in-the-cloud.html,0,"R & Python in the Cloud Below is some information you may find useful for your work in GCP Use R in Google cloud: cloudyr project Use R in Google cloud: cloudyr project Use Python in Google Cloud Use Python in Google Cloud Codelabs - Google Developers Codelabs provide a guided, tutorial, hands-on coding experience. Most Codelabs tutorials will step you through the process of building a small application, or adding a new feature to an existing application. They cover a wide range of topics such as Android Wear, Google Compute Engine, Project Tango, and Google APIs on iOS."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_spark-interactive-scala-python-r.html,0,"Spark interactive: Scala, Python, R Prepare Prepare directory mkdir /scratch/$USER/spark_pycd /scratch/$USER/spark_pyGet Spark Go to https://spark.apache.org/downloads.html and choose a desired version of spark (pre-built for Hadoop) Click to download link - choose mirror - Copy URL, for example: https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz download wget <url from above>unzip mkdir sbintar -xvf spark-*.tgz -C sbin --strip 1Get compute resources using slurm Note: make sure to request enough RAM srun --cpus-per-task=2 --nodes 2 --mem=10GB --pty /bin/bashSetup env variables and start Spark cluster (take script from -> here <- and copy it to your directory) source spark-prince-prepare.shStart SPARK within Slurm job environment start_all# later, you can stop Spark cluster using command stop_allConnect to running Spark cluster Spark UI After you will start spark, script will tell you location of logs: -------------------start spark starting org.apache.spark.deploy.master.Master, logging to <PATH_TO_LOGS> Read this log file You will see the URL for web UI Use this <URL_WEB_UI> to setup tunnel You can use putty or a separate terminal window for port forwarding Open http://localhost:8080/ in browser Execute Scala spark-shell $SPARK_URL# or# ./sbin/bin/spark-shell $SPARK_URLPython Conda environment module load anaconda3/2020.07## pythonconda create -p $(pwd)/cenv python=3.7conda activate /scratch/<NetID>/spark_py/cenvInstall packages as needed ## for pythonconda install -c conda-forge pysparkExample import osimport pysparkfrom pyspark import SparkContextsc =SparkContext(master = os.getenv('SPARK_URL'))## testnums= sc.parallelize([1,2,3,4]) nums.take(1) squared = nums.map(lambda x: x*x).collect()for num in squared: print('%i ' % (num))R Conda environment module load anaconda3/2020.07## Rconda create -p $(pwd)/cenv r=4.1conda activate /scratch/<NetID>/spark_py/cenvInstall packages as needed ## for Rconda install -c r r-sparklyr r-tidyverseconda install -c conda-forge r-lahman r-nycflights13 ## for test example belowExample library(sparklyr)conf <- spark_config()#conf$spark.dynamicAllocation.enabled <- ""false""conf$sparklyr.connect.cores.local <- Sys.getenv('SLURM_JOB_CPUS_PER_NODE')sc <- spark_connect(master = Sys.getenv('SPARK_URL'))library(dplyr)iris_tbl <- copy_to(sc, iris)flights_tbl <- copy_to(sc, nycflights13::flights, ""flights"")batting_tbl <- copy_to(sc, Lahman::Batting, ""batting"")src_tbls(sc)flights_tbl %>% filter(dep_delay == 2)spark_disconnect_all()"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_sqlite-handling-large-structured-data.html,0,"SQLite: Handling Large Structured Data Overview Storing your data in SQLite, format allows you to get benefits of database, and at the same time simple and storage of data in a file on a disk. From developer's website: ""SQLite is the most used database engine in the world. SQLite is built into all mobile phones and most computers and comes bundled inside countless other applications that people use every day. The SQLite file format is stable, cross-platform, and backwards compatible and the developers pledge to keep it that way through at least the year 2050."" Some use-cases You think you need MySQL, PostreSQL, etc for your ML project. Usually you don't You have to deal with hundreds of GB of table-stuctured data (or larger) and your script (for whatever reason) can't be made parallel. You would request a lot of RAM and work with data slowly..... This would be a waste of RAM."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_sqlite-handling-large-structured-data.html,1,"It is better in this case to request smaller amount of RAM and read data (efficiently) from disk - for example using SQLite Benefits: You are not limited by RAM any longer comparing to other file formats SQLite is very good in selecting certain lines (especially if you use indexing) you can use familiar dplyr syntax or execute SQL queries directly dplyr is an interface for working with data in a database, not for modifying remote tables. DBI package allows to both read and modify tables SQLite is actually faster for common data analysis tasks than other popular databases. You can have multiple threads accessing an SQLite database simultaneously (for read operations. Writing is more tricky) Merging/Joining datasets on disk Major benefits of SQLite comparing to MySQL (PostgreSQL, etc) You control your own data (sqlite file). You don't depend on any service like MySQL You can copy a file to your own laptop and work with it Again, SQLite is faster!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_sqlite-handling-large-structured-data.html,2,"Limits SQLite has some limitations in terms of concurrency, which usually don't apply for typical ML/AI jobs. Read more here. Command line (CLI) example Create environment mkdir projects/sqlite-testcd projects/sqlite-testconda create -p ./cenvconda activate ./cenvconda install -y sqliteThen follow this SQLite example. sqlite3 db_file.sqlitecreate table tbl1(one varchar(10), two smallint);insert into tbl1 values('hello!',10);insert into tbl1 values('goodbye', 20);select * from tbl1;Now Close session (Ctrl-D). Reopen session to check if changes are saved sqlite3 db_file.sqliteselect * from tbl1;R example Install Here we use conda, as a great way to keep everything isolated and reproducible. Note: conda will install pre-compiled packages."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_ai-at-hpc-tips_sqlite-handling-large-structured-data.html,3,"Which is good (faster) and bad (not fully optimized for a specific hardware) Alternative: install packages to a local directory or use renv as described here mkdir /scratch/$USER/projects/myTempProjectcd /scratch/$USER/projects/myTempProjectmodule load anaconda3/2020.07 conda create -p ./cenv -c conda-forge r=4.1conda activate ./cenvconda install -c r r-rsqliteconda install -c r r-tidyverseconda install -c conda-forge r-remotesconda install -c r r-featherconda install -c r r-nycflights13 Note: window functions (row_number in particular) require newer version of rsqlite (https://github.com/r-dbi/RSQLite/issues/268) Rremotes::install_github(""r-dbi/RSQLite"")## update ALLSave list of installed packages for reproducibility ## conda list --export > requirements.txtUse Many examples can be found here: dplyr syntax https://db.rstudio.com/dplyr/ SQL syntax https://db.rstudio.com/databases/sqlite/ Copy data frame to database (dplyr) copy_to(con, nycflights13::flights, ""fl"", temporary=FALSE)Or copy data to database using DBI dbCreateTable(con, ""fl"", nycflights13::flights, temporary = FALSE)dbAppendTable(con, ""fl"", nycflights13::flights)Connect to a specific table dbListTables(con)df_con <- tbl(con, ""fl"")## check number of rowsdf_con %>% count()Subset df_temp <- df_con %>% filter( row_number() %in% c(1, 3) ) %>% collectSave as feather feather::write_feather(df_temp, paste0(""file_"", ind, "".feather""))Alternative: read csv file to SQLite directly If you already have a large csv file on disk, and you don't want to read it to RAM, you can read it to SQLite file directly UI for SQLite - SQLiteStudio Once you have SQLite file, you can easily transfer it to your own laptop and explore it using SQLiteStudio, if you like to use UI instead of terminal"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,0,"Hadoop User Guide NOTICE: This page refers to the Peel Hadoop Cluster which was retired in 2023 and is in the process of being archived. Its contents should not be referenced for Dataproc Courses. What is Hadoop? Hadoop is an open-source software framework for storing and processing big data in a distributed/parallel fashion on large clusters of commodity hardware. Essentially, it accomplishes two tasks: massive data storage and faster processing. The core Hadoop consists of HDFS - the Hadoop File System - and Hadoop implementation of MapReduce. What is MapReduce? MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm on a cluster. A MapReduce job splits a large data set into independent chunks and organizes them into key-value pairs for parallel processing. The mapping and reducing functions receive not just values, but (key, value) pairs."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,1,"Every MapReduce job consists of at least two parts: The Mapper The Reducer Mapping Phase: Takes input as <key,value> pairs, processes them, and produces another set of intermediate <key,value> pairs as output. Reducing Phase: Reducing lets you aggregate values together. A reducer function receives an iterator of input values from an input list. It then combines these values together, returning a single output value. MapReduce Data Flow for WordCount Problem To Trigger the Job export HADOOP_LIPATH=/opt/cloudera/parcels/CDH-6.3.4-1.cdh6.3.4.p0.6626826/libhadoop jar <jarfilename>.jar <DriverClassName> <ip_file_in_HDFS> <op_dir_name>Where <jarfilename>.jar is the jar file from Mapper, Reducer and DriverClass. Accessing the Peel Hadoop Cluster All active HPC users have an account on Peel. If you need an account, review the HPC Getting and Renewing Accounts page for instructions on how to get an account."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,2,"The Peel login nodes can be reached directly using the following command (NYU VPN required): ssh <NetID>@peel.hpc.nyu.eduFor more details about logging into the Peel cluster read the Accessing HPC Systems page. YARN Scheduler YARN is the resource manager and job scheduler in the Peel cluster. YARN allows you to use various data processing engines for batch, interactive, and real-time stream processing of data stored in HDFS. Job Queues The memory available to users’ Yarn containers is 4.45 TB in total. There is a queue named 'q1' created to accommodate users with a large memory requirement. Majority of users are in the 'default' queue which is guaranteed resources. If you want to be placed in 'q1', please contact us. To check which queue you are using, run a Spark application then go to the page All Yarn Applications, and look at the 'Queue' column for your application. Application status and logs Please find the list of current running apps using 'Yarn' script."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,3,"Running the yarn script without any arguments prints the description for all commands. $ yarn application -listTo kill a currently running app because the submitted app started malfunctioning or in worst case scenario, it's stuck in an infinite loop. Get the app ID and then kill it as given below $ yarn application -kill <application_ID>To download an app logs for examination on the command line $ yarn logs -applicationId <application_ID>What is Spark? Spark is a framework for performing general data analytics on a distributed computing cluster like Hadoop. It provides in-memory computations for increased speed and data processing over mapreduce. It runs on top of existing hadoop cluster and access hadoop data store (HDFS), can also process structured data in Hive and Streaming data from HDFS, Flume, Kafka, Twitter. Running Spark Jobs The default deploy mode for Spark on Peel is set to 'cluster'."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,4,"When running Spark jobs, please make sure Spark is in the 'cluster' deploy mode, which helps reduce load on the login nodes, which are not for computing. To get an interactive session for debugging, run the following: $ pyspark --deploy-mode clientor $ spark-shell --deploy-mode clientIf you need Python 3 for Spark, use the setup: $ module purge$ module load python/gcc/3.7.9 $ pyspark [......]Add the option '--conf spark.yarn.submit.waitAppCompletion=false' to your Spark command, if you want to exit from a terminal after applications submitted. Running PySpark with Conda Env The section is based on a post in the Cloudera Community. You may read the article before trying it out. We modified the recipe, tailored it for Peel and tested it. And it works great. The example demonstrates the use of Conda env to transport a python environment with a PySpark application needed to be executed."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,5,"This sample application uses the NLTK package with the additional requirement of making tokenizer and tagger resources available to the application as well. So it is a bit complicated than those utilizing many other packages. Users are encouraged to make own Conda env by following the steps as demonstrated here. The sample application 'spark_nltk_sample.py': import osimport sysfrom pyspark.context import SparkContextfrom pyspark.conf import SparkConfconf = SparkConf()conf.setAppName(""spark-ntlk-env"")sc = SparkContext(conf=conf)data = sc.textFile('/user/tst867/1970-Nixon.txt')def word_tokenize(x): import nltk return nltk.word_tokenize(x)def pos_tag(x): import nltk return nltk.pos_tag([x])words = data.flatMap(word_tokenize)words.saveAsTextFile('./nixon_tokens')pos_word = words.map(pos_tag)pos_word.saveAsTextFile('./nixon_tokens_pos')Below are the commands to run, to make the magic happening."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,6,"[tst867@hlog-1 ~]$ module load miniconda3/2020.11[tst867@hlog-1 ~]$ conda create -p /scratch/tst867/conda/envs/nltk_env --copy -y -q python=3.7 nltk numpy[tst867@hlog-1 ~]$ cd /scratch/tst867/conda/envs/[tst867@hlog-1 envs]$ zip -r nltk_env.zip nltk_env[tst867@hlog-1 envs]$ conda info --envs[tst867@hlog-1 envs]$ source activate /scratch/tst867/conda/envs/nltk_env(nltk_env) [tst867@hlog-1 envs]$ python -m nltk.downloader -d nltk_data all(nltk_env) [tst867@hlog-1 envs]$ conda deactivate[tst867@hlog-1 envs]$ hdfs dfs -put nltk_data/corpora/state_union/1970-Nixon.txt /user/tst867/[tst867@hlog-1 envs]$ cd nltk_data/tokenizers/[tst867@hlog-1 tokenizers]$ zip -r ../../tokenizers.zip *[tst867@hlog-1 tokenizers]$ cd ../../[tst867@hlog-1 envs]$ cd nltk_data/taggers/[tst867@hlog-1 taggers]$ zip -r ../../taggers.zip *[tst867@hlog-1 taggers]$ cd ../../# Using vi, emacs or nano editor, create the application script with content as shown previously [tst867@hlog-1 envs]$ emacs -nw spark_nltk_sample.py# At this point we should have generated these files and directories [tst867@hlog-1 envs]$ ls -ltrtotal 541186-rw-rw-r-- 1 tst867 tst867 519 Apr 6 14:54 spark_nltk_sample.pydrwxrwsr-x 10 tst867 tst867 4096 Apr 6 15:42 nltk_env-rw-rw-r-- 1 tst867 tst867 483854238 Apr 6 15:43 nltk_env.zipdrwxrwsr-x 12 tst867 tst867 4096 Apr 6 15:47 nltk_data-rw-rw-r-- 1 tst867 tst867 27415412 Apr 6 15:49 tokenizers.zip-rw-rw-r-- 1 tst867 tst867 42663686 Apr 6 15:50 taggers.zip# Run spark-submit [tst867@hlog-1 envs]$ PYSPARK_PYTHON=./NLTK/nltk_env/bin/python spark-submit --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./NLTK/nltk_env/bin/python --conf spark.yarn.appMasterEnv.NLTK_DATA=./ --conf spark.executorEnv.NLTK_DATA=./ --archives nltk_env.zip#NLTK,tokenizers.zip#tokenizers,taggers.zip#taggers spark_nltk_sample.py# The application succeeded, now to check results[tst867@hlog-1 envs]$ hdfs dfs -cat /user/tst867/nixon_tokens/* | head -n 20[tst867@hlog-1 envs]$ hdfs dfs -cat /user/tst867/nixon_tokens_pos/* | head -n 20Once the zip files with Conda env folded in, we may re-use them to submit many applications requiring same packages."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,7,"Additionally we can put the zip files to a HDFS directory, and tell Spark to grab them there by giving option e.g.: --archives hdfs:///user/tst867/archives/nltk_env.zip#NLTK,hdfs:///user/tst867/archives/tokenizers.zip#tokenizers,hdfs:///user/tst867/archives/taggers.zip#taggersUsing Hive Apache Hive is a Data Warehouse software that facilitates querying and managing large datasets residing in a distributed storage (Example: HDFS). Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called HiveQL. The Hive Query Language (HiveQL or HQL) for MapReduce processes structured data using Hive. We configure Sentry so that by default a regular user can see only your own database which is pre-created. We can grant permission to your project members' database upon request."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,8,"[tst867@hlog-1 ~]$ beeline --silentWARNING: Use ""yarn jar"" to launch YARN applications.SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-6.3.4-1.cdh6.3.4.p0.6626826/jars/log4j-slf4j-impl-2.8.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-6.3.4-1.cdh6.3.4.p0.6626826/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]beeline> !connect jdbc:hive2://hm-1.hpc.nyu.edu:10000/Enter username for jdbc:hive2://hm-1.hpc.nyu.edu:10000/: tst867Enter password for jdbc:hive2://hm-1.hpc.nyu.edu:10000/: ****************0: jdbc:hive2://hm-1.hpc.nyu.edu:10000/> use tst867;0: jdbc:hive2://hm-1.hpc.nyu.edu:10000/> create table messages5 (users STRING, post STRING, time BIGINT, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;0: jdbc:hive2://hm-1.hpc.nyu.edu:10000/> load data inpath 'hdfs://horton.hpc.nyu.edu:8020/user/tst867/user_posts.txt' overwrite into table messages5; 0: jdbc:hive2://hm-1.hpc.nyu.edu:10000/> 0: jdbc:hive2://hm-1.hpc.nyu.edu:10000/> insert overwrite directory '/user/tst867/hiveoutput' select * from foo;Using the Impala Shell Apache Impala is and open source parallel processing SQL query engine running on all compute nodes (hc[01-18].nyu.cluster)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,9,"To connect to Impala use hostname where impala daemon is running. In fact, impala daemons are running on all computing nodes on Peel 'hc[01-18].nyu.cluster'. So, connecting to any compute nodes, impala will work. Here is the process to connect to Impala-shell."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,10,"[tst867@hlog-1 ~]$ impala-shellStarting Impala Shell without Kerberos authenticationError connecting: TTransportException, Could not connect to hlog-1.hpc.nyu.edu:21000***********************************************************************************Welcome to the Impala shell.(Impala Shell v3.2.0-cdh6.3.4 (5fe4723) built on Wed Oct 21 08:51:34 PDT 2020)The HISTORY command lists all shell commands in chronological order.***********************************************************************************[Not connected] > connect hc07.nyu.cluster;Connection lost, reconnecting...Error connecting: TTransportException, Could not connect to hlog-1.hpc.nyu.edu:21000Opened TCP connection to hc07.nyu.cluster:21000Connected to hc07.nyu.cluster:21000Server version: impalad version 3.2.0-cdh6.3.4 RELEASE (build 5fe4723ad8fe1c3aaecbeb32c7533048be2420cf)[hc07.nyu.cluster:21000] default>Zeppelin Apache Zeppelin is a web-based interactive computational environment that could use Apache Spark as a backend."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,11,"In some sense it is like the IPython Notebook. Zeppelin is installed on Peel. Peel is a YARN cluster, not a standalone Spark cluster. Here is the process to work with Zeppelin on Peel. # Create personal directories and copy the configuration over$ mkdir -p $HOME/zeppelin/{conf,logs,notebook,run,webapps} $ cp /share/apps/peel/zeppelin/0.9.0/conf/* $HOME/zeppelin/conf/See [users] section in $HOME/zeppelin/conf/shiro.ini for the default user/password and the instruction to change password. The procedures to start and stop a Zeppelin server on Peel login nodes are: # Start a Zeppelin daemon$ /share/apps/peel/zeppelin/0.9.0/bin/zeppelin-daemon.sh --config $HOME/zeppelin/conf startZeppelin start at port 9178 [ OK ]# please remember to clean up when you are done, not to leave outdated processes hanging around # Stop the daemon$ /share/apps/peel/zeppelin/0.9.0/bin/zeppelin-daemon.sh --config $HOME/zeppelin/conf stopZeppelin stop [ OK ]There are two Peel login nodes."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,12,"Verify which login node the Zeppelin server is just started by using command 'hostname' at command line. Suppose that is 'hlog-1.hpc.nyu.edu', and NYU VPN is established. Open a new terminal (or PuTTY session) on your computer, and enable SSH port forwarding by running command ""ssh -L 4321:localhost:9178 <net_id>@216.165.13.149"", or equivalent in PuTTY. Note that 216.165.13.149 is the external IP address for the node 'hlog-1'. The node 'hlog-2' external IP address is 216.165.13.150. Also note that the Zeppelin port is randomized, it could be different next time you start a new server. The number 9178 is generated as shown in the example above. Now it is time to open a new web browser tab, input the following address to get the Zeppelin UI: File Permissions and Access Control Lists Users can share files with others using ACL's. An access control list (or ACL) gives per-file, per-directory and per-user control over who have permissions on the files."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,13,"You can see the ACL for a file or directory with the getfacl command: $ hdfs dfs -getfacl /user/<net_id>/testdir# To modify permissions for files or directories, use setfacl: $ hdfs dfs -setfacl -R -m user:<net_id>:rwx /user/<net_id>/testdir $ hdfs dfs -setfacl -R -m default:user:<net_id>:rwx /user/<net_id>/testdir To open subdirectory permission to others, you need to open each higher level directory's navigation permission too: $ hdfs dfs -setfacl -m user:<net_id>:--x /user/<net_id>Hadoop File System (HDFS) HDFS stands for Hadoop Distributed File System. HDFS is a highly fault-tolerant file system and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. TO UPLOAD DATA TO HDFS HDFS stands for Hadoop Distributed File System. HDFS is a highly fault-tolerant file system and is designed to be deployed on low-cost hardware."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,14,"HDFS provides high throughput access to application data and is suitable for applications that have large data sets. To Upload Data to HDFS hadoop fs -put <filename_in_lfs> <hdfs_name>To Get Data from HDFS hadoop fs -get <hdfs_name> <filename_in_lfs>Peel Account Access All HPC users should have an account on Peel. If you are enrolled in a class using the clusters, you may already have an account, try logging in first to check. See the HPC 'Getting and Renewing Accounts' page for instructions on how to get an account. (NYU VPN is required). Your home directory on Peel is the same as that on the HPC Greene cluster. To see which cluster you are on, on the command prompt run 'echo $CLUSTER'. Transferring Local Files To replicate files from your computer to a Peel HDFS directory, first use 'scp' to copy from your computer to $HOME or $SCRATCH, then run 'hdfs dfs -put' to move them to the HDFS directory."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_hadoop-user-guide.html,15,"my_laptop$ scp inetutils-1.9.4.tar.gz <net_id>@peel.hpc.nyu.edu:~/peel$ hdfs dfs -put inetutils-1.9.4.tar.gz /user/<net_id> Transferring Big Data Files Use Globus to transfer the data to /scratch first. The same /scratch folder is also available on Peel login nodes. Then copy the data from /scratch to your HDFS directory in Peel. /home is shared too between multiple clusters i.e., Greene and Peel. See instructions Data Transfers page to transfer files to and from NYU HPC storage."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_singularity-run-custom-applications-with-containers.html,0,"Singularity: Run Custom Applications with Containers What is Singularity Singularity is a container based Linux kernel workspace that works just like docker. You can run pre-built programs in containers without having to worry about the pre-install environment. For users who are familiar with Docker containers, Singularity works very similarly, and can even run Docker containers. For a detailed introduction on Singularity, visit their official site here why do we use Singularity There are multiple reasons to use Singularity on the HPC clusters: Security: Singularity provides a layer of security as it does not require any root access on our clusters. This makes it safer against malware and bad scripts that might jeopardize the outer system."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_singularity-run-custom-applications-with-containers.html,1,"Thus we only support Singularity on our clusters(there are not other options such as Kubernetes or Docker on our clusters right now) Containerization: Singularity will run all your images(packaged and pre-built programs) inside of its containers, each container works like a small vm. They contain all the required environment and files of a single Linux kernel and you don't have to worry about any pre-installation nonsense Inter-connectivity: Containers are able to talk to each other, as well as the home system, so while each container has its own small space, they are still a part of a big interconnected structure. Thus enabling you to connect your programs. Accessibility: Probably the most important feature of all, Singularity allows you to run your program in 2 to 3 simple steps, as shown in the topic how to run a singularity container."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_singularity-run-custom-applications-with-containers.html,2,"how to run a singularity container There are 3 steps to run a Singularity container on our clusters: pulling a image from Singularity hub or Docker hub $ singularity pull <image name># image name can be for example shub://vsoch/hello-world or docker://godlovedc/lolcowbuild the image $ singularity build <a name of your choosing>.simg:rw <image name># the image name can be a local image or an image from a hubWe add the :rw tag at the end of the .simg to explicitly give it ""read and write"" permissions while building.You can now run your container using the built image: run container # this is one way of running a container$ singularity run <image name>.simg:ro# this is another way to run a container$ ./<image name>.simg:roUnlike in the build phase, we add the :ro tag which means ""read only"" - as we are now just executing the image, not building it, and thus do not need it to be written."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_singularity-run-custom-applications-with-containers.html,3,"Writing access causes the Singularity image to be locked and it can become inaccessible while it is in read/write mode, so read only mode is best for executing commands. running this would yield a menu for output: go into container singularity shell <image name>.simg:ro# after this step, you will be going into the container and start your programmingyou can run commands for the container using exec arguments without actually going into the container Example: That's it! Now you're good to go and can just use these simple steps to run singularity images and run your programs For full information and documentation on Singularity, visit their site here How to Create a Singularity Container So what if you want to create an image from your container and save it for a rainy day? The instructions are here for your convenience, read through them to create your own Singularity container and package it into an image!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_singularity-run-custom-applications-with-containers.html,4,"For those that know how docker containers are built, you can build docker containers using the information here and upload them onto docker hub and pulling them using Singularity. Singularity supports all docker images! Singularity vs Docker Why are there so many mentions of Docker? The reason is that Singularity is essentially the same as Docker and you don't need to relearn Singularity if you already have experience with Docker. Now let's get into some pros and cons between the two programs. 1. Docker is more accepted commercially than Singularity. You can download and run Docker on your own computer with any operating system and build containers with ease while Singularity is used in a more academic setting. Singularity only supports Linux operating systems and cannot run on a windows linux kernel(your windows ubuntu), so it is much more limited. 2."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_singularity-run-custom-applications-with-containers.html,5,"However, Docker requires root or admin access for the operating system it deploys on, and our clusters do not offer that access to any software that requires this criteria. Thus Docker is not available on the clusters and Singularity is. 3. A silver lining in all of this is that Singularity fully supports docker images and you can do everything in docker and push your image to docker hub and pull them on the clusters. Thus making sure that you don't need to relearn Singularity all over again and can just use it through the simplest of commands in this wiki. Good luck with Singularity, and have fun!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,0,"SLURM: Main Commands Slurm Commands Submit jobs - [sbatch] Batch job submission can be accomplished with the command sbatch. Like in Torque qsub, we create a bash script to describe our job requirement: what resources we need, what softwares and processing we want to run, memory and CPU requested, and where to send job standard output and error etc. After a job is submitted, Slurm will find the suitable resources, schedule and drive the job execution, and report outcome back to the user. The user can then return to look at the output files. Example-1: In the first example, we create a small bash script, run it locally, then submit it as a job to Slurm using sbatch, and compare the results. $ mkdir -p /scratch/$USER/mytest1$ cd /scratch/$USER/mytest1 $ cat > simple1.sh#!/bin/bashhostnamedatesleep 20date $ chmod +x simple1.sh # This is just for demo purpose."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,1,Real work should be submitted # to Slurm to run on computing nodes.$ ./simple1.shlog-1Mon Feb 6 15:34:52 EST 2017Mon Feb 6 15:35:12 EST 2017 $ sbatch simple1.shSubmitted batch job 22140$ cat slurm-22140.outc17-01Mon Feb 6 15:35:21 EST 2017Mon Feb 6 15:35:41 EST 2017Example-2: Follow the recipe below to submit a job. The job can be used later as an example for practicing how to check job status. In my test its running duration is about 7 minutes.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,2,"$ cd /scratch/$USER/mytest1$ cp /share/apps/Tutorials/slurm/example/run-matlab.s .$ cp /share/apps/Tutorials/slurm/example/thtest.m .$ sbatch run-matlab.sSubmitted batch job 11615Below is the content of the bash script ""run-matlab.s"" just used in the job submission: #!/bin/bash###SBATCH --nodes=1#SBATCH --nodes=1#SBATCH --ntasks-per-node=1#SBATCH --cpus-per-task=2#SBATCH --time=1:00:00#SBATCH --mem=4GB#SBATCH --job-name=myMatlabTest#SBATCH --mail-type=END##SBATCH --mail-user=bob.smith@nyu.edu#SBATCH --output=slurm_%j.outmodule purgemodule load matlab/2020bcd /scratch/$USER/mytest1cat thtest.m | srun matlab -nodisplayThe job has been submitted successfully. And as the example box showing, its job ID is 11615. Usually we should let the scheduler to decide on what nodes to run jobs. In cases there is a need to request a specific set of nodes, use the directive nodelist, e.g. '#SBATCH --nodelist=c09-01,c09-02'."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,3,"Check cluster status - [sinfo] The sinfo command gives information about the cluster status, by default listing all the partitions. Partitions group computing nodes into logical sets, which serves various functionalities such as interactivity, visualization and batch processing. A partition is a group of nodes. A partition can be made up of nodes with a specific feature/functionality, such as nodes equipped with GPU accelerators (gpu partition). A partition can have specific parameters, such as how long jobs can run. So partitions can be thought as ""queues"" in other batch systems. Partitions may overlap."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,4,"$ sinfoPARTITION AVAIL TIMELIMIT NODES STATE NODELISTc01_25* up 1-00:00:00 4 mix c13-[01-04]c01_25* up 1-00:00:00 95 idle c01-[01-04],c02-[01-04],c03-[01-04],c04-[01-04],c05-[01-04],c06-[01-04],c07-[01-04],c08-[01-04],c09-[01-04],c10-[01-04],c11-[01-04],c12-[01-04],c14-[01-04],c15-[01-04],c16-[01-04],c17-[01-03],c18-[01-04],c19-[01-04],c20-[01-04],c21-[01-04],c22-[01-04],c23-[01-04],c24-[01-04],c25-[01-04]c26 up 1-00:00:00 16 idle c26-[01-16]c27 up 1-00:00:00 16 idle c27-[01-16]gpu up 1-00:00:00 2 mix gpu-[01-02]gpu up 1-00:00:00 7 idle gpu-[03-09]sinfo by default prints information aggregated by partition and node state. As shown above, there are four partitions namely c01_25, c26, c27 and gpu. The partition marked with an asterisk is the default one. Except there are two lines with the node state 'mix', which means some CPU cores occupied, all other nodes are idle. See two useful sinfo command examples: 1. the first one lists those nodes in idle state in the gpu partition; 2."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,5,the second outputs information in a node-oriented format.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,6,"$ sinfo -p cpu_gpu -t idlePARTITION AVAIL TIMELIMIT NODES STATE NODELISTgpu up 1-00:00:00 5 idle gpu-[05-09] $ sinfo -lNeMon Jan 16 15:05:49 2017NODELIST NODES PARTITION STATE CPUS S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON c01-01 1 c01_25* idle 28 2:14:1 128826 61889 1 (null) none c01-02 1 c01_25* idle 28 2:14:1 128826 61889 1 (null) none c01-03 1 c01_25* idle 28 2:14:1 128826 61889 1 (null) none c01-04 1 c01_25* idle 28 2:14:1 128826 61889 1 (null) none c02-01 1 c01_25* idle 28 2:14:1 128826 61889 1 (null) none c02-02 1 c01_25* idle 28 2:14:1 128826 61889 1 (null) none c02-03 1 c01_25* idle 28 2:14:1 128826 61889 1 (null) none c02-04 1 c01_25* idle 28 2:14:1 128826 61889 1 (null) none c03-01 1 c01_25* idle 28 2:14:1 128826 61889 1 (null) none c03-02 1 c01_25* idle 28 2:14:1 128826 61889 1 (null) none......Check job status - [squeue, sstat, sacct] The squeue command lists jobs which are in a state of either running, or waiting or completing etc."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,7,"It can also display jobs owned by a specific user or with specific job ID. Run 'man sinfo' or 'man squeue' to see the explanations for the results. With the job ID in hand, we can track the job status through its lifetime. The job first appears in the Slurm queue in the PENDING state. Then when its required resources become available, the job gets priority for its turn to run, and is allocated resources, the job will transit to the RUNNING state. Once the job runs to the end and completes successfully, it goes to the COMPLETED state; otherwise it would be in the FAILED state. Use squeue -j <jobID> to check a job status. Most of the columns in the output of the squeue command are self-explanatory."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,8,"The column ""ST"" in the middle is the job status, which can be : PD - pending: waiting for resource allocation S - suspended R - running F - failed: non-zero exit code or other failures CD - completed: all processes terminated with zero exit code CG - completing: in the completing process, some processes may still be alive The column ""NODELIST(REASON)"" in the end is job status due to the reason(s), which can be : JobHeldUser: (obviously) Priority: higher priority jobs exist Resources: waiting for resources to become available BeginTime: start time not reached yet Dependency: wait for a depended job to finish QOSMaxCpuPerUserLimit: number of CPU core limit reached You may select what columns to display, in a width specified with an integer number between %. and a letter, %.10M."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,9,"$ squeue -j 9874 -o ""%.18i %.9P %.8j %.8u %.8T %.10M %.9l %.6D %R %m"" JOBID PARTITION NAME USER STATE TIME TIME_LIMI NODES NODELIST(REASON) MIN_MEMORY 9874 c01_25 model_ev johd RUNNING 23:31 1:00:00 4 c13-[01-04] 2000MRun the command sstat to display various information of running job/step. Run the command sacct to check accounting information of jobs and job steps in the Slurm log or database. There is a '–-helpformat' option in these two commands to help checking what output columns are available."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,10,"$ sstat -j 23221 -o JobID,NodeList,Pids,MaxRSS,AveRSS,MaxVMSize JobID Nodelist Pids MaxRSS AveRSS MaxVMSize------------ -------------------- -------------------- ---------- ---------- ----------23221.0 c03-04 158503 3462088K 2681124K 8357328K $ sacct -j 7050 --format JobID,jobname,NTasks,nodelist,MaxRSS,MaxVMSize,AveRSS,AveVMSize JobID JobName NTasks NodeList MaxRSS MaxVMSize AveRSS AveVMSize------------ ---------- -------- --------------- ---------- ---------- ---------- ----------7050 mpiexec-t+ c17-[01-03] 7050.batch batch 1 c17-01 149112K 208648K 149112K 113260K7050.extern extern 3 c17-[01-03] 0 4316K 0 4316K7050.0 orted 2 c17-[02-03] 141016K 370880K 140024K 370868KType ""man <command>"" to look up detailed usage on the manual pages of command squeue, sstat and sacct. Cancel a job - [scancel] Things can go wrong, or in a way unexpected. Should you decide to terminate a job before it finishes, scancel is the tool to help."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,11,"Slurm Job Results Job results includes the job execution logs (standard output and error), and of course the output data files if any defined when submitting the job. Log files should be created in the working directory, and output data files in your specified directory. Examine log files with a text viewer or editor, to gain a rough idea of how the execution goes. Open output data files to see exactly what result is generated. Run sacct command to see resource usage statistics. Should you decide that the job needs to be rerun, submit it again with sbatch with a modified version of batch script and/or updated execution configuration. Iteration is one characteristic of a typical data analysis! SLURM Environment Variables To get the list of SLURM_* variables, you may run a job to check, e.g. srun sh -c 'env | grep SLURM | sort' . The command 'man sbatch' explains what these variables stand for."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-main-commands.html,12,"Below are a few frequently used: SLURM_JOB_ID - the job ID SLURM_SUBMIT_DIR - the job submission directory SLURM_SUBMIT_HOST - name of the host from which the job was submitted SLURM_JOB_NODELIST - names of nodes allocated to the job SLURM_ARRAY_TASK_ID - job array job index SLURM_JOB_CPUS_PER_NODE - CPU cores on this node allocated to the job SLURM_NNODES - number of nodes allocated to the job Exceeded Step Memory Limit Warning Message The current Slurm implementation utilizes Linux Control Groups (cgroups) for resource containment. If necessary please see this page at kernel.org for a detailed description of cgroups. If you get the correct outputs, please just ignore this warning message - ""slurmstepd: error: Exceeded job memory limit at some point"". You can also check job exit state to confirm. For reference there is some explanation in the bug report."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,0,"SLURM: Submitting Jobs Batch vs Interactive Jobs HPC workloads are usually better suited to batch processing than interactive working. A batch job is sent to the system (submitted) with sbatch. The working pattern we are all familiar with is interactive - I type (or click) something, and the computer performs the associated action. Then I type (or click) the next thing. Comments at the start of the script, which match a special pattern (#SBATCH) are read as Slurm options The trouble with interactive environments There is another reason why GUIs are less common in HPC environments: point-and-click is necessarily interactive. In HPC environments (as we'll see in session 3) work is scheduled in order to allow exclusive use of the shared resources. On a busy system there may be several hours wait between when you submit a job and when the resources become available, so a reliance on user interaction is not viable."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,1,"In Unix, commands need not be run interactively at the prompt, you can write a sequence of commands into a file to be run as a script, either manually (for sequences you find yourself repeating frequently) or by another program such as the batch system. The job might not start immediately, and might take hours or days, so we prefer a batch approach: plan the sequence of commands which will perform the actions we need write them into a script I can now run the script interactively, which is a great way to save effort if I frequently use the same workflow, or ... submit the script to a batch system, to run on dedicated resources when they become available Where does the output go?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,2,"The batch system writes stdout and stderr from a job to a file named ""slurm-12345.out"" You can change either stdout or stderr using sbatch options While a job is running, it caches the stdout and stderr in the job working directory You can use redirection to send output of a specific command into a file Writing and Submitting a job There are two aspects to a batch job script: A set of SBATCH directives describing the resources required and other information about the job The script itself, comprised of commands to set up and perform the computations without additional user interaction A simple example A typical batch script on an NYU HPC cluster looks something like these: module purgemodule load stata/17.0RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*}mkdir -p $RUNDIR DATADIR=$SCRATCH/my_project/datacd $RUNDIRstata -b do $DATADIR/data_0706.do #!/bin/bash#SBATCH --nodes=1#SBATCH --ntasks-per-node=1#SBATCH --cpus-per-task=1#SBATCH --time=5:00:00#SBATCH --mem=2GB#SBATCH --job-name=myTest#SBATCH --mail-type=END#SBATCH --mail-user=bob.smith@nyu.edu#SBATCH --output=slurm_%j.out#SBATCH --error=slurm_%j.err module purge SRCDIR=$HOME/my_project/codeRUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*}mkdir -p $RUNDIR cd $SLURM_SUBMIT_DIRcp my_input_params.inp $RUNDIR cd $RUNDIRmodule load fftw/intel/3.3.9$SRCDIR/my_exec.exe < my_input_params.inp We'll work through them more closely in a moment."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,3,"You submit the job with sbatch: $ sbatch myscript.sAnd monitor its progress with: $ squeue -u $USERWhat just happened? Here's an annotated version of the first script: #!/bin/bash# This line tells the shell how to execute this script, and is unrelated# to SLURM. # at the beginning of the script, lines beginning with ""#SBATCH"" are read by# SLURM and used to set queueing options. You can comment out a SBATCH# directive with a second leading #, eg:##SBATCH --nodes=1 # we need 1 node, will launch a maximum of one task and use one cpu for the task: #SBATCH --nodes=1#SBATCH --ntasks-per-node=1#SBATCH --cpus-per-task=1 # we expect the job to finish within 5 hours. If it takes longer than 5# hours, SLURM can kill it:#SBATCH --time=5:00:00 # we expect the job to use no more than 2GB of memory:#SBATCH --mem=2GB # we want the job to be named ""myTest"" rather than something generated# from the script name."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,4,"This will affect the name of the job as reported# by squeue:#SBATCH --job-name=myTest # when the job ends, send me an email at this email address.#SBATCH --mail-type=END#SBATCH --mail-user=bob.smith@nyu.edu # both standard output and standard error are directed to the same file.# It will be placed in the directory I submitted the job from and will# have a name like slurm_12345.out#SBATCH --output=slurm_%j.out # once the first non-comment, non-SBATCH-directive line is encountered, SLURM# stops looking for SBATCH directives. The remainder of the script is executed# as a normal Unix shell script # first we ensure a clean running environment:module purge# and load the module for the software we are using:module load stata/17.0 # next we create a unique directory to run this job in."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,5,"We will record its# name in the shell variable ""RUNDIR"", for better readability.# SLURM sets SLURM_JOB_ID to the job id, ${SLURM_JOB_ID/.*} expands to the job# id up to the first '.' We make the run directory in our area under $SCRATCH, because at NYU HPC# $SCRATCH is configured for the disk space and speed required by HPC jobs.RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*}mkdir $RUNDIR # we will be reading data in from somewhere, so define that too:DATADIR=$SCRATCH/my_project/data # the script will have started running in $HOME, so we need to move into the# unique directory we just createdcd $RUNDIR # now start the Stata job:stata -b do $DATADIR/data_0706.doThe second script has the same SBATCH directives, but this time we are using code we compiled ourselves."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,6,"Starting after the SBATCH directives:# first we ensure a clean running environment:module purge # and ensure we can find the executable:SRCDIR=$HOME/my_project/code # create a unique directory to run this job in, as per the script aboveRUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*}mkdir $RUNDIR # By default the script will have started running in the directory we ran sbatch from.# Let's assume our input file is in the same directory in this example. SLURM# sets some environment variables with information about the job, including# SLURM_SUBMIT_DIR which is the directory the job was submitted from."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,7,"So lets# go there and copy the input file to the run directory on /scratch:cd $SLURM_SUBMIT_DIRcp my_input_params.inp $RUNDIR # go to the run directory to begin the run:cd $RUNDIR # load whatever environment modules the executable needs:module load fftw/intel/3.3.9 # run the executable (sending the contents of my_input_params.inp to stdin)$SRCDIR/my_exec.exe < my_input_params.inpBatch Jobs Jobs are submitted with the sbatch command: $ sbatch options job-scriptThe options tell SLURM information about the job, such as what resources will be needed. These can be specified in the job-script as SBATCH directives, or on the command line as options, or both (in which case the command line options take precedence should the two contradict each other)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,8,"For each option there is a corresponding SBATCH directive with the syntax: #SBATCH optionFor example, you can specify that a job needs 2 nodes and 4 cores on each node (by default one CPU core per task) on each node by adding to the script the directive: #!/bin/bash#SBATCH --nodes=2#SBATCH --ntasks-per-node=4or as a command-line option to sbatch when you submit the job: $ sbatch --nodes=2 --ntasks-per-node=4 my_script.sOptions to manage job output -J jobname Give the job a name. The default is the filename of the job script. Within the job, $SLURM_JOB_NAME expands to the job name -o path/for/stdout Send stdout to path/for/stdout. The default filename is slurm-${SLURM_JOB_ID}.out, e.g. slurm-12345.out, in the directory from which the job was submitted -e path/for/stderr Send stderr to path/for/stderr. --mail-user=my_email_address@nyu.edu Send email to my_email_address@nyu.edu when certain events occur. --mail-type=type Valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL..."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,9,"Options to set the job environment --export=VAR1,VAR2=""some value"",VAR3 Pass variables to the job, either with a specific value (the VAR= form) or from the submitting environment (without ""="") --get-user-env[=timeout][mode] Run something like ""su - <username> -c /usr/bin/env"" and parse the output. Default timeout is 8 seconds. The mode value can be ""S"", or ""L"" in which case ""su"" is executed with ""-"" option Options to request compute resources -t, --time=time Set a limit on the total run time. Acceptable formats include ""minutes"", ""minutes:seconds"", ""hours:minutes:seconds"", ""days-hours"", ""days-hours:minutes"" and ""days-hours:minutes:seconds"" --mem=MB Maximum memory per node the job will need in MegaBytes --mem-per-cpu=MB Memory required per allocated CPU in MegaBytes -N, --nodes=num Number of nodes are required. Default is 1 node -n, --ntasks=num Maximum number tasks will be launched."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,10,"Default is one task per node --ntasks-per-node=ntasks Request that ntasks be invoked on each node -c, --cpus-per-task=ncpus Require ncpus number of CPU cores per task. Without this option, allocate one core per task Requesting the resources you need, as accurately as possible, allows your job to be started at the earliest opportunity as well as helping the system to schedule work efficiently to everyone's benefit. Options for running interactively on the compute nodes with srun -nnum Specify the number of tasks to run, e.g. -n4. Default is one CPU core per task. Don't just submit the job, but also wait for it to start and connect stdout, stderr and stdin to the current terminal -ttime Request job running duration, e.g. -t1:30:00 --mem=MB Specify the real memory required per node in MegaBytes, e.g. --mem=4000 --pty Execute the first task in pseudo terminal mode, e.g."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,11,"--pty /bin/bash, to start a bash command shell --x11 Enable X forwarding, so programs using a GUI can be used during the session (provided you have X forwarding to your workstation set up) To leave an interactive batch session, type exit at the command prompt. Options for delaying starting a job --begin=time Delay starting this job until after the specified date and time, e.g. --begin=9:42:00, to start the job at 9:42:00 am."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,12,"-d, --dependency=dependency_list (More info here https://slurm.schedmd.com/sbatch.html) Example 1 --dependency=afterok:12345, to delay starting this job until the job 12345 has completed successfully Example 2 Let say job 1 uses sbatch file job1.sh, and job 2 uses job2.sh Inside the batch file of the second job (job2.sh) add #SBATCH --dependency=afterok:$jid1 # start the first job and get id of the job jid1=$(echo $(sbatch job1.sh) | grep -Eo ""[0-9]+"") # schedule second jobs to start when the first one ends sbatch job2.sh Options for running many similar jobs -a, --array=indexes Submit an array of jobs with array ids as specified. Array ids can be specified as a numerical range, a comma-separated list of numbers, or as some combination of the two. Each job instance will have an environment variable SLURM_ARRAY_JOB_ID and SLURM_ARRAY_TASK_ID."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,13,"For example: --array=1-11, to start an array job with index from 1 to 11 --array=1-7:2, to submit an array job with index step size 2 --array=1-9%4, to submit an array job with simultaneously running job elements set to 4 The srun command is similar to pbsdsh. It launches tasks on allocated resources. R Job Example Create a directory and an example R script mkdir /scratch/$USER/examplescd /scratch/$USER/examplesCreate example.R inside the examples directory: df <- data.frame(x=c(1,2,3,1), y=c(7,19,2,2))dfindices <- order(df$x)order(df$x)df[indices,]df[rev(order(df$y)),]Create the following SBATCH script: $ cat run-R.sbatch#!/bin/bash##SBATCH --job-name=RTest#SBATCH --nodes=1#SBATCH --tasks-per-node=1#SBATCH --mem=2GB#SBATCH --time=01:00:00 module purgemodule load r/intel/4.0.4 cd /scratch/$USER/examplesR --no-save -q -f example.R > example.out 2>&1Run the job using ""sbatch""."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,14,"$ sbatch run-R.sbatchArray Jobs Using job array you may submit many similar jobs with almost identical job requirement. This reduces loads on both shoulders of users and the scheduler system. Job array can only be used in batch jobs. Usually the only requirement difference among jobs in a job array is the input file or files. Please follow the recipe below to try the example. There are 5 input files named 'sample-1.txt', 'sample-2.txt' to 'sample-5.txt' in sequential order. Running one command ""sbatch --array=1-5 run-jobarray.s"", you submit 5 jobs to process each of these input files individually."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,15,"$ mkdir -p /scratch/$USER/myjarraytest$ cd /scratch/$USER/myjarraytest$ cp /share/apps/Tutorials/slurm/example/jobarray/* .$ lsrun-jobarray.s sample-1.txt sample-2.txt sample-3.txt sample-4.txt sample-5.txt wordcount.py$ sbatch --array=1-5 run-jobarray.sSubmitted batch job 23240The content of the job script 'run-jobarray.s' is copied below: #!/bin/bash##SBATCH --job-name=myJobarrayTest#SBATCH --nodes=1 #SBATCH --ntasks-per-node=1#SBATCH --time=5:00#SBATCH --mem=1GB#SBATCH --output=wordcounts_%A_%a.out#SBATCH --error=wordcounts_%A_%a.errmodule purgemodule load python/intel/3.8.6 cd /scratch/$USER/myjarraytestpython wordcount.py sample-$SLURM_ARRAY_TASK_ID.txt Job array submission induces an environment variable SLURM_ARRAY_TASK_ID, which is unique for each job array job. It is usually embedded somewhere so that at a job running time its unique value is incorporated into producing a proper file name."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,16,"Also as shown above: two additional options %A and %a, denoting the job ID and the task ID (i.e. job array index) respectively, are available for specifying a job's stdout, and stderr file names. More examples You can find more examples here /scratch/work/public/examples/slurm/jobarry/ GPU Jobs To request one GPU card, use SBATCH directives in job script: #SBATCH --gres=gpu:1To request a specific card type, use e. g. --gres=gpu:v100:1. As an example, let's submit an Amber job. Amber is a molecular dynamics software package. The recipe is: $ mkdir -p /scratch/$USER/myambertest$ cd /scratch/$USER/myambertest$ cp /share/apps/Tutorials/slurm/example/amberGPU/* .$ sbatch run-amber.sSubmitted batch job 14257There are three NVIDIA GPU types and one AMD GPU type that can be used (CAUTION: AMD GPUs require code to be compatible with ROCM drivers, not CUDA)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,17,"To Request NVIDIA GPUs RTX8000 V100 A100 To Request AMD GPUs #SBATCH --gres=gpu:mi50:1From the tutorial example directory we copy over Amber input data files ""inpcrd"", ""prmtop"" and ""mdin"", and the job script file ""run-amber.s"". The content of the job script ""run-amber.s"" is: #!/bin/bash##SBATCH --job-name=myAmberJobGPU#SBATCH --nodes=1#SBATCH --cpus-per-task=1#SBATCH --time=00:30:00#SBATCH --mem=3GB#SBATCH --gres=gpu:1module purgemodule load amber/openmpi/intel/20.06cd /scratch/$USER/myambertestpmemd.cuda -OThe demo Amber job should take ~2 minutes to finish once it starts running. When the job is done, several output files are generated. Check the one named ""mdout"", which has a section most relevant here: |--------------------- INFORMATION ---------------------- | GPU (CUDA) Version of PMEMD in use: NVIDIA GPU IN USE."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,18,"| Version 16.0.0 | | 02/25/2016 [......]|------------------- GPU DEVICE INFO -------------------- | | CUDA_VISIBLE_DEVICES: 0 | CUDA Capable Devices Detected: 1 | CUDA Device ID in use: 0 | CUDA Device Name: Tesla K80 | CUDA Device Global Mem Size: 11439 MB | CUDA Device Num Multiprocessors: 13 | CUDA Device Core Freq: 0.82 GHz | |-------------------------------------------------------- Interactive Jobs Bash Sessions The majority of the jobs on the NYU HPC cluster are submitted with the sbatch command, and executed in the background. These jobs' steps and workflows are predefined by users, and their executions are driven by the scheduler system. There are cases when users need to run applications interactively (interactive jobs). Interactive jobs allow the users to enter commands and data on the command line (or in a graphical interface), providing an experience similar to working on a desktop or laptop."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,19,"Examples of common interactive tasks are: Editing files Compiling and debugging code Exploring data, to obtain a rough idea of characteristics on the topic Getting graphical windows to run visualization Running software tools in interactive sessions Debugging many uses of Matlab etc. To support interactive use in a batch environment, Slurm allows for interactive batch jobs. Can you run interactive jobs on the HPC Login nodes? Since the login nodes of the HPC cluster are shared between many users, running interactive jobs that require significant computing and IO resources on the login nodes will impact many users. Thus running compute and IO intensive interactive jobs on the HPC login nodes is not allowed. Such jobs may be removed without notice! Instead of running interactive jobs on Login nodes, users can run interactive jobs on the HPC Compute nodes using SLURM's srun utility."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,20,"Running interactive jobs on compute nodes does not impact many users and in addition provides access to resources that are not available on the login nodes, such as interactive access to GPUs, high memory, exclusive access to all the resources of a compute node, etc. Note: There is no partition on the HPC cluster that has been reserved for Interactive jobs. Start an Interactive Job When you start an interactive batch job the command prompt is not immediately returned. Instead, you wait until the resource is available when the prompt is returned and you are on a compute node and in a batch job - much like the process of logging in to a host with ssh. To end the session, type 'exit', again just like the process of logging in and out with ssh. [wd35@log-0 ~]$ srun --pty /bin/bash[wd35@c17-01 ~]$ hostnamec17-01To use any GUI-based program within the interactive batch session you will need to extend X forwarding with the --x11 option."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,21,"This of course still relies on you having X forwarding at your login session - try running 'xterm' before starting the interactive to verify that this is working correctly. Request Resources You can request resources for an interactive batch session just as you would for any other job, for example to request 4 processors with 4GB memory for 2 hours: If you do not request resources you will get the default settings. If after some directory navigation in your interactive session, you can jump back to the directory you submitted from with: $ cd $SLURM_SUBMIT_DIR Interactive Job Options (Don't just submit the job, but also wait for it to start and connect stdout, stderr and stdin to the current terminal) -nnum Specify the number of tasks to run, e.g. -n4. Default is one CPU core per task. -ttime Request job running duration, e.g. -t1:30:00 --mem=MB Specify the real memory required per node in MegaBytes, e.g. --mem=4000 --pty Execute the first task in pseudo terminal mode, e.g."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,22,"--pty /bin/bash, to start a bash command shell -gres=gpu:N Specify <N> number of GPUs --x11 Enable X forwarding, so programs using a GUI can be used during the session (provided you have X forwarding to your workstation set up) To leave an interactive batch session, type exit at the command prompt. Certain tasks need user interaction - such as debugging and some GUI-based applications. However the HPC clusters rely on batch job scheduling to efficiently allocate resources. Interactive batch jobs allow these apparently conflicting requirements to be met. Interactive Bash Job Examples Example (Without x11 forwarding) Through srun SLURM provides rich command line options for users to request resources from the cluster, to allow interactive jobs. Please see some examples and short accompanying explanations in the code block below, which should cover many of the use cases."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,23,"# In the srun examples below, through ""--pty /bin/bash"" we request to start bash command shell session in pseudo terminal# # by default the resource allocated is single CPU core and 2GB memory for 1 hour$ srun --pty /bin/bash # To request 4 CPU cores, 4 GB memory, and 2 hour running duration$ srun -c4 -t2:00:00 --mem=4000 --pty /bin/bash # To request one GPU card, 3 GB memory, and 1.5 hour running duration$ srun -t1:30:00 --mem=3000 --gres=gpu:1 --pty /bin/bashExample (x11 forwarding) In srun there is an option ""–x11"", which enables X forwarding, so programs using a GUI can be used during an interactive session (provided you have X forwarding to your workstation set up)."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,24,"# To request computing resources, and export x11 display on allocated node(s)$ srun --x11 -c4 -t2:00:00 --mem=4000 --pty /bin/bash$ xterm # check if xterm popping up okay # To request GPU card etc, and export x11 display$ srun --x11 -t1:30:00 --mem=3000 --gres=gpu:1 --pty /bin/bash$ srun -n4 -t2:00:00 --mem=4000 --pty /bin/bash R interactive Job The following example shows how to work with Interactive R session on a compute node [NetID@log-1 ~]$ srun -c 1 --pty /bin/bash[NetID@c17-01 ~]$ module purge [NetID@c17-01 ~]$ module list No modules loaded [NetID@c17-01 ~]$ module load r/intel/4.0.4[NetID@c17-01 ~]$ module list Currently Loaded Modules: 1) intel/19.1.2 2) r/intel/4.0.4 [NetID@c17-01 ~]$ R R version 4.0.4 (2021-02-15) -- ""Lost Library Book""Copyright (C) 2021 The R Foundation for Statistical ComputingPlatform: x86_64-centos-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY.You are welcome to redistribute it under certain conditions.Type 'license()' or 'licence()' for distribution details."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_slurm-submitting-jobs.html,25,"Natural language support but running in an English locale R is a collaborative project with many contributors.Type 'contributors()' for more information and'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or'help.start()' for an HTML browser interface to help.Type 'q()' to quit R. > 5 + 10 [1] 15 > 6 ** 2 [1] 36 > tan(45) [1] 1.619775 > > q() Save workspace image? [y/n/c]: n [NetID@c17-01 ~]$ exit exit [NetID@log-1 ~]$"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_spark-interactive-standalone-cluster.html,0,"Running Spark in an Interactive Jupyter Notebook The Spark Standalone Cluster application allows you to bring up a Spark instance on Greene and interact with it live. Connect to the Open OnDemand Web Dashboard. In the dropdown menu in the header, select Interactive Apps > Spark Standalone Cluster. Set the cluster's parameters using the provided form, and then click Launch. You will be automatically redirected to an info card for the session, or you can find it again under My Interactive Sessions. Once the cluster has finished startup, you should see the following: The Host link will take you to a shell interface with the remote machine running your cluster. The Standalone Cluster Web UI link provides monitoring and usage information, as well as the cluster's SPARK_URL for directing batch jobs as outlined in Running Multi-Node Spark with Singularity."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_spark-interactive-standalone-cluster.html,1,"The Jupyter Notebook Environment link takes you to your home or scratch file directory on Greene, where you will be able to create or open Jupyter notebooks that run against your standalone cluster. For code examples, see Big Data Tutorial: Spark. Adding custom python libraries to Spark Standalone Cluster Build an overlay with your python libraries Adding python libraries to the Spark Standalone Cluster requires a custom overlay which is compatible with the existing Spark Standalone images. If you have built a Miniconda environment before, e.g. by following the PyTorch Example tutorial, this process should be familiar. However, instead of building installations from scratch, we will start by cloning an existing pyspark image, and install additional packages on top of it. Finally, before saving we will clean up files and packages in the overlay that might overwrite data in the Spark Standalone images. Create a working directory."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_spark-interactive-standalone-cluster.html,2,"mkdir /scratch/<NetID>/pysparkcd /scratch/<NetID>/pysparkCopy an appropriately sized overlay image. In this example we use overlay-15GB-500K.ext3.gz. It has 15GB free space inside and is able to hold 500K files cp -rp /scratch/work/public/overlay-fs-ext3/overlay-15GB-500K.ext3.gz .gunzip overlay-15GB-500K.ext3.gzmv overlay-15GB-500K.ext3 pyspark.ext3Launch a Singularity container to open the overlay in read/write mode. singularity exec --overlay pyspark.ext3:rw /scratch/work/public/singularity/ubuntu-20.04.3.sif /bin/bashNext, inside the container, download and install Miniconda to /ext3/miniconda3. wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.shbash Miniconda3-latest-Linux-x86_64.sh -b -p /ext3/miniconda3# rm Miniconda3-latest-Linux-x86_64.sh # if you don't need this file any longerCreate a wrapper script to activate your base conda environment."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_spark-interactive-standalone-cluster.html,3,"Open /ext3/env.sh and write the following: #!/bin/bashsource /ext3/miniconda3/etc/profile.d/conda.shexport PATH=/ext3/miniconda3/bin:$PATHexport PYTHONPATH=/ext3/miniconda3/bin:$PATH Activate the conda environment, update, and clean. Finally, exit singularity to continue the installation on a compute node. source /ext3/env.shconda update -n base conda -yconda clean --all --yes # Exit Singularityexit Clone a pyspark image and install additional packages First, start an interactive job with adequate compute and memory resources to install packages. This will allow you to install large packages which might otherwise crash on the login node with its 2GB memory limit. srun --cpus-per-task=2 --mem=10GB --time=04:00:00 --pty /bin/bashOnce a node is assigned, open your overlay in read/write mode and activate the base environment."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_spark-interactive-standalone-cluster.html,4,"singularity exec --overlay pyspark.ext3:rw /scratch/work/public/singularity/ubuntu-20.04.3.sif /bin/bashsource /ext3/env.sh Now, clone an existing pyspark environment to use as base. This command uses the file flag -f to create a new conda environment from an existing config file. The prefix flag -p creates the environment in its own top-level directory, /ext3/pyspark. Make sure you use the same pyspark version here that you intend to use when starting up the Spark Standalone Cluster. cp /scratch/work/public/apps/pyspark/<version>/pyspark.yml .conda env create -f pyspark.yml -p /ext3/pysparkActivate the new environment and install the libraries you need using conda or pip. conda activate /ext3/pyspark# Ex: install pydub to handle audio filespip install pydub To double check your installations, you can open an interactive python window within the container. Confirm that your custom libraries are installed with the correct version."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_spark-interactive-standalone-cluster.html,5,"python3>>> import pyspark>>> print(pyspark.__version__)3.3.0>>> exit()Once you are happy with your python environment, clean up files in /ext3 that would conflict with the default Spark Standalone images. This is necessary to make your overlay compatible with the Spark Standalone Cluster. rm /ext3/env.sh rm -R /ext3/miniconda3/As a final optional step, compress your overlay into a squashfs filesystem to save space. # Exit the ubuntu singularity instanceexit# Compress the overlaysingularity exec --overlay pyspark.ext3:ro /scratch/work/public/singularity/centos-8.2.2004.sif mksquashfs /ext3 pyspark.sqf -keep-as-directory -all-root Add the overlay when initializing Spark Standalone Cluster When launching a Spark Standalone Cluster from the Open OnDemand dashboard, include the path to your custom overlay at the bottom of the form. You should then be able to access your custom libraries from any Jupyter notebooks run against the cluster."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_spark-interactive-standalone-cluster.html,6,The same overlay can be re-used in as many cluster instances as needed.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_spark-multi-node-jobs-with-singularity.html,0,"Running Multi-Node Spark with Singularity on Greene Make a directory within your home directory for your analysis (mkdir ~/myanalysis). This directory will also be used by Spark to write config files / logs to. Navigate to that directory using Linux's cd command. Copy the example Spark submission script to your analysis directory: cp /scratch/work/public/apps/pyspark/3.1.2/examples/spark/cluster/run-spark.bash .Copy the example SLURM submission script to your analysis directory: cp /scratch/work/public/apps/pyspark/3.1.2/examples/spark/cluster/run-spark-singularity.sbatch .By default run-spark.bash looks like the following: #!/bin/bashsource /scratch/work/public/apps/pyspark/3.1.2/scripts/spark-setup-slurm.bash start_all spark-submit \ --master $SPARK_URL \ --executor-memory $MEMORY \ wordcount.py /scratch/work/public/apps/pyspark/3.1.2/examples/shakespeare-8G.txt stop_all First, a number of commands are imported that are used to turn a Spark standalone cluster on and off."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_spark-multi-node-jobs-with-singularity.html,1,"After these commands are imported, a Spark cluster is started, a Spark job is submitted, and then the Spark cluster is shut down after the Spark job finishes. In the Spark submission command, the SPARK_URL variable represents your Spark cluster’s master node. You will now need to edit this script so that the spark-submit command reflects your Spark application instead of the wordcount application (which is used as an example). Edit the copy of run-spark-singularity.sbatch within your directory so that the #SBATCH options reflect the resources that you wish to request. Note that currently only whole node allocations are supported, so the line starting with #SBATCH --nodes= must be present. Type sbatch run-spark-singularity.sbatch from within your analysis directory to start your Spark cluster / job."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_tunneling-and-x11-forwarding.html,0,"SSH Configuration and X11 Forwarding Avoiding Man in the Middle Warning If you see this warning: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that a host key has just been changed. Do not be alarmed - this is an issue that occurs because Greene has multiple login nodes (log-1, log-2, and log-3) that greene.hpc.nyu.edu resolves to. To avoid this warning, you can add these lines to your SSH configuration file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_tunneling-and-x11-forwarding.html,1,"Using your favorite editor, open the file ""~/.ssh/config"" and place the following lines in it: Host greene.hpc.nyu.edu dtn.hpc.nyu.edu gw.hpc.nyu.edu StrictHostKeyChecking no ServerAliveInterval 60 ForwardAgent yes StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERRORThe above will also fix SSH timeout errors by extending the ServerAliveInterval argument. SSH Tunneling (Mac, Linux) Setting up your workstation for SSH tunneling will make logging in and transferring files significantly easier, and installing and running an X server will allow you to use graphical software on the HPC clusters. X server is a software package that draws on your local screen windows created on a remote computer (such as an NYU HPC). Linux users have X set up already. Mac users can download and install XQuartz. 1. Set up a tunnel you can reuse To avoid repeatedly setting up a tunnel, you can write the details of the tunnel into your SSH configuration file."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_tunneling-and-x11-forwarding.html,2,"Using your favorite editor, open the file ""~/.ssh/config"" and place the following lines in it: # first we create the tunnel, with instructions to pass incoming# packets on ports 8027 and 8028 through it and to specific locationsHost hpcgwtunnel HostName gw.hpc.nyu.edu ForwardX11 no StrictHostKeyChecking no LocalForward 8027 greene.hpc.nyu.edu:22 UserKnownHostsFile /dev/null User <Your NetID> # next we create an alias for incoming packets on the port# The alias corresponds to where the tunnel forwards these packets Host greene HostName localhost Port 8027 ForwardX11 yes StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR User <Your NetID> Create this file/directory In case you don't have it. Make sure that "".ssh"" directory has correct permissions (it should be ""700"" or ""drwx------"")."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_tunneling-and-x11-forwarding.html,3,"If needed, set permissions with: chmod 700 ~/.sshYou may also need to setup permissions on your local computer chmod 700 $HOMEchmod 700 $HOME/.ssh## to be safe, all files inside ~/.ssh should be set 600chmod 600 ~/.ssh/* 2. Start the tunnel To create the tunnel, ssh to it with the following command: $ ssh hpcgwtunnelImportant: you must leave this window open for the tunnel to remain open. It is best to start a new terminal window for subsequent logins. 3. Log in via the tunnel Open a new terminal window and use ssh to log in to the cluster, as shown below. $ ssh greene # to get to Greene clusterNote that you must use the short name defined above in your .ssh/config file, not the fully qualified domain name: Creating a once-off tunnel."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_tunneling-and-x11-forwarding.html,4,"Alternatively, you can set up a once-off tunnel without editing .ssh/config by running the following command: $ ssh -L 8027:greene:22 NetID@gw.hpc.nyu.edu # to set up a tunnel$ ssh -Y -p 8027 NetID@localhost This is the equivalent to running ""ssh hpcgwtunnel"" in the reusable tunnel instructions, but the port forwarding is specified on the command line. Tunneling (Windows) Step 1: Creating the tunnel First open Putty and prepare to log in to gw.hpc.nyu.edu. If you saved your session during that process, you can load it by selecting from the ""Saved Sessions"" box and hitting ""Load"". Don't hit ""Open"" yet! Under ""Connection"" -> ""SSH"", just below ""X11"", select ""Tunnels 3. Write ""8026"" (the port number) in the ""Source port"" box, and ""greene.hpc.nyu.edu:22"" (the machine you wish to tunnel to - 22 is the port that ssh listens on) in the ""Destination"" box. 4. Click ""Add"". You can repeat step 3 with a different port number and a different destination."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_tunneling-and-x11-forwarding.html,5,"If you do this you will create multiple tunnels, one to each destination. 5. Before hitting ""Open"", go back to the ""Sessions"" page, give the session a name (""hpcgw_tunnel"") and hit ""Save"". Then next time you need not do all this again, just load the saved session. 6. Hit ""Open"" to login in to gw.hpc.nyu.edu and create the tunnel. A terminal window will appear, asking for your login name (NYU NetID) and password (NYU password). Windows may also ask you to allow certain connections through its firewall - this is so you can ssh to port 8026 on your workstation - the entrance to the tunnel. Note: You can add other NYU hosts to the tunnel by adding a new source port and destination and clicking ""Add"". For example, you could add ""Source port = 8025"" and ""Destination = EXAMPLE.hpc.nyu.edu:22"", then press ""Add"". You would then perform Step 2 (below) twice - once for greene on port 8026 and once for an example server on port 8025."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_tunneling-and-x11-forwarding.html,6,"Using your SSH tunnel: To log in via the tunnel, first the tunnel must be open. If you've just completed Step 1, it will be open and you can jump down to ""Step 2: Logging in via your SSH tunnel"". If you completed Step 1 yesterday, and now want to re-use the tunnel you created, first start the tunnel: Starting the tunnel: During a session, you need only do this once - as long as the tunnel is open, new connections will go over it. Start Putty.exe (again, if necessary), and load the session you saved in settings during procedure above Hit ""Open"", and log in to the bastion host with your NYU NetID and password. This will create the tunnel. Step 2: Logging in via your SSH tunnel Start the second Putty.exe. In the ""Host Name"" box, write ""localhost"" and in the ""Port"" box, write ""8026"" (or whichever port number you specified when you set up the tunnel in the procedure above). We use ""localhost"" because the entrance of the tunnel is actually on this workstation, at port 8026. 2."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_tunneling-and-x11-forwarding.html,7,"Go to ""Connections"" -> ""SSH"" -> ""X11"" and check ""Enable X11 forwarding"" 3. Optionally, give this session a name (in ""Saved Sessions"") and hit ""Save"" to save it. Then next time instead of steps 1 and 2 you can simply load this saved session. 4. Hit ""Open"". You will again get a terminal window asking for your login (NYU NetID) and password (NYU password). You are now logged in to the HPC cluster! X11 Forwarding In rare cases when you need to interact with GUI applications on HPC clusters, you need to enable X11 forwarding for your SSH connection. Mac and Linux users will need to run the ssh commands described above with an additional flag: ssh -Y <NYU_NetID>@greene.hpc.nyu.eduHowever, Mac users need to install XQuartz, since X-server is no longer shipped with the macOS (you can find it here). Windows users will also need to install X server software. We recommend two options out there. We recommend installing Xming."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_tunneling-and-x11-forwarding.html,8,"Start Xming application and configure PuTTY to support X11 forwarding: Stata X11 Forwarding Example Please make sure an X server is installed and running on your workstation before running a program like Stata on the HPC cluster. Establish connection Note: example given is for Stata, but the same will apply for other programs For Linux and Mac users If your workstation or laptop is inside the NYU-Net, you can login directly to the HPC cluster login node using SSH. Open up a terminal window and issue the following SSH command. Make sure to specify the -Y option with the SSH command. The same instructions apply if you are outside the NYU-Net, but you use VPN to access the NYU Network. $ ssh -Y <net_id>@greene.hpc.nyu.eduStart an interactive session with X11 forwarding enabled with the following command: $ srun --x11 --pty /bin/bashYou will be redirected to one of the compute nodes."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_tunneling-and-x11-forwarding.html,9,"Load ""stata"" module: $ module avail stata-------- /share/apps/modulefiles ------ stata/16.1 stata/17.0$ module load stata/17.0 Finally, run Stata with X forwarding and Multiprocessing: $ xstata-mpNote: Make sure you exit the node once you are done using Stata to free up the node. You can exit the node by typing the command $exit For Windows Users You will need Xming and Putty to run Stata on Windows. Step 1: Download Xming and PuTTY if you don't have them. You will require both. Step 2: Update the settings in PuTTY as shown in the image above. Step 3: Log into the HPC cluster with PuTTY Step 4: Make sure Xming is running. Step 5: Type in the following commands in your PuTTY session: $ srun --x11 --pty /bin/bashLoad the Stata module: $ module load stata/17.0Run Stata: $ xstata-mpThe Stata window should pop up as below: Note: Make sure you exit the node once you are done using Stata to free up the node. You can exit the node by typing the command $exit"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_vs-code.html,0,"VS Code Overview VS Code is a free editor from Microsoft, which is open source and support many useful extensions. It may be a good alternative to terminal based editors (like vim, nano and Emacs) for some of our users. Benefits Work with files on cluster, almost as if they would be on your own laptop Great syntax highlighting, linting, etc. for many languages (using extensions). Tools for debuging Extensions simplifying work with git (recommended: Git Graphs) Graphs produced on server, can be viewed directly withing VS Code When you are done editing - you can run sbatch job from terminal panel of VS Code You can set up port forwarding And more Using VS Code IMPORTANT: don't run computations in VS Code! Use VS Code only to edit scripts, use linting, etc."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_vs-code.html,1,"Only run calculations using srun/sbatch Download and Install Download and install it on your local machine: download Setting Up Remote Connections to HPC cluster Make sure you activated NYU VPN VS Code Lock files shall NOT be saved in /home. Ways to achieve that In extensions tab of VS Code find extension ""Remote - SSH"" press settings icon -> Extension settings Do one of the following: Don’t use lock files (uncheck ""Remote.SSH: Use Flock"") Store lock file in /tmp (check Remote.SSH: Lockfiles In Tmp) Inside VS Code F1 remote-ssh: Connect to Host New host (or - Configure SSH host) ssh <user>@greene.hpc.nyu.edu specify the path to the file, where the configuration of this connection will be saved More details here: https://code.visualstudio.com/docs/remote/ssh Login using ssh key Instead of typing your password every time you need to log in, you can also specify an ssh key."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_vs-code.html,2,"Only do that on the computer you trust Generate ssh key pair (terminal in Linux/Mac or cmd/WSL in Windows): https://www.ssh.com/ssh/keygen/ Note the path to ssh key files. Don't share key files with anybody - anybody with this key file can login to your account Log into cluster using regular login/password and then add the content of generated public key file (the one with .pub) to $HOME/.ssh/authorized_keys on cluster Next time you will log into cluster no password will be required. StrictHostKeyChecking no ServerAliveInterval 60 UserKnownHostsFile /dev/null LogLevel ERROR User <netid> IdentityFile <path to ssh key> How to Connect VS Code to a Greene Compute Node Sometimes users want to connect their VS Code GUI to a compute node, such as to launch a Jupyter Notebook file on Greene. This requires several steps to properly route through Greene's login nodes to the compute nodes in use."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_vs-code.html,3,"Please follow these instructions to connect and run VSCode server on Greene compute node: On Greene login node, submit a dummy Slurm job to sleep there, 1 CPU core and 1GB memory for 2 hours. This step will enable you to login to compute nodes you have jobs running there. sbatch --time=02:00:00 --mem=1GB --wrap ""sleep infinity""After the job is running, please check the compute node this job is running there with squeue [sw77@log-1 tmp]$ squeue -u sw77 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 23587213 cpu_gpu wrap sw77 R 18:29 1 gr070 In this example, the job is running on gr070On your local computer, add this block to ~/.ssh/config file Host greene-compute HostName <Compute Node your Jupyter Notebook is using e.g. gr070> User <NetID> ProxyJump <NetID>@greene.hpc.nyu.edu StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR Here the ProxyJump is <NetID>@greene, if you are in campus, please change to YourNYUNetID@greene.hpc.nyu.edu."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_general-hpc-topics_vs-code.html,4,"Please be sure to be connected to the NYU VPN (vpn.nyu.edu).Set up ssh login to Greene with ssh key without password (see section above) Now from VSCode, you can directly connect to greene-compute nodes with: ssh greene-compute More settings VS Code has many more settings and options. Explore! Additional notes High CPU load on login node from VS Code You may notice (please check with 'top') that your VS Code connection causes 'node' process running from your user to use a lot of CPU resources. One of the reason leading to that - large number of files within your home directory. Try to remove conda and pip environments from the home directory and check if this will resolve the issue. Python Recommended extensions: Python, Anaconda Extension Pack When those are installed you can switch python env or conda env by clicking on the left bottom line specifying python. Support for python scripts and Jupyter Notebooks Syntax highlighting: extension MagicPython"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_grants.html,0,"Grants Research Grant applications: Description of the Greene HPC cluster Research Grant applications: Description of the Greene HPC cluster A description of the Greene HPC cluster (as well as other NYU HPC clusters and the High Speed Research Network) for grant applications is available in the NYU Research Technology Facilities and Resources document. For more details about resources available for research grants and to request a letter of support for your grant application, please request a consultation with the HPC team via email (hpc@.nyu.edu)"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_hpc-projects-registration-pilot.html,0,"HPC Projects Registration Pilot What are HPC Projects HPC projects allow to manage HPC cluster users and resources and provide detailed usage tracking associated with different schools, departments, or research groups. Types of Projects PI may have projects for a range of topics, such as: Study of Mice Brain Images Study of Particle Physics Course: Intro to Biology Course: Applied Statistics Each of such projects would have independent owner (PI), managers, members, access to special cluster resources (if approved) Management Portal Link: HPC Projects management Types of users: Project owner (PI) who may create new projects Members (users who can run slurm jobs for this project) Managers (change project, assign members/managers, etc) Special resources approver Usage inspector HPC admin Specifying Projects with Slurm Any member of the project can run jobs on slurm for a specific project by specifying corresponding account information."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_hpc-projects-registration-pilot.html,1,"Each project will be assigned an appropriate designation, generally starting with ""pr_"" for ""project"" followed by a numeric designation and a function. For example: In this case, the project specifies general resource access for Project 3. Accessing Special Cluster Resources Some schools/units of NYU have access to certain resources of cluster, such as priority access to GPUs owned by the unit. A project owner may request permission for specific project to have access to such special resources. When a job is submitted with corresponding account specification, it will apply the priority access to such resource. This job at the same time have normal priority access to all other resources of cluster. For example: srun --account=pr_3_tandon_gpuThe above will activate the high priority access to Tandon-owned GPU resources."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_hpc-projects-registration-pilot.html,2,"If project has access to multiple special resources, such as tandon_gpu and cds_gpu, access to only one of them will have high priority - whichever is specified in the slurm job submission. A user must choose which special resource to prioritize for specific job submission. AMD GPUs While Nvidia GPUs can be requested using the standard --gres=gpu Slurm command, AMD GPUs require a slightly different configuration. One can add constraint to specify that AMD GPUs are needed. The following will request any AMD node, similarly to how --gres=gpu will request any Nvidia node on its own --gres=gpu --constraint=amdIf you need a specific AMD GPU (MI50, MI100, MI25) then that should be specified in the constraint as well. The below example will only request MI100 or MI250 AMD GPUs. --gres=gpu --constraint=""mi100|mi250"""
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_provide-feedback-for-our-services.html,0,"Provide Feedback for Our Services In order to see the form, please sign in using your NYU email (link), and return to this page. Alternatively, use the button below to launch the form in a new window. In order to see the form, please sign in using your NYU email (link), and return to this page. Alternatively, use the button below to launch the form in a new window."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resolving-common-issues.html,0,Resolving Common Issues Dear users of HPC cluster The answers to many common issues/questions are available on our website. Here is a list of helpful steps we encourage you to take: Getting Started Account Issues How do I apply/renew an account?
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resolving-common-issues.html,1,"Note: applying for a new account or renewing an existing account require connecting to the VPN Login issues Check that you are using the VPN or the HPC Gateway Server (see Accessing HPC and SSH Tunneling pages for more info) Make sure you did not go over quota (both storage size, and inodes number) in home directory Software Requests / Packages Installation Please read the comprehensive page for supported software and the follow steps For Singularity environments: use “:rw” while installing packages, “:ro” when otherwise running containers (walkthrough here) Use the --nv flag to pass NVIDIA drivers to singularity containers (more details here) Data Management, Disk Space Questions, sharing data, transferring data, etc Read the Best Practices for Storage page for tips on managing files and quotas General information can be found on the Data Management on the NYU HPC Clusters page Issues with Jobs, e.g. Long Queue Times or Jobs Killed As a reminder, don’t run jobs on login node!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resolving-common-issues.html,2,This may result in your job being killed. Please submit a Slurm job. Look at the efficiency of your jobs
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes.html,0,"Resources for Classes We support educational activities at NYU in several ways. If you are an instructor or TA of a course, please take a look at the options below. If you think we could help you to teach classes in other ways, let us know (Contact us). Please note, we do not have a separated cluster dedicated for courses. General HPC (Greene) Classes may use Greene cluster, if this is absolutely necessary due to need of large datasets, heavy computations, etc. Notes: Jobs are submitted to general queue Students of your class can use Greene cluster to run their jobs, as any other user of HPC cluster. Cluster tends to get more busy during period of exams, and thus the wait time to get resource may get significantly longer If you would like for students to use cluster, all of them have to get HPC account."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes.html,1,"Please submit this form to do that (use this form only if you are full-time faculty) HPC has maintenance schedule which may interfere with your class exams, so it is recommended NOT to rely on HPC cluster for exams To share data with students, data can be put on /scratch/ and access can be controlled using ACL Users can use cluster in a classical way (using terminal), or using OOD web interface There is no cost for using Greene cluster for your classes There is no student limit. Any NYU student with valid netID can obtain HPC account and get access One can find more information here Special pre-allocation of resources It is possible in some rear cases to get special allocation of nodes for specific class. However this is very uncommon and requires strong justification, as this prevents other users from using hardware. Sometimes resources can be allocated in cloud instead, to have lower impact on other users' work, using cloud-bursting approach."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes.html,2,"Please contact us for details, if you believe your class needs this kind of resources. Additional cost may be associated with this approach Hadoop (Dataproc): Big Data / HDFS / Spark If you are an instructor or student of a Big Data class, and require students to learn Hadoop, Dataproc is a potential option. You can teach and learn the ecosystem of Hadoop/Spark technologies used in most companies working with large data. For work on large datasets traditional HPC relies on specialized shared file systems (like Lustre, and BeeGFS) and a high speed network. In contrast most companies in industry rely on Hadoop/HDFS. Hadoop provides a perfect model of horizontal scaling - when data input/output requirements grow (for example number of users reading website or sending queries) additional nodes can be added to allow for faster read/write."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes.html,3,"Hadoop's map-reduce approach allows one to write code which brings computations to the same nodes where data is stored, and thus the impact of the relatively slower inter-node communications becomes less important. Notes Any NYU instructor may start using Hadoop cluster for their class. However, we request instructors notify us before the beginning of the semester on their plan of using the Dataproc cluster for their class If you would like for students to use Dataproc, you can submit a bulk request form and your students will be added to the cluster. There is no pre-allocated resources for specific classes To share data with students it can be put on HDFS file system and access can be controlled using ACL There is no cost for using Hadoop cluster for your classes There is no student limit."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes.html,4,"Any NYU student with valid NetID can obtain an HPC account and get access After your class has been added to the cluster, you can manage which students have access by adding/removing students from a Google Group. The Google Group will have a name of the form Dataproc-<Course Catalog Number>-<Fall/Summer/Spring/Winter>-<Last Two Digits of Year>@nyu.edu. Dedicated course coding environment (JupyterHub on GCP) Dedicated environment provides some advantages comparing to students working on their laptops, or on HPC cluster. Advantages include high availability, no HPC queue, simple management of environment. For more info (including ""who is paying"") look at JupyterHub at ResearchCloud. We support classes of various sizes - from very small to classes of hundreds of students Google Cloud (GCP) Instructors: If you need GCP resources for your classes, please email us. There may be cost associated with the GCP services."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes.html,5,"Students: if this is the first time you are using GCP as NYU student, you may be eligible to obtain Google credits If this is the case, please apply for credits using your NYU account (https://cloud.google.com/free/) List of classes using our services Other resources you may find useful Data Robot (Auto ML) - Academic program application"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2019-fall.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2019-fall.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing 2019 Fall Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2020-fall.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2020-fall.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing 2020 Fall Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2020-spring.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2020-spring.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing 2020 Spring Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2020-summer.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2020-summer.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing 2020 Summer Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2021-fall.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2021-fall.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing 2021 Fall Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2021-spring.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2021-spring.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing 2021 Spring Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2021-summer.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2021-summer.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing 2021 Summer Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2022-fall.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2022-fall.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing 2022 Fall 2022 Fall Classes Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2022-spring.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2022-spring.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing 2022 Spring 2022 Spring Classes Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2022-summer.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_courses_2022-summer.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing 2022 Summer 2022 Summer Classes Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_jupyterhub.html,0,"JupyterHub The NYU HPC team, in close collaboration with course instructors and departmental educational technologists, maintains and provides access to a centralized JupyterHub environment to support courses using Google Cloud Platform (GCP) Benefits of JupyterHub on GCP Use JupyterHub on GCP allows to support high availability No need for HPC account (NYU email is sufficient) Helps separate research HPC environment and teaching JupyterHub environment Self service approach allowing instructors to control various parameters of deployment dynamically What is provided R, Python, Julia kernels."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_jupyterhub.html,1,"Tools like SQL, MongoDB, Spark, TensorFlow, PyTorch can be used Custom environments for packages installation (using conda) IDE options: Classical Notebook, JupyterLab, RStudio Designated storage to share large data files with students (large; writable only by instructors/TAs) Persistent storage for students’ home directory (limited size) Sync from github private repositories NBgrader (only in Classical Notebook IDE option) How to create assignments, setup auto-grading, and more: link Watch video about NBgrader use GPUs are available as an option What is controlled by an instructor List of authorized students (whitelist) May be set to remain open for a limited time duration List of admins/TAs IDE for student pods: Classical Notebook, JupyterLab, RStudio Persistent storage need for students RAM for student pods GPU allocation if needed for student pods Link to github repository Custom R/Python conda environments created by instructors themselves."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_jupyterhub.html,2,"An instructor can specify a version of R or Python while creating an environment Instructions and Support To access instructions on how to use the system Submit intake form and wait for an approval Wait for your class environment to become active Instructions will be available in settings webiste NYU group (accessible only to Instructors and TAs) is used to ask/answer/read questions and answers You will be invited to the group, when your class is created In cases of unusual specific requirements, we will work with you one-on-one The HPC team will support designated/named instructors/TA’s. In any case, HPC team can't respond to questions from individual students (hundreds of them!) Who is paying For the current and the next semester, costs will be covered by NYU IT Research Technology budget. This may change in future semesters. Be considerate while requesting more resources (RAM, GPU, etc.), as this will increase the cost."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_jupyterhub.html,3,"You can estimate the cost using this app Intake form Please submit the course in-take form if you would like to use the service Maintenance If significant changes to the environment are necessary, work be done during the semester recess Usage & cost Monitoring In case an actual utilization goes significantly above your estimation, we may need to contact you and figure out whether to continue with the current config Decommissioning Student environment and data will be removed approximately 2 weeks after end of semester Instructor environment and data may be kept active between semester if requested and approved Save files in shared and instructor home directories if needed Hint: in order to easily save a directory, you can tar it in terminal and then download from Jupyter/RStudio UI interface tar -czvf name-of-archive.tar.gz /path/to/directory-or-file Save list of packages and their versions If you installed all the packages using conda install (and nothing using 'pip install' or ""install.packages"") you can use conda list -e > requirements.txt ## saves list of packages installed by conda conda create --name <env> --file requirements.txt If you did install anything with 'pip install', use the following to command to track both: packages installed by conda and by pip conda env export > environment.yaml ## saves to file info about env name, conda packages, pip packages conda env create -f environment.yaml If you did use install.packages Either keep track of such packages yourself or save list of packages and their versions with this command write.csv(data.frame(installed.packages())[c(""Package"", ""Version"")], ""r_packages.txt"") Then install those manually in the next semester."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_jupyterhub.html,4,A more automatic way would be to use the renv package. Make sure to download and save file requirements.txt/environment.yaml/r_packages.txt file ! Who is using now look here FAQ Please look for info on settings site provided to instructors/TAs
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_resources-for-classes_jupyterhub_jupyterhub-intake-form.html,0,"Please make sure you logged in (with NYU account) to see the form. Alternatively, use this link to open the form"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_support.html,0,Support Some of your questions may be already answered here Cluster Getting Started Documentation Consider to sign up for Training and Workshops Contact us directly at hpc@nyu.edu NYU HPC offers personalized help through personal consultations for simple and advanced cases: Do you have troubles with something that seems trivial? Would you like to discuss how to better apply Deep Learning to your case? Something else ? Fill out our consultation request form below and we'll get back to you
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_supported-research-initiatives.html,0,Supported Research Initiatives The NYU HPC team supports several other research system initiatives: REDCap online survey and database tool Secure Research Data Environment (SRDE) for sensitive data analysis The NYU HPC team supports several other research system initiatives: REDCap online survey and database tool Secure Research Data Environment (SRDE) for sensitive data analysis
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_supported-research-initiatives_high-speed-research-network.html,0,"High Speed Research Network The High Speed Research Network (HSRN) is NYU's cutting-edge, high-throughput computer network managed by Research Technology Services. The HSRN operates independently from NYU's enterprise network (NYU-NET) and is purpose-built to meet the escalating needs of research workflows for flexibility, low latency, and dedicated large bandwidth. Researchers that engage in the following may benefit from HSRN: Use existing Research Technology Services such as HPC Big Data Processing Transferring Large Datasets Real-time Quantitative Analysis (Closed Loop Systems) Multimedia Data Edge Computing Low Latency Use Cases To facilitate connection, the HSRN provides 10 Gbps copper upgrades to existing Ethernet outlets, as well as fiber connections up to 100 Gbps directly to specially configured lab servers and workstations."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_supported-research-initiatives_high-speed-research-network.html,1,"Researchers on the network have dedicated high-bandwidth connectivity between HSRN-enabled buildings, High Performance Computing resources, member institutions of the New York State Educational and Research Network (NYSERNET), Internet2 (R&E), the Internet, and access to certain NYU-NET resources. A research Faculty Advisory Board (FAB) provides oversight and input to NYU IT regarding HSRN governance, planning, and operations. If you would like to request HSRN connectivity, please fill out this form. See current and future HSRN locations here."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_supported-research-initiatives_redcap.html,0,REDCap REDCap (Research Electronic Data Capture) was developed in Vanderbilt University. Click here for more information about the Wash Sq REDCap service at NYU. REDCap (Research Electronic Data Capture) was developed in Vanderbilt University. Click here for more information about the Wash Sq REDCap service at NYU.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_supported-research-initiatives_secure-research-data-environment-srde.html,0,"Secure Research Data Environment (SRDE) SRDE Service Overview The Secure Research Data Environment (SRDE) is a centralized secure computing platform designed to support research projects that require storage and computational resources. It provides a space for researchers to design and build secure, scalable, and resilient environments to store, share, and analyze moderate and high-risk data, as per the NYU Electronic Data and System Risk Classification Policy. The Research Technology Services team leverages cloud computing resources to provide flexible, scalable, remotely accessible secure virtual environments. The team provides researchers with consultations and resources to comply with security requirements of research grants and Data Use Agreements (DUAs). SRDE resources intend to meet the security controls outlined in the NIST SP 800-171 to safeguard Controlled Unclassified Information (CUI). For more information, please see the SRDE Infographic."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_supported-research-initiatives_secure-research-data-environment-srde.html,1,"SRDE Eligibility A project must be reviewed and approved by the University's Institutional Review Board (IRB) (or receive a waiver) in order to have an SRDE. Access to SRDE is available to NYU researchers and sponsored members of their research team (i.e. co-investigators, research assistants, external collaborators). SRDE project owners and requesters must be in one of the eligible positions to act as a project Primary Investigator (PI), as outlined by the NYU IRB policy. This includes Tenured/Tenure Track Faculty, Continuing Contract faculty, Honorific Research Faculty, Professional Research Personnel, and Emeriti and retired faculty. The SRDE project owner should be the same as the PI of the IRB protocol or waiver. Users on a research project must have valid and active NYU NetID credentials, including external, non-NYU collaborators, and be listed on the IRB protocol. Non-NYU Researchers/Collaborators need to obtain an affiliate status to obtain access."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_supported-research-initiatives_secure-research-data-environment-srde.html,2,"A full-time NYU faculty member must sponsor a non-NYU collaborator for affiliate status. Please see instructions for affiliate management (NYU NetID login is required to follow the link). A Data Steward must be a faculty member or university employee. A Data Steward must be designated for each research project using SRDE and will be solely responsible for the transport of data for the project’s SRDE. Only users approved according to the Data Provider’s requirements are permitted to access project workspaces and related resources. These individuals are required to go through our on-boarding process, which includes signing a User Agreement and agreeing to our Terms of Use. Requesting an SRDE Project The SRDE form contains details about the project, such as if the project requires IRB (Institutional Review Board) approval, technical requirements (such as data storage and software). The form will be submitted to the SRDE team for review."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_supported-research-initiatives_secure-research-data-environment-srde.html,3,"Link to the Secure Research Data Environment Intake Form After you submit the intake form, the SRDE team will review the submitted documents and will respond to schedule the consultation. SRDE User Guide Coming Soon! FAQ SRDE Frequently Asked Questions Additional Info NYU Secure Research Data Environment Webpage NYU Electronic Data and System Risk Classification System Guidelines for Technology in Faculty Research Browse Data Sharing Requirements by Federal Agency Research Cloud: NIH STRIDES Initiative at NYU About Cybersecurity Maturity Model Certification (CMMC) 2.0 Support Please email your questions to: srde-support@nyu.edu"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials.html,0,Introduction to Unix/Linux Introduction to Slurm Big data tutorial: Map Reduce Big data tutorial: Spark Big data tutorial: Hive Managing your HPC Environment
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-hive.html,0,Big Data Tutorial: Hive NOTICE: This page refers to the Peel Hadoop Cluster which was retired in 2023 and is in the process of being archived. Its contents should not be referenced for Dataproc Courses. What is Apache Hive? Apache Hive is a Data Warehouse software that facilitates querying and managing large datasets residing in a distributed storage (Example: HDFS). Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called HiveQL. The Hive Query Language (HiveQL or HQL) for MapReduce processes structured data using Hive. It also provides: Tools to enable easy data extract/transform/load (ETL) A mechanism to impose structure on a variety of data formats Access to files stored either directly in Apache HDFS or in other data storage systems such as Apache HBase Query execution via MapReduce What is Hadoop?
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-hive.html,1,"Hadoop is an open-source software framework for storing and processing big data in a distributed/parallel fashion on large clusters of commodity hardware. Essentially, it accomplishes two tasks: massive data storage and faster processing. The core Hadoop consists of HDFS and Hadoop's implementation of MapReduce. What is HDFS? HDFS stands for Hadoop Distributed File System. HDFS is a highly fault-tolerant file system and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. What is Peel? Peel is the stand-alone Hadoop cluster running on Cloudera CDH version 6.3.4. Cloudera Enterprise(CDH) combines Apache Hadoop 3.0.0 and Apache Spark 2.4.0 with a number of other open-source projects to create a single, massively scalable system where you can unite storage with an array of powerful processing and analytic frameworks."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-hive.html,2,"Steps to connect to Connect on Windows Connect to NYU VPN User hostname ""peel.hpc.nyu.edu"" with port 22 in putty. Provide your credentials. Steps to connect to Connect on Mac Connect to NYU VPN Then from terminal, use ""ssh <net id>@peel.hpc.nyu.edu"". It will connect to the peel cluster. For more information, please follow the instructions on this link Components of Hive: HCatalog is a component of Hive. It is a table and storage management layer for Hadoop that enables users with different data processing tools — including Pig and MapReduce — to more easily read and write data on the grid. WebHCat provides a service that you can use to run Hadoop MapReduce (or YARN), Pig, Hive jobs or performs Hive metadata operations using an HTTP (REST style) interface. Hive is not... A relational database Designed for Online Transaction Processing (OLTP) A language for real-time queries and row-level updates."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-hive.html,3,"Data Types on Hive All the data types in Hive are classified into four types, given as follows: Column Types: Integrals (INT), String (CHAR), TimeStamp, Dates, Decimals & Union. Literals: Floating Point Null Values: NULL Complex Types: Arrays, Maps & Structs Basic Commands in HiveQL: Accessing Hive To access Hive there are two ways: For Hive Grunt Shell: Type “beeline” on peel For Hue UI: Provides a Hive interface -> http://demo.gethue.com/# Using Hive Grunt Shell Command Line Functions and MySQL EquivalentFunctions Using Beeline: Hive CLI is deprecated and migration to Beeline is recommended. -bash-4.1$ beelinebeeline> !connect jdbc:hive2://hm-1.hpc.nyu.edu:10000/ Enter password for jdbc:hive2://hm-1.hpc.nyu.edu:10000/: ********** 0: jdbc:hive2://hm-1.hpc.nyu.edu:10000/> use <net_id>; Hive Statements: User databases are pre-created for users on Peel."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-hive.html,4,"The following query is used to verify the list of databases you have access to: 0: jdbc:hive2://hm-1.hpc.nyu.edu:10000/> show databases;Now lets try an example: Before we start working with examples, please copy the Tutorial2 directory from '/share/apps/Tutorials/Tutorial2/' to '/home/netid/' cp -r /scratch/work/public/peel/tutorials/Tutorial2/ $HOMEcd $HOME/Tutorial2 hdfs dfs -put $HOME/Tutorial2/user_posts.txt /user/<net_id>/ Example 1: Create a table ""messages"" with columns user, post and time. : Use 'describe' to display the list of columns in the table. or describe extended <table_name>;Load data into hive table. Usage of 'select' statement. Delete table. NOTE: Do not delete table 'messages', as the data from this table is used in the next example. With Hive we are operating on the Apache Hadoop data store. Any query you write, table that you create, data that you copy persists from query to query."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-hive.html,5,"You can think of Hive as providing a data workbench where you can examine, modify and manipulate the data in Apache Hadoop. So when we perform our data processing task we will execute it one query or line at a time. Once a line successfully executes you can look at the data objects to verify if the last operation did what you expected. All your data is live, This kind of flexibility is Hive’s strength. You can solve problems bit by bit and change your mind on what to do next depending on what you find. Hive is only pointing to the data on the HDFS file system but there is also an option of using ""local inpath"". Hive only stores the structure of the table and not the data. Data is always accessed from HDFS or the local machine. Example 2: External Tables Create an external table ""messages2"" with columns user, post and time. Copy input data manually to HDFS location '/user/<net_id>/messages2' as given below. Now, You can query the table 'messages2' from hive."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-hive.html,6,"0: jdbc:hive2://babar.es.its.nyu.edu:10000/> select * from messages2; PARTITIONING Example 3: Static Partition Hive organizes tables into partitions. It is a way of dividing a table into related parts based on the values of partitioned columns such as date, city, and department. Using partition, it is easy to query a portion of the data. Create a partitioned hive table. Here we are creating a partition for 'country' by using PARTITIONED BY clause. Load data from table 'messages' which was created in the previous example. Alternate way to load data into the table. Now, You can query the table 'messages3' from hive. Alternate way to see data i.e., Raw data display. NOTE: If we go for the above approach, if we have 50 partitions we need to do the insert statement 50 times. That is a tedious task and it is known as Static Partition."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-hive.html,7,"Example 4:Dynamic Partition Static Partition columns: In DML/DDL involving multiple partitioning columns, the columns whose values are known at COMPILE TIME (given by user). Dynamic Partition columns: Columns whose values are only known at EXECUTION TIME. Create a partitioned hive table. Here we are creating the partition for 'country' by using PARTITIONED BY clause. In order to achieve dynamic partition we need to execute below SET commands in hive. set hive.exec.dynamic.partition=true : This enables dynamic partitions, by default it is false. set hive.exec.dynamic.partition.mode=nonstrict : We are using the dynamic partition without a static partition (A table can be partitioned based on multiple columns in hive) in such case we have to enable the non-strict mode. In strict mode, we can use dynamic partition only with a Static Partition. Load the data into the partitioned table 'messages4' from table 'messages' which was created in previous examples."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-hive.html,8,"Now, You can query the table 'messages4'. select * from messages4 where country = 'UK'; select * from messages4 limit 5; select count(*) from messages4; Example 5:Bucketing Tables or partitions are sub-divided into buckets, to provide extra structure to the data that is used for more efficient querying. Bucketing works based on the value of hash function of some column of a table. In hive, bucketing does not work by default. You will have to set the following variable to enable bucketing. set hive.enforce.bucketing=true; Create a bucketed table. Load data into bucketed table 'messages5' from table 'messages' created in the previous example. INSERT into table messages5 SELECT users,post,time,country; Now, You can query the table 'messages5'."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,0,"Big Data Tutorial: Map Reduce NOTICE: This page refers to the Peel Hadoop Cluster which was retired in 2023 and is in the process of being archived. Its contents should not be referenced for Dataproc Courses. What is Hadoop? Hadoop is an open-source software framework for storing and processing big data in a distributed/parallel fashion on large clusters of commodity hardware. Essentially, it accomplishes two tasks: massive data storage and faster processing. The core Hadoop consists of HDFS and Hadoop's implementation of MapReduce. What is HDFS? HDFS stands for Hadoop Distributed File System. HDFS is a highly fault-tolerant file system and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. What is Map-Reduce? MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm on a cluster."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,1,"Let's go to the slide deck for more information: Mapreduce Phases in MapReduce A MapReduce job splits a large data set into independent chunks and organizes them into key-value pairs for parallel processing. A key-value pair (KVP) is a set of two linked data items: a key, which is a unique identifier for some item of data, and the value, which is either the data that is identified or a pointer to the location of that data. The mapping and reducing functions receive not just values, but (key, value) pairs.This parallel processing improves the speed and reliability of the cluster, returning solutions more quickly and with greater reliability. Every MapReduce job consists of at least three parts: The driver The Mapper The Reducer Mapping Phase The first phase of a MapReduce program is called mapping. A list of data elements are provided, one at a time, to a function called the Mapper, which transforms each element individually to an output data element."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,2,"The Map function divides the input into ranges by the InputFormat and creates a map task for each range in the input. The JobTracker distributes those tasks to the worker nodes. The output of each map task is partitioned into a group of key-value pairs for each reduce. Reducing Phase Reducing lets you aggregate values together. A reducer function receives an iterator of input values from an input list. It then combines these values together, returning a single output value. The Reduce function then collects the various results and combines them to answer the larger problem that the master node needs to solve. Each reduce pulls the relevant partition from the machines where the maps executed, then writes its output back into HDFS. Thus, the reduce is able to collect the data from all of the maps for the keys and combine them to solve the problem. MapReduce Data Flow What is Peel? Peel is the stand-alone Hadoop cluster running on Cloudera CDH version 6.3.4."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,3,"Cloudera Enterprise(CDH) combines Apache Hadoop 3.0.0 and Apache Spark 2.4.0 with a number of other open-source projects to create a single, massively scalable system where you can unite storage with an array of powerful processing and analytic frameworks. Commands for HDFS & MapReduce To access peel the Hadoop cluster Windows: Step - 1: Connect to NYU VPN Step - 2: User hostname ""peel.hpc.nyu.edu"" with port 22 in putty. Provide your credentials. Mac: Step - 1: Connect to NYU VPN Step - 2: Then from terminal, use ""ssh <net id>@peel.hpc.nyu.edu"". It will connect to the peel cluster. For more information, please follow the instructions on the Hadoop User Guide."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,4,HDFS COMMANDS TO UPLOAD DATA TO HDFS hadoop fs -put <filename_in_lfs> <hdfs_name> orhadoop fs -copyFromLocal <filename_in_lfs> <hdfs_name> orhdfs dfs -put <filename_in_lfs> <hdfs_name>TO GET DATA FROM HDFS hadoop fs -get <hdfs_name> <filename_in_lfs> orhadoop fs -copyToLocal <hdfs_name> <filename_in_lfs>TO CHECK HDFS FOR YOUR FILE hadoop fs -lsMAPREDUCE COMMANDS usage: hadoop [--config confdir] COMMAND where COMMAND is one of: fs run a generic filesystem user client version print the version jar <jar> run a jar file distcp <srcurl> <desturl> copy file or directories recursively archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive classpath prints the class path needed to get the Hadoop jar and the required libraries daemonlog get/set the log level for each daemon or CLASSNAME run the class named CLASSNAMETo trigger the job hadoop jar <jarfilename>.jar <DriverClassName> <ip_file_in_HDFS> <op_dir_name>To check running jobs hadoop job -listoryarn application -listTo kill a job hadoop job -kill <job_id>oryarn application -kill <job_id>Examples for Map-Reduce Jobs Example 1: Word Count: The objective here is to count the number of occurrences of each word by using key-value pairs.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,5,Step 1: Mac Users: ssh <NYUID>@peel.hpc.nyu.edu or Windows Users: Use Putty Step 2: Copy example1 folder to /home/<net_id>/ cp -r /scratch/work/public/peel/tutorials/Tutorial1/example1 /home/<net_id>/cd /home/<net_id>/example1Step 3: Place the book.txt file on to hdfs hadoop fs -put /home/<net_id>/example1/book.txt /user/<net_id>/book.txtStep 4: Compile code with java compiler and create a jar file using generated class files.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,6,export HADOOP_LIPATH=/opt/cloudera/parcels/CDH-6.3.4-1.cdh6.3.4.p0.6626826/libjavac -classpath $HADOOP_LIPATH/hadoop/*:$HADOOP_LIPATH/hadoop-mapreduce/*:$HADOOP_LIPATH/hadoop-hdfs/* *.javajar cvf WordCount.jar *.classStep 5: Run the MapReduce job using WordCount.jar hadoop jar WordCount.jar WordCount /user/<net_id>/book.txt /user/<net id>/wordcountoutputStep 6: Check output by accessing HDFS directories hadoop fs -ls /user/<net_id>/wordcountoutputhadoop fs -cat /user/<net_id>/wordcountoutput/part-r-00000ORhadoop fs -getmerge /user/<net_id>/wordcountoutput $HOME/output.txtcat $HOME/output.txtExample 2: Standard Deviation: The objective is to find the standard deviation of the length of the words.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,7,"Step 1: copy example2 folder to /home/<net_id>/ cp -r /scratch/work/public/peel/tutorials/Tutorial1/example2 /home/<net_id>/example2.txt - Input file StandardDeviation.jar - compiled jar file Step 2: Place the example2.txt file on to hdfs hadoop fs -put /home/<net_id>/example2/example2.txt /user/<net_id>/example2.txtStep 3: Run the MapReduce job using StandardDeviation.jar hadoop jar StandardDeviation.jar wordstandarddeviation /user/<net_id>/example2.txt /user/<net_id>/standarddeviationoutputStep 4: Check output by accessing HDFS directories hadoop fs -ls /user/<net_id>/standarddeviationoutputhadoop fs -cat /user/<net_id>/standarddeviationoutput/part-r-00000Example 3: Step 1: Create a directory to work with example3 mkdir /home/<net_id>/example3cd /home/<net_id>/example3Step 2: Copy the input to the local directory, then to HDFS."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,8,"cp /scratch/work/public/peel/tutorials/Tutorial1/example3/MapReduce-master/examples/inputComments.xml /home/<net_id>/example3/hadoop fs -put /home/<net_id>/example3/inputComments.xml /user/<net_id>/Step 3: Clone a git repository to create a local copy of the code git clone https://github.com/geftimov/MapReduce.gitcd /home/<net_id>/example3/MapReduceStep 4: Build/compile using maven. Make sure pom.xml is present in the same directory. This command will generate the ""target"" directory. /share/apps/peel/maven/3.5.2/bin/mvn installcd targetStep 5: Extract jar file. This command creates directory ""com"". jar -xvf MapReduce-0.0.1-SNAPSHOT.jarStep 6: Execute the process using the class files created in directory ""com"". export JAVA_CLASS=com/eftimoff/mapreduce/summarization/numerical/averagehadoop jar MapReduce-0.0.1-SNAPSHOT.jar $JAVA_CLASS/Average /user/<net_id>/inputComments.xml /user/<net_id>/AverageOutputStep 7: Check output by accessing HDFS directories."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,9,"hadoop fs -ls /user/<net_id>/AverageOutputhadoop fs -cat /user/<net_id>/AverageOutput/part-r-00000MapReduce Streaming Even though the Hadoop framework is written in Java, programs for Hadoop need not be coded in Java but can also be developed in other languages like Python, shell scripts or C++. Hadoop streaming is a utility that comes with the Hadoop distribution. This utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer. Streaming runs a MapReduce job from the command line. You specify a map script, a reduce script, an input, and an output. Streaming takes care of the Map-Reduce details such as making sure that your job is split into separate tasks, that the map tasks are executed where the data is stored."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,10,"Hadoop Streaming works a little differently (your program is not presented with one record at a time, you have to iterate yourself) -input – The data in hdfs that you want to process -output – The directory in hdfs where you want to store the output -mapper script – the program script command line or process that you want to use for your mapper -reducer script – the program script command or process that you want to use for your reducer. -file – Make the mapper, reducer, or combiner executable available locally on the compute nodes. There is an example of Hadoop-streaming at /share/apps/examples/hadoop-streaming on Peel."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-map-reduce.html,11,The README file explains how to run the example and where to find the hadoop-streaming.jar Command used to run a MapReduce job using streaming: cp -r /share/apps/examples/hadoop-streaming $HOME/example/ cd $HOME/example/An example of how to run a Hadoop-streaming job is: export HADOOP_LIPATH=/opt/cloudera/parcels/CDH-6.3.4-1.cdh6.3.4.p0.6626826/libhadoop jar $HADOOP_LIPATH/hadoop-mapreduce/hadoop-streaming.jar -numReduceTasks 2 -file $HOME/example -mapper example/mapper.py -reducer example/reducer.py -input /user/<net_id>/book.txt -output /user/<net_id>/example.out
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-spark.html,0,"Big Data Tutorial: Spark Today, Spark is being adopted by major players like Amazon, eBay, and Yahoo! Many organizations run Spark on clusters with thousands of nodes. According to the Spark FAQ, the largest known cluster has over 8000 nodes. Indeed, Spark is a technology well worth taking note of and learning about. This tutorial provides an introduction and practical knowledge to Spark. It contains information from the Apache Spark website as well as the book Learning Spark - Lightning-Fast Big Data Analysis. Spark on Hadoop: Spark is a framework for performing general data analytics on a distributed computing cluster like Hadoop. It provides in-memory computations for increase speed and data process over mapreduce. It runs on top of existing hadoop cluster and access hadoop data store (HDFS), can also process structured data in Hive and Streaming data from HDFS, Flume, Kafka, Twitter. Refer Big Data Tutorial 1 to know more about hadoop. What is Apache Spark?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-spark.html,1,"An Introduction Spark is an Apache project advertised as “lightning-fast cluster computing”. It has a thriving open-source community and is the most active Apache project at the moment. Spark provides a faster and more general data processing platform. Spark lets you run programs up to 100x faster in memory, or 10x faster on disk than Hadoop. Last year, Spark took over Hadoop by completing the 100 TB Daytona GraySort contest 3x faster on one-tenth the number of machines and it also became the fastest open-source engine for sorting a petabyte. Spark also makes it possible to write code more quickly as you have over 80 high-level operators at your disposal. To demonstrate this, let’s have a look at the “Hello World!” of BigData: the Word Count example."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-spark.html,2,"Written in Java for MapReduce it has around 50 lines of code, whereas in Spark (and Scala) you can do it as simply as this: sparkContext.textFile(""hdfs://..."").flatMap(line => line.split("" "")) .map(word => (word, 1)).reduceByKey(_ + _) .saveAsTextFile(""hdfs://..."") Q) Is there a point of learning Mapreduce, then? A) Yes. For the following reasons: Mapreduce is a paradigm used by many big data tools including Spark. So, understanding the MapReduce paradigm and how to convert a problem into a series of MR tasks is very important. When the data grows beyond what can fit into the memory on your cluster, the Hadoop Map-Reduce paradigm is still very relevant. Almost, every other tool such as Hive or Pig converts its query into MapReduce phases. If you understand the Mapreduce then you will be able to optimize your queries better. Q) When do you use apache spark? or What are the benefits of Spark over Mapreduce? A) The benefits of sparks are: Spark is really fast."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-spark.html,3,"As per their claims, it runs programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk. It aptly utilizes RAM to produce faster results. In map-reduce paradigm, you write many Map-reduce tasks and then tie these tasks together using Oozie/shell script. This mechanism is very time consuming and the map-reduce task has heavy latency. Quite often, translating the output out of one MR job into the input of another MR job might require writing another code because Oozie may not suffice. In Spark, you can basically do everything using a single application/console (pyspark or scala console) and get the results immediately. Switching between 'Running something on cluster' and 'doing something locally' is fairly easy and straightforward. This also leads to less context switch of the developer and more productivity. Spark kind of equals to MapReduce and Oozie put together."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-spark.html,4,"Additional key features of Spark include: It currently provides APIs in Scala, Java, and Python, and R.with support for other languages on the way. Integrates well with the Hadoop ecosystem and data sources (HDFS, Amazon S3, Hive, HBase, Cassandra, etc.) it can run on clusters managed by Hadoop YARN or Apache Mesos, and also run standalone. On peel, it's managed by YARN. The Spark core is complemented by a set of powerful, higher-level libraries that can be seamlessly used in the same application. These libraries currently include SparkSQL, Spark Streaming, MLlib (for machine learning), and GraphX. Spark Core Spark Core is the base engine for large-scale parallel and distributed data processing. It is responsible for: memory management and fault recovery scheduling, distributing and monitoring jobs on a cluster interacting with storage systems Spark introduces the concept of an RDD (Resilient Distributed Dataset). Q) What is RDD - Resilient Distributed Dataset?"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-spark.html,5,"A) RDD is a representation of data located on a network which is Immutable - You can operate on the RDD to produce another RDD but you can’t alter it. Partitioned / Parallel - The data located on RDD is operated in parallel. Any operation on RDD is distributed among using multiple nodes. Resilience - If one of the nodes hosting the partition fails, another node takes its data. RDD is not store the data itself, but the location and calculation process of data. Main Primitives - RDDs support two types of operations: Transformations and Actions Q) What are Transformations? A) The transformations are the functions that are applied on an RDD (resilient distributed data set). The transformation results in another RDD. A transformation is not executed until an action follows. (Lazy Execucation) The example of transformations are: map(func) - applies the function passed to it on each element of RDD resulting in a new RDD."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-spark.html,6,"filter(func) - creates a new RDD by picking the elements from the current RDD which pass the function argument. coalesce(numPartitions) - reduce RDD partition to numParitions Q) What are Actions? A) An action brings back the data from the RDD to the local machine. Execution of an action results in all the previously created transformation. The example of actions are: reduce(func) - executes the function passed again and again until only one value is left. The function should take two arguments and return one value. take(n) - take n values back to the local node form RDD. collect() - return all element in the dataset in format of array Transformations in Spark are “lazy”, meaning that they do not compute their results right away. Instead, they just “remember” the operation to be performed and the dataset (e.g., file) to which the operation is to be performed. The transformations are only actually computed when an action is called and the result is returned to the driver program."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-spark.html,7,"This design enables Spark to run more efficiently, as it can optimize the DAG of transformations we have created. For example, if a big file was transformed in various ways and passed to the first action, Spark would only process and return the result for the first line, rather than do the work for the entire file. By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist or cache method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. Hands-on Exercise Using PySpark Set up a Spark Standalone Cluster and open a new interactive Jupyter notebook. A notebook containing all of the code below can be found at /scratch/work/public/spark/spark_examples.ipynb. Initializing SparkContext: SparkContext represents the connection to a Spark execution environment. In fact, the main entry point to Spark functionality is SparkContext."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-spark.html,8,"You have to create a Spark context before using Spark features and services in your application. A Spark context can be used to create RDDs to access Spark services and run jobs. # Initialize the SparkContextfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName(""SparkStandaloneTest"").getOrCreate()sc = spark.sparkContextYou may see a ""WARNING: An illegal reflective access operation has occurred"", but the code should still run."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_big-data-tutorial-spark.html,9,"Creating RDD # Turn a collection into an RDDsc.parallelize([1, 2, 3])# Load text file from storage on Greenesc.textFile(""/scratch/<net_id>/path/to/file.txt"") Basic Transformations: nums = sc.parallelize([1, 2, 3])# Pass each element through a function squares = nums.map(lambda x: x*x)squares.collect() # => [1, 4, 9] # Keep elements passing a predicate evens = squares.filter(lambda x: x%2 == 0)evens.collect() # => [4] # Map each element to zero or more others ranges = nums.flatMap(lambda x: range(x))ranges.collect() # => [0, 0, 1, 0, 1, 2] Basic Actions: nums = sc.parallelize([1, 2, 3])# Retrieve RDD contents as a local collectionnums.collect() # => [1, 2, 3] # Return first K elementsnums.take(2) # => [1, 2] # Count number of elementsnums.count() # => 3 # Merge elements with an associative functionnums.reduce(lambda x, y: x + y) # => 6 Working with Key-Value Pairs: pets = sc.parallelize([(""cat"", 1), (""dog"", 1), (""cat"", 2)])# reduceByKey also automatically implements combiners on the map side reduced_pets = pets.reduceByKey(lambda x,y: x + y)reduced_pets.collect() # => [('dog', 1), ('cat', 3)] sorted_pets = pets.sortByKey()sorted_pets.collect() # => [('cat', 1), ('cat', 2), ('dog', 1)] # groupByKey generates iterables of all the key's associated values grouped_pets = pets.groupByKey()grouped_pets.collect() # => [('dog', <ResultIterable>), ('cat', <ResultIterable>)] Other Operations: Joins and Grouping page_visits = sc.parallelize([ (""index.html"", ""1.2.3.4""), (""about.html"", ""3.4.5.6""), (""index.html"", ""1.3.3.7"")])page_names = sc.parallelize([ (""index.html"", ""Home""), (""about.html"", ""About"")])page_visits.join(page_names).collect()# => [('about.html', ('3.4.5.6', 'About')),# ('index.html', ('1.2.3.4', 'Home')),# ('index.html', ('1.3.3.7', 'Home'))] cogroup = page_visits.cogroup(page_names).collect()[(x, tuple(map(list, y))) for x, y in sorted(list(page_visits.cogroup(page_names).collect()))]# => [('about.html', (['3.4.5.6'], ['About'])),# ('index.html', (['1.2.3.4', '1.3.3.7'], ['Home']))] Input/Output # Write RDD to a text file# Elements will be saved in string format# The output will automatically be sharded into multiple files within the 'output' directorynums.saveAsTextFile(""/scratch/<net id>/path/to/output"")# Read RDD from a saved text filereadnums = sc.textFile(""/scratch/<net id>/path/to/output/part-*"")readnums.collect() # => ['1', '2', '3']"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_containers-on-the-hpc.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_containers-on-the-hpc.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing Using Containers on HPC Using containers on Greene HPC (public) Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_globus-file-transfer.html,0,Search this site Skip to main content Skip to navigation NYU High Performance Computing Home Search Services HPC Systems Greene Hardware specs Storage Specs Getting Started Getting Started - AMD nodes Best Practices Software R Packages with renv RStudio + R + Python.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_globus-file-transfer.html,1,"Reproducible way Python packages with Virtual Environments Conda Environments (Python, R) Singularity with Miniconda Open OnDemand (OOD)with Conda/Singularity LLAMA on HPC System Status HPC Storage Hardware Specs Best Practices Storage Status Data Management Data Transfers Transferring Cloud Storage Data with rclone Globus Sharing Data on HPC Research Project Space (RPS) Squash File System and Singularity Datasets Cloud Computing GCP Self-Managed Projects HPC Bursting to Cloud NIH STRIDES MATLAB online Penguin On Demand Dataproc Retired clusters Accessing HPC Getting and Renewing an Account Training & Support Support General HPC Topics SLURM: Submitting Jobs SLURM: Main Commands Singularity: Run Custom Applications with Containers Tunneling and X11 Forwarding VS Code AI at HPC: tips Large Number of Small Files R & Python in the Cloud SQLite: Handling Large Structured Data Joblib Example DASK Example Independent R tasks example LLM on HPC Spark: Interactive Standalone Cluster Spark: Multi-Node Jobs with Singularity Tutorials Linux Tutorial Slurm Tutorial Big Data Tutorial: Spark Containers on the HPC Globus File Transfer Resources for classes JupyterHub Grants Supported Research Initiatives REDCap Secure Research Data Environment (SRDE) High Speed Research Network Provide Feedback for Our Services Resolving Common Issues HPC Projects Registration - Pilot HPC Projects Projects Registration About Cluster Stats and Usage Data Efficiency of User Jobs Resources Consumption Acknowledgement Statement Events Collaborations HPC Policies Announcements & News Featured Research Sign Language App Cognitive Phenomena Heart Attack Treatment DNA Repair MRI Ocean Warming Job Opportunities NYU High Performance Computing Globus File Transfer Globus and File Transfers (public) Report abuse Page details Page updated Report abuse"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_linux-tutorial.html,0,"Linux Tutorial Getting a new Account on the NYU HPC clusters It is expected of everyone to have an NYU HPC Cluster Account. If not follow steps here to apply for a new account. Getting Started on HPC Greene Cluster In a Linux cluster, there are hundreds of computing nodes interconnected by high-speed networks. Linux operating system runs on each of the nodes individually. The resources are shared among many users for their technical or scientific computing purposes. The process to log into the Greene Cluster: NYU Campus: From within the NYU network, that is, from an on-campus location, or after you VPN inside NYU's network, you can login to the HPC clusters directly. Off-campus: The host name of Greene is 'greene.hpc.nyu.edu'. Logging in to Greene is the two-stage process. The HPC clusters (Greene) are not directly visible to the internet (outside the NYU Network). If you are outside NYU's Network (off-campus) you must first login to a bastion host named gw.hpc.nyu.edu."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_linux-tutorial.html,1,"From within the NYU network, that is, from an on-campus location, or after you VPN inside NYU's network, you can login to the HPC clusters directly. You do not need to login to the bastion host. To login into the HPC cluster (Greene), simply use: ssh <NYUNetID>@greene.hpc.nyu.edu For access from Windows station using PuTTY, please click here. To connect to VPN from Linux/MAC, please click here. From an off-campus location (outside NYU-NET), logging in to the HPC clusters is a two-step process: 1) First login to the bastion host, gw.hpc.nyu.edu. From a Mac or Linux workstation, this is a simple terminal command (replace my_netid with your NetId). Your password is the same password you use for NYU Home: ssh <NYUNetID>@gw.hpc.nyu.edu Windows users will need to use Putty, see here for instructions. 2) Next login to the cluster."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_linux-tutorial.html,2,"For Greene, this is done with: ssh <NYUNetID>@greene.hpc.nyu.edu Available file systems on Greene Files Systems for usage: The NYU HPC clusters have multiple file systems for user's files. Each file system is configured differently to serve a different purpose. Basic Linux Commands Navigating the directory structure We've already seen ssh, which takes us from the host we are on to a different host, and hostname, which tells us which host we are on now. Mostly you'll move around filesystems and directories, which resemble inverted tree structures as shown below schematically: ""ls"" - To see what files are in the current directory, use ""ls"" (""list""). If this is your first time using the HPC cluster, ""ls"" probably won't return anything, because you have no files to list. There are a couple of useful options for ls: ""ls -l"" lists the directory contents in long format, one file or directory per line, with extra information about who owns the file, how big it is, and what permissions are set."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_linux-tutorial.html,3,"""ls -a"" lists hidden files. In Unix, files whose names begin with ""."" are hidden. This does not stop anything from using those files, it simply instructs ls not to show the files unless the -a option is used. A command typed at the Unix command prompt, looks something like this: ""pwd"" - print working directory, or ""where am I now?"". In Unix, filesystems and directories are arranged in a hierarchy. A forward slash ""/"" is the directory separator, and the topmost directory visible to a host is called ""/"". Filesystems are also mounted into this directory structure, so you can access everything that is visible on this host by moving around in the directory hierarchy. You should see something like /home/NetID ""cd"" - To change to a different directory, use ""cd"" (""change directory""). You'll need to give it the path to the directory you wish to change into, eg ""cd /scratch/NetID""."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_linux-tutorial.html,4,"You can go up one directory with ""cd .."".If you run ""cd"" with no arguments, you will be returned to your home directory and if you run ""cd -"", you will be returned to the directory you were in most recently. ""mkdir"" - To create a new location, use ""mkdir new_directory_name"".""rmdir"" - To remove a directory, use ""rmdir new_directory_name"". ""man"" - Manual page. This command provides more information about a command eg., ""man ls""""cat"" - Prints the content of the file eg., ""cat filename"" Copying, moving or deleting files locally Copying: The ""cp"" command makes a duplicate copy of files and directories within a cluster or machine. The general usage is ""cp source destination"": Moving: The ""mv"" command renames files and directories within a cluster or machine. The general usage is ""mv source destination"": Deleting files: The ""rm"" (remove) command deletes files and optionally directories within a cluster or machine. There is no undelete in Unix. Once it is gone, it is gone."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_linux-tutorial.html,5,"Text Editor (NANO) ""nano"" is a friendly text editor that can be used to edit the content of an existing file or create a new file. Here are some options used in nano editor. For more details for nano Click Here. Writing Scripts An entire sequence of commands can be captured in a script for repeated or later execution. This is the mechanism by which batch jobs are run on the HPC clusters. The essential elements of a script are illustrated in the example below: #!/bin/bash# the first line should begin with #! and the path to the interpreter under which the script should run # do stuff as if it were an interactive session: cd $HOME/some_place date ls -l pwd # scripts can use loops and conditionals. See 'man bash' for syntax for f in `ls`; do echo ""found a file called $f"" done There are two ways to run the script: 1) Give the script execute permission."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_linux-tutorial.html,6,"and run it as a command: $ chmod u+x my_script$ ./my_script2) Run a shell, and pass the script as an argument $ bash my_scriptNotice in the first example that to run the script, we prefixed it with ""./"". If the script is not somewhere in the shell's $PATH, it won't find it to run unless its location is explicitly specified. This is even true when the script is in the current directory - in fact for security reasons the current directory is not normally in the $PATH. Therefore, we specify that the script is in the current directory with ./. Setting execute permission with chmod In Unix, a file has three basic permissions, each of which can be set for three levels of user. The permissions are: Read permission (""r"") - numeric value 4. Write permission (""w"") - numeric value 2. Execute permission (""x""). - numeric value 1. When applied to a directory, execute permission refers to whether the directory can be entered with 'cd'."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_linux-tutorial.html,7,"The three levels of user are: The user who owns the file (the ""user"", referred to with ""u"") The group to which the file belongs - referred to with ""g"". Each user has a primary group and is optionally a member of other groups, when a user creates a file it is normally associated with the user's primary group. At NYU HPC all users are in a group named 'users', so group permission has little meaning. All other users are referred to with ""o"". You grant permissions with ""chmod who+what file"" and revoke them with ""chmod who-what file"". (Notice that the first has ""+"" and the second ""-""). Here ""who"" some combination of ""u"", ""g"" and ""o"" and what is some combination of ""r"", ""w"" and ""x"". So to set execute permission, as in the example above, we use: $ chmod u+x my_script"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_managing-your-hpc-environment.html,0,Managing Your HPC Environment Managing Files Managing Files To Add: Storage Quotas SCP Globus SquashFS? Loading Programs with Modules Loading Programs with Modules Customizable Environments with Singularity Customizable Environments with Singularity
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,0,"Slurm Tutorial Introduction to High Performance Computing Clusters In a High Performance Computing cluster, such as the NYU-IT HPC Greene cluster, there are hundreds of computing nodes interconnected by high-speed networks. Linux operating system runs on each of the nodes individually. The resources are shared among many users for their technical or scientific computing purposes. Slurm is a cluster software layer built on top of the interconnected nodes, aiming at orchestrating the nodes' computing activities, so that the cluster could be viewed as a unified, enhanced and scalable computing system by its users. In NYU HPC clusters the users coming from many departments with various disciplines and subjects, with their own computing projects, impose on us very diverse requirements regarding hardware, software resources, and processing parallelism. Users submit jobs, which compete for computing resources."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,1,"The Slurm software system is a resource manager and a job scheduler, which is designed to allocate resources and schedule jobs. Slurm is an open-source software, with a large user community, and has been installed on many top 500 supercomputers. This tutorial assumes you have a NYU HPC account. If not, you may find the steps to apply for an account here. It also assumes you are comfortable with Linux command-line environment. To learn about Linux please read Tutorial 1. Please read this page for Hardware Specs of Greene. Slurm Commands For an overview of useful Slurm commands, please read Slurm Main Commands page before continuing the tutorial. Software and Environment Modules Lmod, an Environment Module system, is a tool for managing multiple versions and configurations of software packages and is used by many HPC centers around the world."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,2,"With Environment Modules, software packages are installed away from the base system directories, and for each package, an associated modulefile describes what must be altered in a user's shell environment - such as the $PATH environment variable - in order to use the software package. The modulefile also describes dependencies and conflicts between this software package and other packages and versions. To use a given software package, you load the corresponding module. Unloading the module afterwards cleanly undoes the changes that loading the module made to your environment, thus freeing you to use other software packages that might have conflicted with the first one."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,3,Below is a list of modules and their associated functions: module unload <module-name> : unload a module module show <module-name> : see exactly what effect loading the module will have with module purge : remove all loaded modules from your environment module load <module-name> : load a module module whatis <module-name> : Find out more about a software package module list : check which modules are currently loaded in your environment module avail : check what software packages are available module help <module-name> : A module file may include more detailed help for the software package Batch Job Example Batch jobs require a script file for the SLURM scheduler to interpret and execute. The SBATCH file contains both commands specific for SLURM to interpret as well as programs for it execute.
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,4,"Below is a simple example of a batch job to run a Stata do file, the file is named myscript.sbatch: #!/bin/bash#SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=5:00:00 #SBATCH --mem=2GB #SBATCH --job-name=myTest #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu #SBATCH --output=slurm_%j.out module purge module load stata/14.2 RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR DATADIR=$SCRATCH/my_project/data cd $RUNDIR stata -b do $DATADIR/data_0706.do Below we will break down each line of the SBATCH script. More options can be found on the SchedMD website. ## This tells the shell how to execute the script#!/bin/bash ## The #SBATCH lines are read by SLURM for options. ## In the lines below we ask for a single node, one task for that node, and one cpu for each task. #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 ## Time is the estimated time to complete, in this case 5 hours."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,5,"#SBATCH --time=5:00:00 ## We expect no more than 2GB of memory to be needed #SBATCH --mem=2GB ## To make them easier to track, it's best to name jobs something recognizable. ## You can then use the name to look up reports with tools like squeue. #SBATCH --job-name=myTest ## These lines manage mail alerts for when the job ends and who the email should be sent to. #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu ## This places the standard output and standard error into the same file, in this case slurm_<job_id>.out #SBATCH --output=slurm_%j.out ## First we ensure a clean environment by purging the current one module purge ## Load the desired software, in this case stata 14.2 module load stata/14.2 ## Create a unique directory to run the job in. RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR ## Set an environment variable for where the data is stored."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,6,"DATADIR=$SCRATCH/my_project/data ## Change directories to the unique run directory cd $RUNDIR ## Execute the desired Stata do file script stata -b do $DATADIR/data_0706.do You can submit the job with the following command: $ sbatch myscript.sbatchThe command will result in the job queuing as it awaits resources to become available (which varies on the number of other jobs being urn on the cluster). You can see the status of your jobs with the following command: $ squeue -u $USERLastly, you can read the output of your job in the slurm-<job_ID>.out file produced by running your job. This is where logs regarding the execution of your job can be found, including errors or system messages."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,7,"You can print the contents to the screen from the directory containing the output file with the following command: $ cat slurm-<job_ID>.outInteractive Job Example While the majority of the jobs on the cluster are submitted with the 'sbatch' command, and executed in the background, there are also methods to run applications interactively through the 'srun' command. Interactive jobs allow the users to enter commands and data on the command line (or in a graphical interface), providing an experience similar to working on a desktop or laptop. Examples of common interactive tasks are: Editing files Compiling and debugging code Exploring data, to obtain a rough idea of characteristics on the topic Getting graphical windows to run visualization Running software tools in interactive sessions Interactive jobs also help avoid issues with the login nodes. If you are working on a login node and your job is too IO intensive, it may be removed without notice."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,8,"Running interactive jobs on compute nodes does not impact many users and in addition provides access to resources that are not available on the login nodes, such as interactive access to GPUs, high memory, exclusive access to all the resources of a compute node, etc. In the srun examples below, through ""--pty /bin/bash"" we request to start bash command shell session in pseudo terminal. By default the resource allocated is single CPU core and 2GB memory for 1 hour."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,9,"$ srun --pty /bin/bashTo request 4 CPU cores, 4 GB memory, and 2 hour running duration, you can add the following arguments: $ srun --cpus-per-task=4 --time=2:00:00 --mem=4000 --pty /bin/bashSimilarly, to request one GPU card, 3 GB memory, and 1.5 hour running duration you can add the following: $ srun --time=1:30:00 --mem=3000 --gres=gpu:1 --pty /bin/bashOnce the job begins you will notice your prompt change, for example: [mdw303@log-3 ~]$ srun --pty /bin/bashsrun: job 7864254 queued and waiting for resources srun: job 7864254 has been allocated resources [mdw303@cs080 ~]$ You can see above that the prompt changed from log-3 to cs080, meaning the prompt was no longer on the login node but rather the compute node. You can then load modules and software and run them interactively without impacting the cluster."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,10,"Below outlines the steps to start an interactive session and launch R: [sk6404@log-1 ~]$ srun --cpus-per-task=1 --pty /bin/bash[sk6404@cs022 ~]$ module purge [sk6404@cs022 ~]$ module load r/intel/4.0.3 [sk6404@cs022 ~]$ module list Currently Loaded Modules: 1) intel/19.1.2 2) r/intel/4.0.3 [sk6404@cs022 ~]$ R R version 4.0.3 (2020-10-10) -- ""Bunny-Wunnies Freak Out"" Copyright (C) 2020 The R Foundation for Statistical Computing Platform: x86_64-centos-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,11,"Type 'q()' to quit R. > 5 + 10 [1] 15 > q() Save workspace image? [y/n/c]: n [sk6404@cs022 ~]$ exit exit [sk6404@log-1 ~]$ MPI Job Example MPI stands for ""Message Passing Interface"" and is managed by a program, such as OpenMPI, to coordinate code and resources across the HPC cluster for your job to run workloads in parallel. You may have heard of HPC sometimes referred to as ""parallel computing"" because the ability to run many processes simultaneously - aka in parallel - is how the greatest efficiencies can be realized on the cluster. Users interested in MPI generally must compile the program they want to run using an MPI compiler. Greene supports two common OpenMPI versions, Intel and GCC. These can be loaded with either of the following: Intel $ module load openmpi/intel/4.1.1GCC $ module load openmpi/gcc/4.1.1Below we will illustrate an example of how to compile a C script for MPI."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,12,"Copy this into your working directory as hellompi.c: #include <stdio.h>#include <stdlib.h> #include <mpi.h> int main(int argc, char *argv[], char *envp[]) { int numprocs, rank, namelen; char processor_name[MPI_MAX_PROCESSOR_NAME]; MPI_Init(&argc, &argv); MPI_Comm_size(MPI_COMM_WORLD, &numprocs); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Get_processor_name(processor_name, &namelen); printf(""Process %d on %s out of %d\n"", rank, processor_name, numprocs); MPI_Finalize(); } Once copied into your directory, load OpenMPI and compile it with the following: $ module load openmpi/intel/4.1.1$ mpicc hellompi.c -o hellompi Next, create a hellompi.sbatch script: #!/bin/bash#SBATCH --nodes=4 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=hellompi #SBATCH --output=hellompi.out # Load the default OpenMPI module. module purge module load openmpi/intel/4.1.1 # Run the hellompi program with mpirun."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,13,"The -n flag is not required; # mpirun will automatically figure out the best configuration from the # Slurm environment variables. mpirun ./hellompi Run the job with the following command: $ sbatch hellompi.sbatchAfter the job runs, cat the hellompi.out to see that your processes ran on multiple nodes. There may be some errors, but your output should contain something like the following, indicating the process was run in parallel on multiple nodes: Process 0 on cs265.nyu.cluster out of 4Process 1 on cs266.nyu.cluster out of 4 Process 2 on cs267.nyu.cluster out of 4 Process 3 on cs268.nyu.cluster out of 4 GPU Job Example To request one GPU card, use SBATCH directives in job script: #SBATCH --gres=gpu:1To request a specific card type, use e. g. --gres=gpu:v100:1. The card types currently available are v100 and RTX 8000. As an example, let's submit an Amber job. Amber is a molecular dynamics software package."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,14,"The recipe is: $ mkdir -p /scratch/$USER/myambertest$ cd /scratch/$USER/myambertest $ cp /share/apps/Tutorials/slurm/example/amberGPU/* . $ sbatch run-amber.s Submitted batch job 14257 From the tutorial example directory we copy over Amber input data files ""inpcrd"", ""prmtop"" and ""mdin"", and the job script file ""run-amber.s"". NOTE: At the time of writing this you may need to update the run-amber.s script to load amber version 20.06, rather than the default 16.06. The content of the job script ""run-amber.s"" should be as follows: #!/bin/bash# #SBATCH --job-name=myAmberJobGPU #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:30:00 #SBATCH --mem=3GB #SBATCH --gres=gpu:1 module purge module load amber/openmpi/intel/20.06 cd /scratch/$USER/myambertest pmemd.cuda -O The demo Amber job should take ~2 minutes to finish once it starts running. When the job is done, several output files are generated."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,15,"Check the one named ""mdout"", which has a section most relevant here: |--------------------- INFORMATION ---------------------- | GPU (CUDA) Version of PMEMD in use: NVIDIA GPU IN USE. | Version 16.0.0 | | 02/25/2016 [......] |------------------- GPU DEVICE INFO -------------------- | | CUDA_VISIBLE_DEVICES: 0 | CUDA Capable Devices Detected: 1 | CUDA Device ID in use: 0 | CUDA Device Name: Tesla V100 | CUDA Device Global Mem Size: 11439 MB | CUDA Device Num Multiprocessors: 13 | CUDA Device Core Freq: 0.82 GHz | |-------------------------------------------------------- Array Job Example Using job array you may submit many similar jobs with almost identical job requirement. This reduces loads on both users and the scheduler system. Job arrays can only be used in batch jobs. Usually the only requirement difference among jobs in a job array is the input file or files. Please follow the recipe below to try the example."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,16,"There are 5 input files named 'sample-1.txt', 'sample-2.txt' to 'sample-5.txt' in sequential order. Running one command ""sbatch run-jobarray.s"", you submit 5 jobs to process each of these input files individually. Run the following commands to create the directory and submit the array job: $ mkdir -p /scratch/$USER/myjarraytest$ cd /scratch/$USER/myjarraytest $ cp /share/apps/Tutorials/slurm/example/jobarray/* . $ ls run-jobarray.s sample-1.txt sample-2.txt sample-3.txt sample-4.txt sample-5.txt wordcount.py $ sbatch --array=1-5 run-jobarray.s The content of the job script 'run-jobarray.s' is copied below: #!/bin/bash# #SBATCH --job-name=myJobarrayTest #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --array=1-5 # this creates an array!"
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_nyu-hpc_training-support_tutorials_slurm-tutorial.html,17,"#SBATCH --time=5:00 #SBATCH --mem=1GB #SBATCH --output=wordcounts_%A_%a.out #SBATCH --error=wordcounts_%A_%a.err module purge module load python/intel/3.8.6 cd /scratch/$USER/myjarraytest python2 wordcount.py sample-$SLURM_ARRAY_TASK_ID.txt Job array submissions create an environment variable called SLURM_ARRAY_TASK_ID, which is unique for each job array job. It is usually embedded somewhere so that at a job running time its unique value is incorporated into producing a proper file name. Also as shown above: two additional options %A and %a, denoting the job ID and the task ID (i.e. job array index) respectively, are available for specifying a job's stdout, and stderr file names."
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\nyu.edu_redcap.html,0,Sign in Use your Google Account Email or phone Forgot email? Type the text you hear or see Not your computer? Use Guest mode to sign in privately. Learn more about using Guest mode Next Create account
c:\Users\Danyal\Documents\HPC-LLM-2\HPC_LLM\LLM_UI\resources\scraped_data_nyu_hpc\sites.google.com\search_nyu.edu_nyu-hpc.html,0,"... containers (more details here) Data Management, Disk Space Questions, sharing data ... at the efficiency of your jobs Don't See Your Question Here ... ... will be available in settings webiste NYU group (accessible only to Instructors and TAs) is used to ask/answer/read questions and answers You will ... ... use: sacct -b -j <JobID> When reaching out to the HPC team asking for help with failing jobs, it is useful to find an exit code from the job at question ..."
